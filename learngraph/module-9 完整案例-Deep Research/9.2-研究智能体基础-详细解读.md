# LangGraph ç ”ç©¶æ™ºèƒ½ä½“åŸºç¡€è¯¦ç»†è§£è¯»

## ğŸ“š æ¦‚è¿°

**Research Agentï¼ˆç ”ç©¶æ™ºèƒ½ä½“ï¼‰** æ˜¯ Deep Research ç³»ç»Ÿçš„æ ¸å¿ƒæ‰§è¡Œå•å…ƒï¼Œè´Ÿè´£å®é™…çš„ä¿¡æ¯æœé›†å·¥ä½œã€‚æœ¬æ–‡æ¡£é‡ç‚¹è®²è§£ Agent å¦‚ä½•è‡ªä¸»è¿›è¡Œå¤šè½®æœç´¢ã€åæ€ç»“æœå¹¶åšå‡ºå†³ç­–ã€‚

**æ ¸å¿ƒèƒ½åŠ›ï¼š**
- ğŸ” è‡ªä¸»æœç´¢ - ä½¿ç”¨ Tavily Search API
- ğŸ¤” åæ€å†³ç­– - ä½¿ç”¨ think_tool åˆ†æç»“æœ
- ğŸ“Š ä¸Šä¸‹æ–‡ç®¡ç† - å‹ç¼©ç ”ç©¶ç»“æœé¿å… token çˆ†ç‚¸
- âš¡ æ™ºèƒ½ç»ˆæ­¢ - çŸ¥é“ä½•æ—¶åœæ­¢æœç´¢

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µï¼šAgent å·¥å…·è°ƒç”¨å¾ªç¯

### ä»€ä¹ˆæ˜¯ Agent å¾ªç¯ï¼Ÿ

Agent æ¨¡å¼çš„æ ¸å¿ƒæ˜¯**æŒç»­çš„å†³ç­–-æ‰§è¡Œå¾ªç¯**ï¼š

```
ç ”ç©¶ä¸»é¢˜è¾“å…¥
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM å†³ç­–èŠ‚ç‚¹                    â”‚
â”‚  - åˆ†æå½“å‰çŠ¶æ€                   â”‚
â”‚  - å†³å®šè°ƒç”¨å“ªäº›å·¥å…·                â”‚
â”‚  æˆ–æä¾›æœ€ç»ˆç­”æ¡ˆ                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
æœ‰å·¥å…·è°ƒç”¨ï¼Ÿ
    â”œâ”€ YES â†’ æ‰§è¡Œå·¥å…·
    â”‚         â†“
    â”‚    think_tool åæ€
    â”‚         â†“
    â”‚    å›åˆ° LLM å†³ç­– â†â”
    â”‚                  â”‚
    â””â”€ NO â†’ å‹ç¼©ç ”ç©¶ â”€â”€â”˜
              â†“
          ç»“æŸï¼Œè¿”å›ç»“æœ
```

**å…³é”®ç‚¹ï¼š**
- ğŸ”„ **å¾ªç¯æ‰§è¡Œ** - Agent å¯ä»¥è¿›è¡Œå¤šè½®æœç´¢
- ğŸ¤” **æ¯è½®åæ€** - ä½¿ç”¨ think_tool é¿å…ç›²ç›®æœç´¢
- ğŸ›‘ **æ™ºèƒ½ç»ˆæ­¢** - LLM å†³å®šä½•æ—¶æœ‰è¶³å¤Ÿä¿¡æ¯

---

## âš ï¸ æ ¸å¿ƒæŒ‘æˆ˜ï¼šSpin-out é—®é¢˜

### é—®é¢˜æè¿°

**Spin-outï¼ˆå¤±æ§æ—‹è½¬ï¼‰** æ˜¯ Agent ç³»ç»Ÿæœ€å¸¸è§çš„å¤±è´¥æ¨¡å¼ï¼š

```python
# âŒ Spin-out ç¤ºä¾‹
Agent:
  Search 1: "best coffee SF"
  Search 2: "top coffee shops SF"
  Search 3: "SF coffee recommendations"
  Search 4: "best rated coffee SF"
  Search 5: "SF specialty coffee"
  Search 6: "coffee shops San Francisco"
  ... (20+ æ¬¡ç±»ä¼¼æœç´¢ï¼Œå†…å®¹é‡å¤)
```

**åŸå› ï¼š**
- Agent ä¸æ»¡è¶³äºç°æœ‰ç»“æœ
- ä¸æ–­å°è¯•ç•¥å¾®ä¸åŒçš„æŸ¥è¯¢è¯
- æ²¡æœ‰åœæ­¢æœºåˆ¶

### è§£å†³æ–¹æ¡ˆï¼šä¸‰ç®¡é½ä¸‹

#### 1. **Hard Limitsï¼ˆç¡¬æ€§é™åˆ¶ï¼‰**

```python
# åœ¨ Prompt ä¸­æ˜ç¡®è§„å®š
"""
<Hard Limits>
- Simple queries: 2-3 search calls maximum
- Complex queries: Up to 5 search calls maximum
- Always stop: After 5 searches if you cannot find answers
</Hard Limits>
"""
```

#### 2. **think_tool å¼ºåˆ¶åæ€**

```python
@tool
def think_tool(reflection: str) -> str:
    """
    åœ¨æ¯æ¬¡æœç´¢åå¼ºåˆ¶ Agent åæ€ï¼š
    - æˆ‘æ‰¾åˆ°äº†ä»€ä¹ˆå…³é”®ä¿¡æ¯ï¼Ÿ
    - è¿˜ç¼ºå°‘ä»€ä¹ˆï¼Ÿ
    - æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å›ç­”é—®é¢˜ï¼Ÿ
    - åº”è¯¥ç»§ç»­æœç´¢è¿˜æ˜¯ç»™å‡ºç­”æ¡ˆï¼Ÿ
    """
    return f"Reflection recorded: {reflection}"
```

**å…³é”®ï¼š** think_tool åˆ›å»ºä¸€ä¸ª"æ€è€ƒæš‚åœ"ï¼Œè®© Agent è¯„ä¼°è¿›å±•è€Œéç›²ç›®ç»§ç»­ã€‚

#### 3. **Prompt Engineeringï¼šå…·ä½“å¯å‘å¼**

```python
research_agent_prompt = """
<Instructions>
Think like a human researcher with limited time:

1. **Read the question carefully** - What specific information is needed?
2. **Start with broader searches** - Use comprehensive queries first
3. **After each search, pause and assess** - Do I have enough? What's missing?
4. **Execute narrower searches** - Fill in the gaps
5. **Stop when you can answer confidently** - Don't search for perfection
</Instructions>

<Stop Immediately When>:
- You can answer the question comprehensively
- You have 3+ relevant examples/sources
- Your last 2 searches returned similar information
</Stop Immediately When>
"""
```

**å¯¹æ¯”æ•ˆæœï¼š**

| åœºæ™¯ | æ— ä¼˜åŒ– | æœ‰ä¼˜åŒ– |
|------|--------|--------|
| "SF æœ€ä½³å’–å•¡åº—" | 20+ æ¬¡æœç´¢ | 3-4 æ¬¡æœç´¢ |
| æœç´¢å†…å®¹ | é«˜åº¦é‡å¤ | é€’è¿›å¼ç»†åŒ– |
| æœ€ç»ˆè´¨é‡ | ä¿¡æ¯è¿‡è½½ | ç²¾å‡†å›ç­” |

---

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. çŠ¶æ€å®šä¹‰

```python
from typing_extensions import TypedDict, Annotated, List
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages

class ResearcherState(TypedDict):
    """ç ”ç©¶ Agent çŠ¶æ€"""
    # æ¶ˆæ¯å†å²ï¼ˆå·¥å…·è°ƒç”¨ã€ç»“æœç­‰ï¼‰
    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]
    # å·¥å…·è°ƒç”¨è¿­ä»£è®¡æ•°ï¼ˆç”¨äºé™åˆ¶ï¼‰
    tool_call_iterations: int
    # ç ”ç©¶ä¸»é¢˜
    research_topic: str
    # å‹ç¼©åçš„ç ”ç©¶ç»“æœ
    compressed_research: str
    # åŸå§‹ç¬”è®°ï¼ˆæœªå‹ç¼©ï¼‰
    raw_notes: Annotated[List[str], operator.add]
```

---

### 2. ç ”ç©¶å·¥å…·ï¼šTavily Search

```python
from tavily import TavilyClient
from langchain_core.tools import tool

tavily_client = TavilyClient()

@tool
def tavily_search(query: str) -> str:
    """
    ä½¿ç”¨ Tavily API è¿›è¡Œ Web æœç´¢

    è‡ªåŠ¨å¤„ç†ï¼š
    - æœç´¢æ‰§è¡Œ
    - å†…å®¹æå–
    - ç½‘é¡µæ‘˜è¦ï¼ˆå»é™¤å¹¿å‘Šã€å¯¼èˆªç­‰å™ªéŸ³ï¼‰
    """
    # æ‰§è¡Œæœç´¢
    result = tavily_client.search(
        query,
        max_results=3,
        include_raw_content=True,  # è·å–å®Œæ•´å†…å®¹
        topic="general"
    )

    # å»é‡
    unique_results = deduplicate_by_url(result['results'])

    # æ‘˜è¦æ¯ä¸ªç½‘é¡µï¼ˆé‡è¦ï¼šå‡å°‘ tokenï¼‰
    summarized_results = {}
    for url, page in unique_results.items():
        if page.get("raw_content"):
            # ä½¿ç”¨ LLM æå–å…³é”®ä¿¡æ¯
            summary = summarize_webpage_content(page['raw_content'])
        else:
            summary = page['content']

        summarized_results[url] = {
            'title': page['title'],
            'content': summary
        }

    # æ ¼å¼åŒ–è¾“å‡º
    return format_search_output(summarized_results)
```

**å…³é”®ä¼˜åŒ–ï¼šç½‘é¡µå†…å®¹æ‘˜è¦**

```python
def summarize_webpage_content(webpage_content: str) -> str:
    """
    å°†åŸå§‹ç½‘é¡µï¼ˆå¯èƒ½åŒ…å«å¤§é‡å™ªéŸ³ï¼‰å‹ç¼©ä¸ºç»“æ„åŒ–æ‘˜è¦

    è¿™æ˜¯ Context Engineering çš„ç¬¬ä¸€å±‚å‹ç¼©
    """
    class Summary(BaseModel):
        summary: str = Field(description="ç®€æ´æ‘˜è¦")
        key_excerpts: str = Field(description="å…³é”®å¼•ç”¨")

    structured_model = llm.with_structured_output(Summary)
    result = structured_model.invoke([
        HumanMessage(content=f"""
        ç½‘é¡µå†…å®¹: {webpage_content}

        æå–ï¼š
        1. æ ¸å¿ƒä¿¡æ¯æ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰
        2. å…³é”®å¼•ç”¨å’Œæ•°æ®ç‚¹

        å¿½ç•¥å¹¿å‘Šã€å¯¼èˆªã€æ ·æ¿å†…å®¹
        """)
    ])

    return f"<summary>{result.summary}</summary>\n<key_excerpts>{result.key_excerpts}</key_excerpts>"
```

**ä¸ºä»€ä¹ˆéœ€è¦æ‘˜è¦ï¼Ÿ**
- åŸå§‹ç½‘é¡µå¯èƒ½æœ‰ 10k+ tokens
- å¤§éƒ¨åˆ†æ˜¯æ— ç”¨å†…å®¹ï¼ˆå¹¿å‘Šã€å¯¼èˆªï¼‰
- æ‘˜è¦åé€šå¸¸åªéœ€ 200-500 tokens
- **æ•ˆæœï¼šå‹ç¼©æ¯” 20:1**

---

### 3. Agent æ ¸å¿ƒèŠ‚ç‚¹

#### LLM å†³ç­–èŠ‚ç‚¹

```python
from langchain.chat_models import init_chat_model

model = init_chat_model(model="anthropic:claude-sonnet-4-20250514")
tools = [tavily_search, think_tool]
model_with_tools = model.bind_tools(tools)

def llm_call(state: ResearcherState):
    """
    Agent å†³ç­–ä¸­å¿ƒï¼š
    1. åˆ†æå½“å‰ç ”ç©¶è¿›å±•
    2. å†³å®šè°ƒç”¨å·¥å…·æˆ–æä¾›ç­”æ¡ˆ
    """
    return {
        "researcher_messages": [
            model_with_tools.invoke(
                [SystemMessage(content=research_agent_prompt)] +
                state["researcher_messages"]
            )
        ]
    }
```

#### å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹

```python
def tool_node(state: ResearcherState):
    """
    æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    """
    tool_calls = state["researcher_messages"][-1].tool_calls

    # æ‰§è¡Œæ‰€æœ‰å·¥å…·
    observations = []
    for tool_call in tool_calls:
        tool = tools_by_name[tool_call["name"]]
        observation = tool.invoke(tool_call["args"])
        observations.append(observation)

    # åˆ›å»ºå·¥å…·æ¶ˆæ¯
    tool_outputs = [
        ToolMessage(
            content=obs,
            name=tc["name"],
            tool_call_id=tc["id"]
        )
        for obs, tc in zip(observations, tool_calls)
    ]

    return {"researcher_messages": tool_outputs}
```

#### è·¯ç”±é€»è¾‘

```python
def should_continue(state: ResearcherState) -> Literal["tool_node", "compress_research"]:
    """
    å†³å®šç»§ç»­æœç´¢è¿˜æ˜¯ç»“æŸ
    """
    last_message = state["researcher_messages"][-1]

    if last_message.tool_calls:
        return "tool_node"  # ç»§ç»­æ‰§è¡Œå·¥å…·
    else:
        return "compress_research"  # ç»“æŸï¼Œå‹ç¼©ç»“æœ
```

---

## ğŸ—œï¸ æ ¸å¿ƒæŠ€æœ¯ï¼šContext Engineeringï¼ˆä¸Šä¸‹æ–‡å·¥ç¨‹ï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦å‹ç¼©ï¼Ÿ

**é—®é¢˜åœºæ™¯ï¼š**

```python
# 5 è½®æœç´¢ï¼Œæ¯è½® 3 ä¸ªç½‘é¡µï¼Œæ¯ä¸ªç½‘é¡µ 500 tokensï¼ˆå·²æ‘˜è¦ï¼‰
5 * 3 * 500 = 7,500 tokens

# åŠ ä¸Šå·¥å…·è°ƒç”¨ã€think_tool åæ€
æ€»è®¡å¯èƒ½è¾¾åˆ° 10,000+ tokens

# è¿™äº›éƒ½åœ¨ researcher_messages ä¸­
# å¦‚æœç›´æ¥ä¼ ç»™ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ â†’ token çˆ†ç‚¸
```

### ä¸¤å±‚å‹ç¼©ç­–ç•¥

#### ç¬¬ä¸€å±‚ï¼šç½‘é¡µçº§å‹ç¼©ï¼ˆå‰é¢å·²è®²ï¼‰

åŸå§‹ç½‘é¡µ â†’ æ‘˜è¦ï¼ˆå‹ç¼©æ¯” 20:1ï¼‰

#### ç¬¬äºŒå±‚ï¼šç ”ç©¶çº§å‹ç¼©ï¼ˆå…³é”®ï¼ï¼‰

```python
def compress_research(state: ResearcherState) -> dict:
    """
    å°†å¤šè½®å·¥å…·è°ƒç”¨çš„å®Œæ•´å†å²å‹ç¼©ä¸ºæ ¸å¿ƒå‘ç°

    è¿™æ˜¯ Context Engineering çš„ç¬¬äºŒå±‚å‹ç¼©
    """

    # å‹ç¼© Prompt
    system_message = """
    ä½ æ˜¯ä¸€ä¸ªç ”ç©¶æ‘˜è¦ä¸“å®¶ã€‚å°†ä»¥ä¸‹ç ”ç©¶è¿‡ç¨‹å‹ç¼©ä¸ºæ ¸å¿ƒå‘ç°ã€‚

    è¦æ±‚ï¼š
    1. æå–æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼ˆæ•°æ®ã€è¯„åˆ†ã€æ’åï¼‰
    2. ä¿ç•™æ¥æºå¼•ç”¨
    3. ç»„ç»‡ä¸ºç»“æ„åŒ–æ ¼å¼
    4. åˆ é™¤é‡å¤å†…å®¹
    """

    # é‡è¦ï¼šåœ¨æœ«å°¾æ·»åŠ  Human Message é‡ç”³ä»»åŠ¡
    human_reminder = f"""
    åŸå§‹ç ”ç©¶ä¸»é¢˜: {state['research_topic']}

    è¯·ç¡®ä¿å‹ç¼©ç»“æœåŒ…å«æ‰€æœ‰ä¸è¯¥ä¸»é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œ
    ç‰¹åˆ«æ˜¯æ•°æ®ã€è¯„åˆ†ã€å…·ä½“ä¾‹å­ã€‚
    """

    messages = (
        [SystemMessage(content=system_message)] +
        state["researcher_messages"] +
        [HumanMessage(content=human_reminder)]
    )

    # ä½¿ç”¨æ”¯æŒé•¿è¾“å‡ºçš„æ¨¡å‹
    compress_model = init_chat_model(
        model="openai:gpt-4.1",
        max_tokens=32000  # å…³é”®ï¼šé¿å…è¾“å‡ºè¢«æˆªæ–­
    )

    response = compress_model.invoke(messages)

    # æå–åŸå§‹ç¬”è®°ï¼ˆä¾›æœ€ç»ˆæŠ¥å‘Šä½¿ç”¨ï¼‰
    raw_notes = [
        str(m.content)
        for m in filter_messages(
            state["researcher_messages"],
            include_types=["tool", "ai"]
        )
    ]

    return {
        "compressed_research": str(response.content),
        "raw_notes": ["\n".join(raw_notes)]
    }
```

### å‹ç¼©çš„ä¸‰ä¸ªå…³é”®ç‚¹

#### 1. **ä¸ºä»€ä¹ˆåœ¨æœ«å°¾æ·»åŠ  Human Messageï¼Ÿ**

```python
# âŒ æ²¡æœ‰æé†’
system_prompt + 10,000 tokens ç ”ç©¶å†å²
â†’ LLM å¯èƒ½"é—å¿˜"ä»»åŠ¡ï¼Œäº§ç”Ÿé€šç”¨æ‘˜è¦

# âœ… æ·»åŠ æé†’
system_prompt + 10,000 tokens ç ”ç©¶å†å² + "è®°ä½ï¼šç ”ç©¶ä¸»é¢˜æ˜¯ X"
â†’ LLM ä¿æŒèšç„¦ï¼Œä¿ç•™ç›¸å…³ä¿¡æ¯
```

è¿™ä¸ªæŠ€å·§æ¥è‡ªå®é™…ç»éªŒï¼šé•¿ä¸Šä¸‹æ–‡ä¼šå¯¼è‡´ LLM "lost in the middle" é—®é¢˜ã€‚

#### 2. **ä¸ºä»€ä¹ˆè®¾ç½® max_tokens=32000ï¼Ÿ**

```python
# âŒ ä½¿ç”¨é»˜è®¤ï¼ˆå¯èƒ½åªæœ‰ 1024ï¼‰
compress_model = init_chat_model("openai:gpt-4.1")
â†’ è¾“å‡ºå¯èƒ½åœ¨ç¬¬ 1024 ä¸ª token è¢«æˆªæ–­
â†’ "Sextant Coffee Ro..." â† ä¿¡æ¯ä¸¢å¤±ï¼

# âœ… æ˜ç¡®è®¾ç½®
compress_model = init_chat_model("openai:gpt-4.1", max_tokens=32000)
â†’ å…è®¸å®Œæ•´è¾“å‡ºï¼ˆGPT-4.1 æœ€å¤§è¾“å‡º 33k tokensï¼‰
```

#### 3. **ä¸ºä»€ä¹ˆåŒæ—¶ä¿ç•™ raw_notesï¼Ÿ**

```python
return {
    "compressed_research": compressed,  # ç”¨äºåç»­ Agent å¤„ç†
    "raw_notes": raw_notes              # ç”¨äºæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ
}
```

**åŸå› ï¼š**
- `compressed_research` - é«˜å±‚æ¬¡æ€»ç»“ï¼Œä¾› supervisor å†³ç­–
- `raw_notes` - å®Œæ•´ç»†èŠ‚ï¼Œä¾›æŠ¥å‘Šç”Ÿæˆä½¿ç”¨
- è¿™æ ·æ—¢èŠ‚çœ tokenï¼Œåˆä¸ä¸¢å¤±ä¿¡æ¯

---

## ğŸ”„ å®Œæ•´æ‰§è¡Œæµç¨‹ç¤ºä¾‹

### ç¤ºä¾‹ï¼šç ”ç©¶æ—§é‡‘å±±å’–å•¡åº—

```python
ç ”ç©¶ä¸»é¢˜è¾“å…¥: "ç ”ç©¶æ—§é‡‘å±±åŸºäºå’–å•¡è´¨é‡çš„æœ€ä½³å’–å•¡åº—..."

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Round 1: åˆå§‹æœç´¢                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM å†³ç­–:                               â”‚
â”‚   "æˆ‘éœ€è¦æœç´¢ SF å’–å•¡åº—çš„ä¸“ä¸šè¯„åˆ†"       â”‚
â”‚                                         â”‚
â”‚ Tool Call 1: tavily_search(             â”‚
â”‚   "San Francisco specialty coffee       â”‚
â”‚    quality ratings Coffee Review 2024"  â”‚
â”‚ )                                       â”‚
â”‚                                         â”‚
â”‚ ç»“æœ: æ‰¾åˆ° Sightglass 94åˆ†, Blue Bottle â”‚
â”‚                                         â”‚
â”‚ Tool Call 2: think_tool(                â”‚
â”‚   "æ‰¾åˆ°äº†ä¸“ä¸šè¯„åˆ†ï¼Œä½†éœ€è¦æ›´å¤šä¾‹å­..."   â”‚
â”‚ )                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Round 2: è¡¥å……æœç´¢                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM å†³ç­–:                               â”‚
â”‚   "éœ€è¦æœç´¢ç”¨æˆ·è¯„ä»·éªŒè¯ä¸“ä¸šè¯„åˆ†"         â”‚
â”‚                                         â”‚
â”‚ Tool Call 3: tavily_search(             â”‚
â”‚   "SF coffee shops Yelp Google quality" â”‚
â”‚ )                                       â”‚
â”‚                                         â”‚
â”‚ ç»“æœ: Ritual 4.4/5, Saint Frank 4.3/5  â”‚
â”‚                                         â”‚
â”‚ Tool Call 4: think_tool(                â”‚
â”‚   "å·²æœ‰ 5 å®¶å’–å•¡åº—çš„ç»¼åˆæ•°æ®ï¼Œ           â”‚
â”‚    åŒ…æ‹¬ä¸“ä¸šè¯„åˆ†å’Œç”¨æˆ·è¯„ä»·ï¼Œè¶³å¤Ÿå›ç­”"     â”‚
â”‚ )                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Round 3: å†³å®šç»“æŸ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LLM å†³ç­–:                               â”‚
â”‚   "æˆ‘æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œæä¾›æœ€ç»ˆç­”æ¡ˆ"           â”‚
â”‚                                         â”‚
â”‚ æ— å·¥å…·è°ƒç”¨ â†’ è§¦å‘ compress_research     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‹ç¼©ç ”ç©¶ç»“æœ                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¾“å…¥: 10,000+ tokens (4 æ¬¡æœç´¢ç»“æœ)     â”‚
â”‚ è¾“å‡º: 2,000 tokens (ç»“æ„åŒ–å‘ç°)         â”‚
â”‚                                         â”‚
â”‚ æ ¸å¿ƒå‘ç°:                               â”‚
â”‚ 1. Sightglass - 94/100 (Coffee Review) â”‚
â”‚ 2. Blue Bottle - 90+/100               â”‚
â”‚ 3. Ritual - 4.4/5 (ç”¨æˆ·è¯„ä»·)           â”‚
â”‚ 4. Saint Frank - 4.3/5                 â”‚
â”‚ 5. Four Barrel - 88-92/100             â”‚
â”‚                                         â”‚
â”‚ è´¨é‡æŒ‡æ ‡: ä¸“ä¸šè¯„åˆ† + ç”¨æˆ·åé¦ˆ + è®¤è¯     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®è§‚å¯Ÿï¼š**
- âœ… åªç”¨äº† 4 æ¬¡å·¥å…·è°ƒç”¨ï¼ˆ2æ¬¡æœç´¢ + 2æ¬¡åæ€ï¼‰
- âœ… æ¯æ¬¡æœç´¢åéƒ½åæ€ï¼Œé¿å…ç›²ç›®ç»§ç»­
- âœ… ç¬¬äºŒæ¬¡åæ€åå†³å®šè¶³å¤Ÿï¼Œæ²¡æœ‰ spin-out
- âœ… å‹ç¼©æ¯”çº¦ 5:1ï¼ˆ10k â†’ 2k tokensï¼‰

---

## ğŸ“Š æ€§èƒ½è€ƒè™‘

### æ¨¡å‹é€‰æ‹©å¯¹æ¯”

| ç»´åº¦ | GPT-4.1 | Claude Sonnet 4 |
|------|---------|-----------------|
| **å‹ç¼©å»¶è¿Ÿ** | 38s | 99s |
| **è¾“å‡ºè´¨é‡** | ä¼˜ç§€ | ä¼˜ç§€ |
| **æœ€å¤§è¾“å‡º** | 33k tokens | 64k tokens |
| **æˆæœ¬** | ä¸­ç­‰ | è¾ƒé«˜ |
| **æ¨èç”¨é€”** | å‹ç¼©èŠ‚ç‚¹ | LLM å†³ç­–èŠ‚ç‚¹ |

**å®è·µå»ºè®®ï¼š**
- Agent å†³ç­–ï¼ˆéœ€è¦æ¨ç†ï¼‰â†’ Claude Sonnet 4
- å‹ç¼©èŠ‚ç‚¹ï¼ˆéœ€è¦é€Ÿåº¦ï¼‰â†’ GPT-4.1
- ç½‘é¡µæ‘˜è¦ï¼ˆç®€å•ä»»åŠ¡ï¼‰â†’ GPT-4.1-mini

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. Prompt è®¾è®¡çš„é»„é‡‘æ³•åˆ™

```python
"""
âœ… DO: Think like a human researcher with limited time
âœ… DO: Provide concrete heuristics (2-3 for simple, 5 for complex)
âœ… DO: Use think_tool after each search
âœ… DO: Stop when you have 3+ relevant sources

âŒ DON'T: Search for perfection
âŒ DON'T: Repeat similar queries
âŒ DON'T: Ignore the hard limits
"""
```

### 2. å·¥å…·è°ƒç”¨é¢„ç®—

```python
# åœ¨çŠ¶æ€ä¸­è·Ÿè¸ªè¿­ä»£æ¬¡æ•°
class ResearcherState(TypedDict):
    tool_call_iterations: int  # è®¡æ•°å™¨

# åœ¨èŠ‚ç‚¹ä¸­æ£€æŸ¥
def llm_call(state):
    if state["tool_call_iterations"] >= MAX_ITERATIONS:
        # å¼ºåˆ¶ç»“æŸ
        return {"messages": [AIMessage("è¾¾åˆ°æœç´¢é™åˆ¶ï¼ŒåŸºäºç°æœ‰ä¿¡æ¯å›ç­”...")]}
```

### 3. é”™è¯¯å¤„ç†

```python
def tool_node(state):
    try:
        # æ‰§è¡Œå·¥å…·
        observations = [tool.invoke(args) for tool in tool_calls]
    except Exception as e:
        # é™çº§ç­–ç•¥
        return {
            "researcher_messages": [
                ToolMessage(
                    content=f"æœç´¢å¤±è´¥: {e}. åŸºäºç°æœ‰ä¿¡æ¯ç»§ç»­.",
                    tool_call_id=tool_call["id"]
                )
            ]
        }
```

---

## ğŸ“ æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

### Agent å¾ªç¯æ¨¡å¼

```python
while True:
    decision = llm_call(state)

    if no_tool_calls(decision):
        break  # ç»“æŸå¾ªç¯

    results = execute_tools(decision)
    state = update_state(results)
```

### Context Engineering ä¸¤å±‚å‹ç¼©

1. **ç½‘é¡µçº§** - åŸå§‹å†…å®¹ â†’ æ‘˜è¦ï¼ˆ20:1ï¼‰
2. **ç ”ç©¶çº§** - å¤šè½®ç»“æœ â†’ æ ¸å¿ƒå‘ç°ï¼ˆ5:1ï¼‰
3. **æ€»å‹ç¼©æ¯”** - çº¦ 100:1

### think_tool çš„ä½œç”¨

- å¼ºåˆ¶æš‚åœå’Œåæ€
- é¿å… spin-out
- æé«˜å†³ç­–è´¨é‡

### Prompt Engineering æ ¸å¿ƒ

- åƒäººç±»ç ”ç©¶å‘˜ä¸€æ ·æ€è€ƒ
- å…·ä½“çš„å¯å‘å¼è§„åˆ™
- ç¡¬æ€§é™åˆ¶é˜²æ­¢å¤±æ§

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬èŠ‚ï¼Œä½ å·²ç»æŒæ¡äº† Research Agent çš„æ ¸å¿ƒæœºåˆ¶ã€‚

**ä¸‹ä¸€ç« ï¼š9.3 MCP é›†æˆ** - å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Model Context Protocol æ‰©å±• Agent çš„å·¥å…·èƒ½åŠ›ï¼Œè®¿é—®æ–‡ä»¶ç³»ç»Ÿã€æ•°æ®åº“ç­‰å¤–éƒ¨èµ„æºï¼
