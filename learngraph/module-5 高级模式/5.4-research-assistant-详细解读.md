# LangGraph ç ”ç©¶åŠ©æ‰‹æ¡ˆä¾‹è¯¦ç»†è§£è¯»

## ğŸ“‹ æ¡ˆä¾‹æ¦‚è§ˆ

è¿™ä¸ªæ¡ˆä¾‹å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangGraph æ„å»ºä¸€ä¸ª**æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿ**ï¼Œå®ƒèƒ½å¤Ÿï¼š

1. æ ¹æ®ç ”ç©¶ä¸»é¢˜è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªä¸“ä¸šåˆ†æå¸ˆ
2. æ¯ä¸ªåˆ†æå¸ˆå¯¹ç‰¹å®šå­ä¸»é¢˜è¿›è¡Œæ·±åº¦è®¿è°ˆ
3. å¹¶è¡Œæ‰§è¡Œå¤šä¸ªè®¿è°ˆä»¥æé«˜æ•ˆç‡
4. æœ€ç»ˆå°†æ‰€æœ‰è®¿è°ˆæ•´åˆæˆä¸€ä»½å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Š

---

## ğŸ“š æœ¯è¯­è¡¨

| æœ¯è¯­åç§° | LangGraph å®šä¹‰å’Œè§£è¯» | Python å®šä¹‰å’Œè¯´æ˜ | é‡è¦ç¨‹åº¦ |
|---------|---------------------|------------------|---------|
| **Human-in-the-Loop** | åœ¨å…³é”®å†³ç­–ç‚¹äººå·¥ä»‹å…¥å®¡æ ¸å’Œä¿®æ”¹çš„æœºåˆ¶ | é€šè¿‡ `interrupt_before` å‚æ•°å®ç°æ‰§è¡Œæš‚åœï¼Œ`update_state()` ä¿®æ”¹çŠ¶æ€ | â­â­â­â­â­ |
| **Send API** | åŠ¨æ€åˆ›å»ºå¹¶è¡Œä»»åŠ¡çš„æ ¸å¿ƒå·¥å…·ï¼Œ`Send(èŠ‚ç‚¹å, çŠ¶æ€)` | æ ¹æ®è¿è¡Œæ—¶æ•°æ®ä¸ºæ¯ä¸ªåˆ†æå¸ˆåˆ›å»ºç‹¬ç«‹çš„è®¿è°ˆå­å›¾å®ä¾‹ | â­â­â­â­â­ |
| **Checkpointerï¼ˆæ£€æŸ¥ç‚¹ï¼‰** | ä¿å­˜æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡ŒåçŠ¶æ€çš„æœºåˆ¶ | `from langgraph.checkpoint.memory import MemorySaver`ï¼Œæ”¯æŒä¸­æ–­æ¢å¤ | â­â­â­â­â­ |
| **interrupt_before** | åœ¨æŒ‡å®šèŠ‚ç‚¹å‰æš‚åœæ‰§è¡Œçš„ç¼–è¯‘å‚æ•° | `graph.compile(interrupt_before=['èŠ‚ç‚¹å'])`ï¼Œå®ç°äººå·¥å®¡æ ¸ç‚¹ | â­â­â­â­â­ |
| **Pydantic BaseModel** | å®šä¹‰ç»“æ„åŒ–æ•°æ®æ¨¡å‹å¹¶æ”¯æŒéªŒè¯çš„ç±» | ç¡®ä¿ LLM è¾“å‡ºæ ¼å¼ä¸€è‡´ï¼Œ`with_structured_output(Model)` ä½¿ç”¨ | â­â­â­â­â­ |
| **Sub-graph åµŒå¥—** | å°†ç¼–è¯‘åçš„å­å›¾ä½œä¸ºèŠ‚ç‚¹æ·»åŠ åˆ°ä¸»å›¾ | `builder.add_node("åç§°", sub_graph.compile())`ï¼Œå®ç°æ¨¡å—åŒ– | â­â­â­â­â­ |
| **Map-Reduce æ¨¡å¼** | Map é˜¶æ®µå¹¶è¡Œæ‰§è¡Œè®¿è°ˆï¼ŒReduce é˜¶æ®µæ±‡æ€»æŠ¥å‘Šçš„æ¶æ„ | Send åˆ›å»ºå¤šä¸ªè®¿è°ˆå®ä¾‹ï¼Œoperator.add èšåˆ sections | â­â­â­â­â­ |
| **RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** | ç»“åˆæœç´¢ç»“æœï¼ˆcontextï¼‰ç”Ÿæˆç­”æ¡ˆçš„æ¨¡å¼ | search_web + search_wikipedia æä¾›äº‹å®ä¾æ®ï¼ŒLLM åŸºäº context å›ç­” | â­â­â­â­â­ |
| **MessagesState** | LangGraph å†…ç½®çŠ¶æ€ç±»ï¼ŒåŒ…å« messages å­—æ®µç®¡ç†å¯¹è¯å†å² | `from langgraph.graph import MessagesState`ï¼Œè‡ªåŠ¨å¤„ç†æ¶ˆæ¯åˆ—è¡¨ | â­â­â­â­ |
| **route_messages** | æ§åˆ¶å¯¹è¯å¾ªç¯çš„è·¯ç”±å‡½æ•° | æ£€æŸ¥è½®æ•°å’Œç»“æŸä¿¡å·ï¼Œå†³å®šç»§ç»­æé—®æˆ–ç»“æŸè®¿è°ˆ | â­â­â­â­ |
| **thread_id** | æ ‡è¯†ä¼šè¯çš„å”¯ä¸€ IDï¼Œç”¨äº checkpointer ä¿å­˜å’Œæ¢å¤çŠ¶æ€ | `{"configurable": {"thread_id": "1"}}`ï¼Œå®ç°å¤šä¼šè¯éš”ç¦» | â­â­â­â­ |
| **update_state()** | åœ¨ä¸­æ–­ç‚¹ä¿®æ”¹å›¾çŠ¶æ€çš„æ–¹æ³• | `graph.update_state(thread, {å­—æ®µ: å€¼}, as_node="èŠ‚ç‚¹å")` | â­â­â­â­ |

---

### æ ¸å¿ƒæŠ€æœ¯ç‚¹

- **äººæœºåä½œ**ï¼ˆHuman-in-the-Loopï¼‰ï¼šç”¨æˆ·å¯ä»¥å®¡æ ¸å¹¶ä¿®æ”¹ç”Ÿæˆçš„åˆ†æå¸ˆå›¢é˜Ÿ
- **ç»“æ„åŒ–è¾“å‡º**ï¼šä½¿ç”¨ Pydantic æ¨¡å‹ç¡®ä¿æ•°æ®æ ¼å¼ä¸€è‡´
- **å­å›¾åµŒå¥—**ï¼šè®¿è°ˆæµç¨‹ä½œä¸ºç‹¬ç«‹å­å›¾åµŒå…¥ä¸»å›¾
- **å¹¶è¡Œæ‰§è¡Œ**ï¼ˆMap-Reduceï¼‰ï¼šä½¿ç”¨ `Send()` API åŒæ—¶è¿è¡Œå¤šä¸ªè®¿è°ˆ
- **æ£€æŸ¥ç‚¹æœºåˆ¶**ï¼šæ”¯æŒä¸­æ–­å’Œæ¢å¤æ‰§è¡Œæµç¨‹

---

## ğŸ—ºï¸ æ•´ä½“æ¶æ„å›¾

![Research Assistant Architecture](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)

```

**ç”Ÿæˆçš„æµç¨‹å›¾ï¼š**

![Flow Diagram](images/research-assistant-output-10-0.jpg)

ç”¨æˆ·è¾“å…¥ç ”ç©¶ä¸»é¢˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆåˆ†æå¸ˆå›¢é˜Ÿ                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  create_analysts â†’ human_feedback       â”‚
â”‚         â†‘                â†“               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚      ï¼ˆå¯å¾ªç¯ä¿®æ”¹ç›´åˆ°æ»¡æ„ï¼‰               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬äºŒé˜¶æ®µï¼šå¹¶è¡Œæ‰§è¡Œè®¿è°ˆï¼ˆMapï¼‰           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åˆ†æå¸ˆ1 â†’ è®¿è°ˆå­å›¾ â†’ ç”Ÿæˆç« èŠ‚           â”‚
â”‚  åˆ†æå¸ˆ2 â†’ è®¿è°ˆå­å›¾ â†’ ç”Ÿæˆç« èŠ‚           â”‚
â”‚  åˆ†æå¸ˆ3 â†’ è®¿è°ˆå­å›¾ â†’ ç”Ÿæˆç« èŠ‚           â”‚
â”‚         ï¼ˆæ‰€æœ‰è®¿è°ˆåŒæ—¶è¿›è¡Œï¼‰             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¬¬ä¸‰é˜¶æ®µï¼šæ±‡æ€»æŠ¥å‘Šï¼ˆReduceï¼‰            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  write_introduction â”                   â”‚
â”‚  write_report       â”œâ†’ finalize_report  â”‚
â”‚  write_conclusion   â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
  æœ€ç»ˆæŠ¥å‘Š
```

---

## ğŸ“š ä»£ç åˆ†æ®µè¯¦è§£

### ç¬¬ä¸€éƒ¨åˆ†ï¼šå®šä¹‰æ•°æ®ç»“æ„

```python
class Analyst(BaseModel):
    affiliation: str  # æ‰€å±æœºæ„
    name: str         # å§“å
    role: str         # è§’è‰²
    description: str  # æè¿°
```

**ä½œç”¨**ï¼šè¿™æ˜¯ä¸€ä¸ª Pydantic æ¨¡å‹ï¼Œå®šä¹‰äº†"åˆ†æå¸ˆ"çš„æ ‡å‡†æ ¼å¼ã€‚

**ä¸ºä»€ä¹ˆé‡è¦**ï¼Ÿ
- ä½¿ç”¨ç»“æ„åŒ–æ•°æ®æ¨¡å‹å¯ä»¥ç¡®ä¿ LLM è¾“å‡ºçš„æ ¼å¼ä¸€è‡´
- `@property` è£…é¥°å™¨åˆ›å»ºäº† `persona` å±æ€§ï¼Œå°†æ‰€æœ‰ä¿¡æ¯ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²
- LangGraph å¯ä»¥åŸºäºè¿™ä¸ªæ¨¡å‹è‡ªåŠ¨éªŒè¯æ•°æ®

---

### ç¬¬äºŒéƒ¨åˆ†ï¼šç”Ÿæˆåˆ†æå¸ˆï¼ˆç¬¬ä¸€ä¸ªå›¾ï¼‰

#### çŠ¶æ€å®šä¹‰

```python
class GenerateAnalystsState(TypedDict):
    topic: str                    # ç ”ç©¶ä¸»é¢˜
    max_analysts: int             # æœ€å¤§åˆ†æå¸ˆæ•°é‡
    human_analyst_feedback: str   # ç”¨æˆ·åé¦ˆ
    analysts: List[Analyst]       # ç”Ÿæˆçš„åˆ†æå¸ˆåˆ—è¡¨
```

**å…³é”®æ¦‚å¿µ**ï¼šçŠ¶æ€ï¼ˆStateï¼‰æ˜¯ LangGraph ä¸­çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå®ƒåƒä¸€ä¸ª"å…±äº«å†…å­˜"ï¼Œåœ¨å„ä¸ªèŠ‚ç‚¹ä¹‹é—´ä¼ é€’æ•°æ®ã€‚

---

#### èŠ‚ç‚¹å‡½æ•°

**1. create_analysts èŠ‚ç‚¹**

```python
def create_analysts(state: GenerateAnalystsState):
    # ä½¿ç”¨ structured_output ç¡®ä¿ LLM è¿”å›ç¬¦åˆ Perspectives æ ¼å¼çš„æ•°æ®
    structured_llm = llm.with_structured_output(Perspectives)

    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    system_message = analyst_instructions.format(
        topic=topic,
        human_analyst_feedback=human_analyst_feedback,
        max_analysts=max_analysts
    )

    # è°ƒç”¨ LLM ç”Ÿæˆåˆ†æå¸ˆ
    analysts = structured_llm.invoke([SystemMessage(...)])

    return {"analysts": analysts.analysts}
```

**æŠ€æœ¯äº®ç‚¹**ï¼š
- `with_structured_output()` å¼ºåˆ¶ LLM è¿”å›ç¬¦åˆ `Perspectives` æ¨¡å‹çš„ JSON
- æç¤ºè¯ä¸­åŒ…å«äº†ä¸»é¢˜ã€ç”¨æˆ·åé¦ˆå’Œæ•°é‡é™åˆ¶
- è¿”å›çš„å­—å…¸ä¼š**æ›´æ–°**çŠ¶æ€ä¸­çš„ `analysts` å­—æ®µ

---

**2. human_feedback èŠ‚ç‚¹**

```python
def human_feedback(state: GenerateAnalystsState):
    pass  # ç©ºæ“ä½œèŠ‚ç‚¹
```

**ä¸ºä»€ä¹ˆéœ€è¦ç©ºèŠ‚ç‚¹ï¼Ÿ**
- è¿™æ˜¯ä¸€ä¸ª"ä¸­æ–­ç‚¹"ï¼ˆinterrupt pointï¼‰
- å½“æ‰§è¡Œåˆ°è¿™é‡Œæ—¶ï¼Œå›¾ä¼šæš‚åœï¼Œç­‰å¾…ç”¨æˆ·è¾“å…¥
- ç”¨æˆ·å¯ä»¥é€šè¿‡ `graph.update_state()` ä¿®æ”¹çŠ¶æ€

---

**3. should_continue æ¡ä»¶è¾¹**

```python
def should_continue(state: GenerateAnalystsState):
    human_analyst_feedback = state.get('human_analyst_feedback', None)

    if human_analyst_feedback:
        return "create_analysts"  # æœ‰åé¦ˆï¼Œé‡æ–°ç”Ÿæˆ

    return END  # æ— åé¦ˆï¼Œç»“æŸ
```

**é€»è¾‘è¯´æ˜**ï¼š
- æ£€æŸ¥çŠ¶æ€ä¸­æ˜¯å¦æœ‰ `human_analyst_feedback`
- å¦‚æœæœ‰ â†’ å›åˆ° `create_analysts` é‡æ–°ç”Ÿæˆ
- å¦‚æœæ²¡æœ‰ â†’ ç»“æŸè¿™ä¸ªé˜¶æ®µ

---

#### å›¾çš„æ„å»º

```python
builder = StateGraph(GenerateAnalystsState)
builder.add_node("create_analysts", create_analysts)
builder.add_node("human_feedback", human_feedback)

# è¾¹çš„è¿æ¥
builder.add_edge(START, "create_analysts")
builder.add_edge("create_analysts", "human_feedback")
builder.add_conditional_edges("human_feedback", should_continue,
                             ["create_analysts", END])

# ç¼–è¯‘å›¾
graph = builder.compile(
    interrupt_before=['human_feedback'],  # åœ¨æ­¤èŠ‚ç‚¹å‰ä¸­æ–­
    checkpointer=memory                   # ä½¿ç”¨å†…å­˜ä¿å­˜çŠ¶æ€
)
```

**å…³é”®æœºåˆ¶**ï¼š
- `interrupt_before` ä½¿å›¾åœ¨ `human_feedback` å‰æš‚åœ
- `checkpointer` ä¿å­˜æ‰§è¡ŒçŠ¶æ€ï¼Œæ”¯æŒæ¢å¤
- `conditional_edges` æ ¹æ®å‡½æ•°è¿”å›å€¼å†³å®šä¸‹ä¸€æ­¥

---

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰§è¡Œè®¿è°ˆï¼ˆå­å›¾ï¼‰

#### è®¿è°ˆçŠ¶æ€

```python
class InterviewState(MessagesState):
    max_num_turns: int      # æœ€å¤§å¯¹è¯è½®æ•°
    context: Annotated[list, operator.add]  # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    analyst: Analyst        # å½“å‰åˆ†æå¸ˆ
    interview: str          # è®¿è°ˆè®°å½•
    sections: list          # ç”Ÿæˆçš„ç« èŠ‚
```

**æ³¨æ„**ï¼š`context: Annotated[list, operator.add]`
- `Annotated` æŒ‡å®šäº†å¦‚ä½•åˆå¹¶çŠ¶æ€
- `operator.add` è¡¨ç¤ºæ–°çš„ context ä¼šè¿½åŠ åˆ°ç°æœ‰åˆ—è¡¨ä¸­
- è¿™æ ·å¤šä¸ªæœç´¢ç»“æœå¯ä»¥ç´¯ç§¯

---

#### æ ¸å¿ƒèŠ‚ç‚¹

**1. generate_question - åˆ†æå¸ˆæé—®**

```python
def generate_question(state: InterviewState):
    analyst = state["analyst"]
    messages = state["messages"]

    # æ„å»ºç³»ç»Ÿæç¤º
    system_message = question_instructions.format(goals=analyst.persona)

    # LLM æ ¹æ®åˆ†æå¸ˆçš„è§’è‰²ç”Ÿæˆé—®é¢˜
    question = llm.invoke([SystemMessage(content=system_message)] + messages)

    return {"messages": [question]}
```

**å·¥ä½œæµç¨‹**ï¼š
1. è·å–åˆ†æå¸ˆçš„äººè®¾ï¼ˆpersonaï¼‰
2. ç»“åˆå¯¹è¯å†å²ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜
3. é—®é¢˜ä¼šè¿½åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­

---

**2. search_web å’Œ search_wikipedia - å¹¶è¡Œæœç´¢**

```python
def search_web(state: InterviewState):
    # å°†å¯¹è¯è½¬æ¢ä¸ºæœç´¢æŸ¥è¯¢
    structured_llm = llm.with_structured_output(SearchQuery)
    search_query = structured_llm.invoke([search_instructions] + state['messages'])

    # ä½¿ç”¨ Tavily æœç´¢
    search_docs = tavily_search.invoke(search_query.search_query)

    # æ ¼å¼åŒ–ç»“æœ
    formatted_search_docs = "\n\n---\n\n".join([
        f'<Document href="{doc["url"]}"/>\n{doc["content"]}\n</Document>'
        for doc in search_docs
    ])

    return {"context": [formatted_search_docs]}
```

**æŠ€æœ¯ç»†èŠ‚**ï¼š
- å…ˆç”¨ LLM å°†å¯¹è¯æç‚¼æˆæœç´¢æŸ¥è¯¢
- è°ƒç”¨å¤–éƒ¨æœç´¢å·¥å…·ï¼ˆTavily/Wikipediaï¼‰
- æ ¼å¼åŒ–æˆå¸¦æ¥æºæ ‡æ³¨çš„æ–‡æ¡£
- è¿”å›çš„ context ä¼šè‡ªåŠ¨åˆå¹¶åˆ°çŠ¶æ€ä¸­

---

**3. generate_answer - ä¸“å®¶å›ç­”**

```python
def generate_answer(state: InterviewState):
    analyst = state["analyst"]
    messages = state["messages"]
    context = state["context"]  # æ£€ç´¢åˆ°çš„æ‰€æœ‰æ–‡æ¡£

    # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤º
    system_message = answer_instructions.format(
        goals=analyst.persona,
        context=context
    )

    # ç”Ÿæˆç­”æ¡ˆ
    answer = llm.invoke([SystemMessage(content=system_message)] + messages)
    answer.name = "expert"  # æ ‡è®°æ¶ˆæ¯æ¥æº

    return {"messages": [answer]}
```

**RAG æ¨¡å¼**ï¼š
- ä½¿ç”¨æ£€ç´¢åˆ°çš„ context ä½œä¸ºçŸ¥è¯†æ¥æº
- LLM åŸºäºè¿™äº›æ–‡æ¡£å›ç­”é—®é¢˜
- ç­”æ¡ˆä¸­åŒ…å«å¼•ç”¨æ ‡æ³¨ï¼ˆå¦‚ [1], [2]ï¼‰

---

**4. route_messages - è·¯ç”±é€»è¾‘**

```python
def route_messages(state: InterviewState, name: str = "expert"):
    messages = state["messages"]
    max_num_turns = state.get('max_num_turns', 2)

    # ç»Ÿè®¡ä¸“å®¶å›ç­”çš„æ¬¡æ•°
    num_responses = len([
        m for m in messages
        if isinstance(m, AIMessage) and m.name == name
    ])

    # è¾¾åˆ°ä¸Šé™åˆ™ä¿å­˜è®¿è°ˆ
    if num_responses >= max_num_turns:
        return 'save_interview'

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ç»“æŸè¯­
    last_question = messages[-2]
    if "Thank you so much for your help" in last_question.content:
        return 'save_interview'

    return "ask_question"  # ç»§ç»­æé—®
```

**æ§åˆ¶é€»è¾‘**ï¼š
- é™åˆ¶è®¿è°ˆè½®æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
- æ£€æµ‹ç»“æŸä¿¡å·ï¼ˆæ„Ÿè°¢è¯­ï¼‰
- å†³å®šæ˜¯ç»§ç»­æé—®è¿˜æ˜¯ç»“æŸè®¿è°ˆ

---

**5. write_section - ç”Ÿæˆç« èŠ‚**

```python
def write_section(state: InterviewState):
    interview = state["interview"]
    context = state["context"]
    analyst = state["analyst"]

    system_message = section_writer_instructions.format(
        focus=analyst.description
    )

    section = llm.invoke([
        SystemMessage(content=system_message),
        HumanMessage(content=f"Use this source to write your section: {context}")
    ])

    return {"sections": [section.content]}
```

**æŠ¥å‘Šç”Ÿæˆ**ï¼š
- åŸºäºè®¿è°ˆå†…å®¹å’Œæ£€ç´¢æ–‡æ¡£
- ç”Ÿæˆç»“æ„åŒ–çš„ Markdown ç« èŠ‚
- åŒ…å«æ‘˜è¦å’Œæ¥æºå¼•ç”¨

---

#### å­å›¾æ„å»º

```python
interview_builder = StateGraph(InterviewState)

# æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
interview_builder.add_node("ask_question", generate_question)
interview_builder.add_node("search_web", search_web)
interview_builder.add_node("search_wikipedia", search_wikipedia)
interview_builder.add_node("answer_question", generate_answer)
interview_builder.add_node("save_interview", save_interview)
interview_builder.add_node("write_section", write_section)

# å…³é”®ï¼šå¹¶è¡Œæœç´¢
interview_builder.add_edge(START, "ask_question")
interview_builder.add_edge("ask_question", "search_web")
interview_builder.add_edge("ask_question", "search_wikipedia")  # åŒæ—¶è§¦å‘
interview_builder.add_edge("search_web", "answer_question")
interview_builder.add_edge("search_wikipedia", "answer_question")

# æ¡ä»¶è·¯ç”±
interview_builder.add_conditional_edges("answer_question", route_messages,
                                       ['ask_question', 'save_interview'])

interview_builder.add_edge("save_interview", "write_section")
interview_builder.add_edge("write_section", END)
```

**æ‰§è¡Œæµç¨‹**ï¼š
```
ask_question
    â”œâ”€â†’ search_web â”€â”€â”
    â””â”€â†’ search_wikipedia â”€â”€â”¤
                           â†“
                    answer_question
                           â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â†“                              â†“
       ask_question                 save_interview
      ï¼ˆç»§ç»­å¾ªç¯ï¼‰                        â†“
                                   write_section
                                        â†“
                                       END
```

---

### ç¬¬å››éƒ¨åˆ†ï¼šä¸»å›¾ - Map-Reduce æ¨¡å¼

#### ä¸»çŠ¶æ€

```python
class ResearchGraphState(TypedDict):
    topic: str
    max_analysts: int
    human_analyst_feedback: str
    analysts: List[Analyst]
    sections: Annotated[list, operator.add]  # ç´¯ç§¯æ‰€æœ‰ç« èŠ‚
    introduction: str
    content: str
    conclusion: str
    final_report: str
```

---

#### initiate_all_interviews - Map æ­¥éª¤

```python
def initiate_all_interviews(state: ResearchGraphState):
    human_analyst_feedback = state.get('human_analyst_feedback')

    if human_analyst_feedback:
        return "create_analysts"  # è¿”å›ä¿®æ”¹åˆ†æå¸ˆ

    # å…³é”®ï¼šä½¿ç”¨ Send() API å¹¶è¡Œå¯åŠ¨å¤šä¸ªå­å›¾
    else:
        topic = state["topic"]
        return [
            Send("conduct_interview", {
                "analyst": analyst,
                "messages": [HumanMessage(
                    content=f"So you said you were writing an article on {topic}?"
                )]
            })
            for analyst in state["analysts"]
        ]
```

**Send() API çš„é­”åŠ›**ï¼š
- ä¸ºæ¯ä¸ªåˆ†æå¸ˆåˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„è®¿è°ˆå­å›¾å®ä¾‹
- æ‰€æœ‰è®¿è°ˆ**å¹¶è¡Œæ‰§è¡Œ**
- æ¯ä¸ªå­å›¾çš„ `sections` è¾“å‡ºä¼šè‡ªåŠ¨åˆå¹¶åˆ°ä¸»çŠ¶æ€

**ç±»æ¯”**ï¼š
- å°±åƒä¸€ä¸ªç»ç†åŒæ—¶æ´¾é£å¤šä¸ªå‘˜å·¥å»ä¸åŒåœ°ç‚¹è°ƒç ”
- æ¯ä¸ªå‘˜å·¥ç‹¬ç«‹å·¥ä½œï¼Œæœ€åæ±‡æŠ¥ç»“æœ
- ç»ç†æ•´åˆæ‰€æœ‰æŠ¥å‘Š

---

#### Reduce æ­¥éª¤

**1. write_report - æ•´åˆç« èŠ‚**

```python
def write_report(state: ResearchGraphState):
    sections = state["sections"]  # åŒ…å«æ‰€æœ‰åˆ†æå¸ˆçš„ç« èŠ‚
    topic = state["topic"]

    # å°†æ‰€æœ‰ç« èŠ‚åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])

    # LLM æ•´åˆæˆè¿è´¯çš„æŠ¥å‘Šä¸»ä½“
    system_message = report_writer_instructions.format(
        topic=topic,
        context=formatted_str_sections
    )

    report = llm.invoke([
        SystemMessage(content=system_message),
        HumanMessage(content="Write a report based upon these memos.")
    ])

    return {"content": report.content}
```

---

**2. write_introduction / write_conclusion**

```python
def write_introduction(state: ResearchGraphState):
    sections = state["sections"]
    formatted_str_sections = "\n\n".join([f"{section}" for section in sections])

    instructions = intro_conclusion_instructions.format(
        topic=topic,
        formatted_str_sections=formatted_str_sections
    )

    intro = llm.invoke([instructions, HumanMessage(content="Write the report introduction")])

    return {"introduction": intro.content}
```

---

**3. finalize_report - ç»„è£…æœ€ç»ˆæŠ¥å‘Š**

```python
def finalize_report(state: ResearchGraphState):
    content = state["content"]

    # æ¸…ç†å’Œåˆ†ç¦»æ¥æºéƒ¨åˆ†
    if content.startswith("## Insights"):
        content = content.strip("## Insights")

    if "## Sources" in content:
        content, sources = content.split("\n## Sources\n")

    # ç»„è£…å®Œæ•´æŠ¥å‘Š
    final_report = (
        state["introduction"] + "\n\n---\n\n" +
        content + "\n\n---\n\n" +
        state["conclusion"]
    )

    if sources:
        final_report += "\n\n## Sources\n" + sources

    return {"final_report": final_report}
```

---

#### ä¸»å›¾æ„å»º

```python
builder = StateGraph(ResearchGraphState)

# ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆåˆ†æå¸ˆ
builder.add_node("create_analysts", create_analysts)
builder.add_node("human_feedback", human_feedback)

# ç¬¬äºŒé˜¶æ®µï¼šæ‰§è¡Œè®¿è°ˆï¼ˆå­å›¾ï¼‰
builder.add_node("conduct_interview", interview_builder.compile())

# ç¬¬ä¸‰é˜¶æ®µï¼šç”ŸæˆæŠ¥å‘Š
builder.add_node("write_report", write_report)
builder.add_node("write_introduction", write_introduction)
builder.add_node("write_conclusion", write_conclusion)
builder.add_node("finalize_report", finalize_report)

# æµç¨‹æ§åˆ¶
builder.add_edge(START, "create_analysts")
builder.add_edge("create_analysts", "human_feedback")
builder.add_conditional_edges("human_feedback", initiate_all_interviews,
                             ["create_analysts", "conduct_interview"])

# å…³é”®ï¼šå¹¶è¡Œç”Ÿæˆä»‹ç»ã€ç»“è®ºå’Œä¸»ä½“
builder.add_edge("conduct_interview", "write_report")
builder.add_edge("conduct_interview", "write_introduction")
builder.add_edge("conduct_interview", "write_conclusion")

# æ±‡æ€»
builder.add_edge(["write_conclusion", "write_report", "write_introduction"],
                "finalize_report")
builder.add_edge("finalize_report", END)

# ç¼–è¯‘
graph = builder.compile(
    interrupt_before=['human_feedback'],
    checkpointer=memory
)
```

**ç”Ÿæˆçš„æµç¨‹å›¾ï¼š**

![Flow Diagram](images/research-assistant-output-28-0.jpg)


---

## ğŸ”‘ æ ¸å¿ƒæŠ€æœ¯è§£æ

### 1. äººæœºåä½œï¼ˆHuman-in-the-Loopï¼‰

**å®ç°æ–¹å¼**ï¼š
```python
graph = builder.compile(interrupt_before=['human_feedback'], checkpointer=memory)
```

**å·¥ä½œæµç¨‹**ï¼š
1. å›¾æ‰§è¡Œåˆ° `human_feedback` å‰æš‚åœ
2. ç”¨æˆ·é€šè¿‡ `graph.update_state()` ä¿®æ”¹çŠ¶æ€
3. ç»§ç»­æ‰§è¡Œ `graph.stream(None, thread)`

**å®é™…åº”ç”¨**ï¼š
```python
# ç¬¬ä¸€æ¬¡è¿è¡Œ
for event in graph.stream(initial_input, thread):
    pass  # åœ¨ human_feedback å‰åœæ­¢

# ç”¨æˆ·å®¡æ ¸å¹¶åé¦ˆ
graph.update_state(thread, {
    "human_analyst_feedback": "Add a startup CEO"
}, as_node="human_feedback")

# ç»§ç»­è¿è¡Œ
for event in graph.stream(None, thread):
    pass  # é‡æ–°ç”Ÿæˆåˆ†æå¸ˆ
```

---

### 2. Send() API - åŠ¨æ€å¹¶è¡Œ

**ä¼ ç»Ÿæ–¹å¼ vs Send()**ï¼š

ä¼ ç»Ÿæ–¹å¼ï¼ˆé™æ€ï¼‰ï¼š
```python
builder.add_edge("node_a", "node_b")
builder.add_edge("node_a", "node_c")
# å¿…é¡»é¢„å…ˆçŸ¥é“æœ‰å¤šå°‘ä¸ªèŠ‚ç‚¹
```

Send() æ–¹å¼ï¼ˆåŠ¨æ€ï¼‰ï¼š
```python
def dynamic_dispatch(state):
    return [
        Send("subgraph", {"input": item})
        for item in state["items"]  # æ•°é‡åœ¨è¿è¡Œæ—¶ç¡®å®š
    ]

builder.add_conditional_edges("dispatcher", dynamic_dispatch)
```

**æœ¬æ¡ˆä¾‹åº”ç”¨**ï¼š
- åˆ†æå¸ˆæ•°é‡ç”±ç”¨æˆ·è¾“å…¥å’Œ LLM å†³å®š
- æ¯ä¸ªåˆ†æå¸ˆå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çš„è®¿è°ˆå­å›¾å®ä¾‹
- æ‰€æœ‰è®¿è°ˆåŒæ—¶è¿è¡Œï¼Œå¤§å¹…æé«˜æ•ˆç‡

---

### 3. å­å›¾ï¼ˆSubgraphï¼‰

**ä¸ºä»€ä¹ˆä½¿ç”¨å­å›¾ï¼Ÿ**
- å°è£…å¤æ‚é€»è¾‘ï¼ˆè®¿è°ˆæµç¨‹ï¼‰
- å¯å¤ç”¨ï¼ˆæ¯ä¸ªåˆ†æå¸ˆå¤ç”¨ç›¸åŒæµç¨‹ï¼‰
- çŠ¶æ€éš”ç¦»ï¼ˆæ¯ä¸ªè®¿è°ˆæœ‰ç‹¬ç«‹çš„æ¶ˆæ¯å†å²ï¼‰

**é›†æˆæ–¹å¼**ï¼š
```python
# å­å›¾
interview_graph = interview_builder.compile()

# åµŒå…¥ä¸»å›¾
builder.add_node("conduct_interview", interview_graph)
```

**ç”Ÿæˆçš„æµç¨‹å›¾ï¼š**

![Flow Diagram](images/research-assistant-output-34-0.jpg)


**çŠ¶æ€ä¼ é€’**ï¼š
- ä¸»å›¾é€šè¿‡ `Send()` ä¼ é€’åˆå§‹çŠ¶æ€ç»™å­å›¾
- å­å›¾çš„ `sections` è¾“å‡ºè‡ªåŠ¨åˆå¹¶åˆ°ä¸»å›¾çŠ¶æ€

---

### 4. ç»“æ„åŒ–è¾“å‡º

**Pydantic æ¨¡å‹ + with_structured_output()**ï¼š

```python
class Analyst(BaseModel):
    name: str
    role: str
    # ... å…¶ä»–å­—æ®µ

structured_llm = llm.with_structured_output(Perspectives)
result = structured_llm.invoke(messages)  # è‡ªåŠ¨éªŒè¯å’Œè§£æ
```

**ä¼˜åŠ¿**ï¼š
- ä¿è¯æ•°æ®æ ¼å¼ä¸€è‡´
- è‡ªåŠ¨ç±»å‹æ£€æŸ¥
- æ˜“äºåç»­å¤„ç†

---

### 5. æ£€æŸ¥ç‚¹ï¼ˆCheckpointerï¼‰

**ä½œç”¨**ï¼š
- ä¿å­˜æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€
- æ”¯æŒä¸­æ–­å’Œæ¢å¤
- å®ç°æ—¶é—´æ—…è¡Œè°ƒè¯•

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# ä½¿ç”¨ thread_id æ ‡è¯†ä¼šè¯
thread = {"configurable": {"thread_id": "1"}}
graph.stream(input, thread)

# æŸ¥çœ‹å½“å‰çŠ¶æ€
state = graph.get_state(thread)

# æ¢å¤æ‰§è¡Œ
graph.stream(None, thread)
```

---

## ğŸ’¡ æœ€ä½³å®è·µå’ŒæŠ€å·§

### 1. æ§åˆ¶ LLM è¾“å‡ºæ ¼å¼

**é—®é¢˜**ï¼šLLM è¾“å‡ºæ ¼å¼ä¸ç¨³å®š

**è§£å†³**ï¼š
```python
# å®šä¹‰ä¸¥æ ¼çš„ Pydantic æ¨¡å‹
class Output(BaseModel):
    field1: str = Field(description="è¯¦ç»†æè¿°")
    field2: int

# å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º
structured_llm = llm.with_structured_output(Output)
result = structured_llm.invoke(prompt)  # ä¿è¯ç¬¦åˆ Output æ ¼å¼
```

---

### 2. å¾ªç¯æ§åˆ¶

**é—®é¢˜**ï¼šå¯¹è¯å¯èƒ½æ— é™å¾ªç¯

**è§£å†³**ï¼š
```python
def route_messages(state):
    max_turns = state.get('max_num_turns', 2)
    current_turns = count_turns(state['messages'])

    if current_turns >= max_turns:
        return 'end_node'

    if detect_end_signal(state['messages']):
        return 'end_node'

    return 'continue_node'
```

---

### 3. å¹¶è¡ŒèŠ‚ç‚¹çš„ç»“æœåˆå¹¶

**ä½¿ç”¨ Annotated**ï¼š
```python
from typing import Annotated
import operator

class State(TypedDict):
    results: Annotated[list, operator.add]  # è‡ªåŠ¨è¿½åŠ 
```

**æ•ˆæœ**ï¼š
- èŠ‚ç‚¹ A è¿”å› `{"results": [1, 2]}`
- èŠ‚ç‚¹ B è¿”å› `{"results": [3, 4]}`
- æœ€ç»ˆçŠ¶æ€ï¼š`{"results": [1, 2, 3, 4]}`

---

### 4. æç¤ºè¯å·¥ç¨‹

**åˆ†å±‚æç¤º**ï¼š
```python
# ç³»ç»Ÿçº§æŒ‡ä»¤
system_prompt = """You are an expert. Follow these rules:
1. Rule 1
2. Rule 2
"""

# è§’è‰²çº§æŒ‡ä»¤
role_prompt = f"Your role: {analyst.persona}"

# ä»»åŠ¡çº§æŒ‡ä»¤
task_prompt = "Answer this question: ..."

# ç»„åˆ
llm.invoke([
    SystemMessage(content=system_prompt),
    SystemMessage(content=role_prompt),
    HumanMessage(content=task_prompt)
])
```

---

## ğŸ¯ å®æˆ˜å»ºè®®

### å¯¹äºåˆå­¦è€…

1. **å…ˆç†è§£å•ä¸ªèŠ‚ç‚¹**
   - ä» `create_analysts` å¼€å§‹
   - å•ç‹¬æµ‹è¯•æ¯ä¸ªå‡½æ•°

2. **é€æ­¥æ„å»ºå›¾**
   - å…ˆæ„å»ºæœ€ç®€å•çš„çº¿æ€§å›¾
   - å†æ·»åŠ æ¡ä»¶è¾¹å’Œå¾ªç¯

3. **å–„ç”¨å¯è§†åŒ–**
   ```python
   graph.get_graph().draw_mermaid_png()
   ```

4. **ä½¿ç”¨ LangSmith è°ƒè¯•**
   - æŸ¥çœ‹æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å…¥è¾“å‡º
   - è¿½è¸ª LLM è°ƒç”¨

---

### å¯¹äºè¿›é˜¶ä½¿ç”¨

1. **ä¼˜åŒ–å¹¶è¡Œæ€§**
   - è¯†åˆ«å¯ä»¥å¹¶è¡Œçš„èŠ‚ç‚¹
   - ä½¿ç”¨ `Send()` åŠ¨æ€åˆ†å‘

2. **æˆæœ¬æ§åˆ¶**
   - ä½¿ç”¨æ›´å°çš„æ¨¡å‹åšè·¯ç”±
   - ç¼“å­˜é‡å¤çš„ LLM è°ƒç”¨

3. **é”™è¯¯å¤„ç†**
   ```python
   def safe_node(state):
       try:
           return process(state)
       except Exception as e:
           return {"error": str(e)}
   ```

4. **æ‰©å±•æ£€ç´¢æº**
   - æ·»åŠ æ›´å¤šæœç´¢å·¥å…·
   - é›†æˆæœ¬åœ°æ–‡æ¡£åº“

---

## ğŸ“Œ æ€»ç»“

è¿™ä¸ªæ¡ˆä¾‹æ˜¯ LangGraph çš„ä¸€ä¸ªç»¼åˆç¤ºèŒƒï¼Œå±•ç¤ºäº†ï¼š

### æ¶æ„è®¾è®¡
- **åˆ†å±‚ç»“æ„**ï¼šä¸»å›¾ + å­å›¾
- **æ¸…æ™°èŒè´£**ï¼šæ¯ä¸ªèŠ‚ç‚¹åªåšä¸€ä»¶äº‹
- **çµæ´»æ§åˆ¶**ï¼šæ¡ä»¶è·¯ç”± + å¾ªç¯æ£€æµ‹

### æŠ€æœ¯è¦ç‚¹
- **ç»“æ„åŒ–è¾“å‡º**ï¼šç¡®ä¿æ•°æ®è´¨é‡
- **äººæœºåä½œ**ï¼šå…³é”®å†³ç­–ç‚¹äººå·¥ä»‹å…¥
- **å¹¶è¡Œæ‰§è¡Œ**ï¼šæé«˜ç³»ç»Ÿæ•ˆç‡
- **æ£€æŸ¥ç‚¹æœºåˆ¶**ï¼šæ”¯æŒä¸­æ–­æ¢å¤

### åº”ç”¨ä»·å€¼
- **å¯å®šåˆ¶**ï¼šè½»æ¾è°ƒæ•´åˆ†æå¸ˆç±»å‹ã€æœç´¢æºã€æŠ¥å‘Šæ ¼å¼
- **å¯æ‰©å±•**ï¼šæ·»åŠ æ–°çš„èŠ‚ç‚¹å’ŒåŠŸèƒ½
- **ç”Ÿäº§çº§**ï¼šå®Œæ•´çš„é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç®¡ç†

---

## ğŸš€ åç»­å­¦ä¹ å»ºè®®

1. **ä¿®æ”¹æ¡ˆä¾‹**
   - æ”¹å˜ç ”ç©¶ä¸»é¢˜
   - è°ƒæ•´åˆ†æå¸ˆæ•°é‡å’Œç±»å‹
   - ä½¿ç”¨ä¸åŒçš„ LLM æ¨¡å‹

2. **æ‰©å±•åŠŸèƒ½**
   - æ·»åŠ å›¾è¡¨ç”Ÿæˆ
   - é›†æˆæ›´å¤šæ•°æ®æº
   - å®ç°å¤šè¯­è¨€æ”¯æŒ

3. **ä¼˜åŒ–æ€§èƒ½**
   - å®ç°ç»“æœç¼“å­˜
   - ä½¿ç”¨æµå¼è¾“å‡º
   - æ·»åŠ é€Ÿç‡é™åˆ¶

4. **æ·±å…¥åŸç†**
   - ç ”ç©¶ LangGraph çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶
   - ç†è§£çŠ¶æ€æ›´æ–°çš„åŸå­æ€§
   - å­¦ä¹ é«˜çº§è·¯ç”±ç­–ç•¥

---

## ğŸ¤ æ·±å…¥ç†è§£ï¼šInterview å­å›¾åœ¨ç³»ç»Ÿä¸­çš„æ ¸å¿ƒä½œç”¨

**Interview å­å›¾**æ˜¯æ•´ä¸ªç ”ç©¶åŠ©æ‰‹ç³»ç»Ÿçš„"å¿ƒè„"ï¼Œå®ƒæ‰¿æ‹…ç€æœ€å…³é”®çš„çŸ¥è¯†è·å–ä»»åŠ¡ã€‚ä»ç³»ç»Ÿæ¶æ„çš„è§’åº¦çœ‹ï¼Œå®ƒæ‰®æ¼”ç€ä¸‰é‡è§’è‰²ï¼š

é¦–å…ˆï¼Œå®ƒæ˜¯**çŸ¥è¯†ç”Ÿäº§çš„å¼•æ“**ã€‚é€šè¿‡æ¨¡æ‹Ÿ"åˆ†æå¸ˆ-ä¸“å®¶"çš„å¯¹è¯æ¨¡å¼ï¼ŒInterview å­å›¾å°†æŠ½è±¡çš„ç ”ç©¶ä¸»é¢˜è½¬åŒ–ä¸ºå…·ä½“çš„ã€æœ‰æ·±åº¦çš„çŸ¥è¯†å†…å®¹ã€‚è¿™ç§å¤šè½®å¯¹è¯æœºåˆ¶ï¼ˆask_question â†’ search â†’ answer â†’ ask_questionï¼‰å®ç°äº†æ¸è¿›å¼çš„çŸ¥è¯†æŒ–æ˜ï¼Œæ¯ä¸€è½®å¯¹è¯éƒ½åœ¨å‰ä¸€è½®çš„åŸºç¡€ä¸Šæ·±å…¥ï¼Œæœ€ç»ˆå½¢æˆæœ‰ä»·å€¼çš„ç ”ç©¶ç« èŠ‚ã€‚

å…¶æ¬¡ï¼Œå®ƒæ˜¯**å¹¶è¡ŒåŒ–æ¶æ„çš„åŸºæœ¬å•å…ƒ**ã€‚é€šè¿‡ `Send()` APIï¼Œç³»ç»Ÿå¯ä»¥åŒæ—¶å¯åŠ¨å¤šä¸ª Interview å®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹ç‹¬ç«‹è¿è¡Œï¼Œäº’ä¸å¹²æ‰°ã€‚è¿™ç§è®¾è®¡ä½¿å¾— 3 ä¸ªåˆ†æå¸ˆçš„è®¿è°ˆå¯ä»¥åœ¨ç›¸åŒæ—¶é—´å†…å®Œæˆï¼Œè€Œä¸æ˜¯ä¸²è¡Œæ‰§è¡Œã€‚å­å›¾çš„çŠ¶æ€éš”ç¦»ç‰¹æ€§ç¡®ä¿äº†æ¯ä¸ªåˆ†æå¸ˆéƒ½æœ‰ç‹¬ç«‹çš„å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ï¼Œé¿å…äº†ä¿¡æ¯æ··æ·†ã€‚

æœ€åï¼Œå®ƒæ˜¯**Map-Reduce æ¨¡å¼çš„ Map é˜¶æ®µå®ç°**ã€‚æ¯ä¸ª Interview å­å›¾æ¥æ”¶ä¸€ä¸ªåˆ†æå¸ˆä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªç»“æ„åŒ–çš„ç ”ç©¶ç« èŠ‚ï¼ˆsectionï¼‰ã€‚è¿™äº›ç« èŠ‚é€šè¿‡ `Annotated[list, operator.add]` è‡ªåŠ¨èšåˆåˆ°ä¸»å›¾çŠ¶æ€ä¸­ï¼Œä¸ºåç»­çš„ Reduce é˜¶æ®µï¼ˆwrite_reportã€write_introductionã€write_conclusionï¼‰æä¾›åŸææ–™ã€‚è¿™ç§æ¸…æ™°çš„è¾“å…¥è¾“å‡ºæ¥å£ä½¿å¾—å­å›¾å¯ä»¥æ— ç¼é›†æˆåˆ°æ›´å¤§çš„ç³»ç»Ÿä¸­ã€‚

ä»æŠ€æœ¯å®ç°ä¸Šçœ‹ï¼ŒInterview å­å›¾è¿˜å±•ç¤ºäº† LangGraph çš„å‡ ä¸ªé‡è¦æ¨¡å¼ï¼š**RAG æ£€ç´¢å¢å¼º**ï¼ˆé€šè¿‡ search_web å’Œ search_wikipedia ä¸ºå›ç­”æä¾›äº‹å®ä¾æ®ï¼‰ã€**å¾ªç¯æ§åˆ¶**ï¼ˆé€šè¿‡ route_messages é˜²æ­¢æ— é™å¯¹è¯ï¼‰ã€**æ¶ˆæ¯è§’è‰²ç®¡ç†**ï¼ˆåŒºåˆ† analyst å’Œ expert çš„å‘è¨€ï¼‰ã€‚è¿™äº›æ¨¡å¼çš„ç»„åˆä½¿å¾— Interview ä¸ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„é—®ç­”æµç¨‹ï¼Œè€Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯æ§çš„ã€é«˜è´¨é‡çš„çŸ¥è¯†ç”Ÿäº§æµæ°´çº¿ã€‚

---

**æœ€åçš„è¯**ï¼šLangGraph çš„å¼ºå¤§ä¹‹å¤„åœ¨äºå®ƒè®©ä½ åƒæ­ç§¯æœ¨ä¸€æ ·æ„å»ºå¤æ‚çš„ AI ç³»ç»Ÿã€‚è¿™ä¸ªç ”ç©¶åŠ©æ‰‹æ¡ˆä¾‹è™½ç„¶å¤æ‚ï¼Œä½†æ¯ä¸ªç»„ä»¶éƒ½å¾ˆç®€å•ã€‚æŒæ¡äº†è¿™äº›åŸºç¡€æ„å»ºå—ï¼Œä½ å°±å¯ä»¥åˆ›é€ å‡ºæ— é™å¯èƒ½çš„åº”ç”¨ï¼
