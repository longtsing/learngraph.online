# Module-5 æœ¬ç« ä»‹ç»ï¼šç²¾é€šé«˜çº§å›¾æ¨¡å¼ä¸æ¶æ„è®¾è®¡

> **æ¥è‡ªå›¾çµå¥–è·å¾—è€…çš„å¯„è¯­**
>
> "è½¯ä»¶å·¥ç¨‹çš„æœ¬è´¨æ˜¯ç®¡ç†å¤æ‚æ€§ã€‚å½“ç³»ç»Ÿå˜å¾—å¤æ‚æ—¶ï¼Œæˆ‘ä»¬ä¸æ˜¯é€ƒé¿å¤æ‚æ€§ï¼Œè€Œæ˜¯é€šè¿‡æ›´å¥½çš„æŠ½è±¡æ¥é©¾é©­å®ƒã€‚åœ¨ LangGraph ä¸­,ä½ å³å°†å­¦ä¹ çš„é«˜çº§æ¨¡å¼â€”â€”å¹¶è¡ŒåŒ–ã€å­å›¾ã€Map-Reduceâ€”â€”æ­£æ˜¯è¿™æ ·çš„æŠ½è±¡å·¥å…·ã€‚å®ƒä»¬è®©ä½ èƒ½å¤Ÿæ„å»ºæ¨¡å—åŒ–ã€å¯æ‰©å±•ã€é«˜æ€§èƒ½çš„ AI ç³»ç»Ÿã€‚è®°ä½:çœŸæ­£çš„å¤§å¸ˆä¸æ˜¯å†™å‡ºæœ€å¤æ‚çš„ä»£ç ,è€Œæ˜¯ç”¨æœ€ç®€æ´çš„æ¶æ„è§£å†³æœ€å¤æ‚çš„é—®é¢˜ã€‚"
>
> â€” *å¯å‘è‡ª Edsger W. Dijkstra å…³äºç»“æ„åŒ–ç¼–ç¨‹çš„å“²å­¦*

---

## ğŸ“š æœ¬ç« æ¦‚è§ˆ

æœ¬ç« å°†æ·±å…¥æ¢è®¨ LangGraph çš„é«˜çº§å›¾æ¨¡å¼å’Œæ¶æ„è®¾è®¡åŸåˆ™ã€‚ä½ å°†å­¦ä¼šå¦‚ä½•æ„å»º**ä¼ä¸šçº§ã€å¯æ‰©å±•ã€é«˜æ€§èƒ½**çš„ AI ç³»ç»Ÿ,æŒæ¡ä»ç®€å•é¡ºåºæ‰§è¡Œåˆ°å¤æ‚å¹¶è¡Œå¤„ç†çš„å®Œæ•´æŠ€èƒ½æ ‘ã€‚è¿™äº›ä¸ä»…æ˜¯æŠ€æœ¯æŠ€å·§,æ›´æ˜¯æ„å»ºç”Ÿäº§çº§ AI åº”ç”¨çš„æ ¸å¿ƒèƒ½åŠ›ã€‚

### å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« å­¦ä¹ ,ä½ å°†æŒæ¡:

1. **Parallelization(å¹¶è¡Œæ‰§è¡Œ)** - Fan-out/Fan-in æ¨¡å¼ã€Reducer æœºåˆ¶ã€å¹¶å‘çŠ¶æ€ç®¡ç†
2. **Sub-graph(å­å›¾)** - æ¨¡å—åŒ–è®¾è®¡ã€çŠ¶æ€éš”ç¦»ã€è¾“å‡ºæ¨¡å¼æ§åˆ¶
3. **Map-Reduce** - Send API åŠ¨æ€åˆ†å‘ã€å¤§è§„æ¨¡ä»»åŠ¡åˆ†è§£ä¸èšåˆ
4. **Research Assistant(ç»¼åˆæ¡ˆä¾‹)** - Human-in-the-Loop + å­å›¾ + Map-Reduce çš„å®Œæ•´é›†æˆ

### æœ¬ç« æ¶æ„å›¾

```
Module-5: é«˜çº§å›¾æ¨¡å¼å®Œæ•´ä½“ç³»
â”œâ”€ 5.1 Parallelization (éš¾åº¦: â­â­â­)
â”‚   â”œâ”€ Fan-out/Fan-in æ¨¡å¼
â”‚   â”œâ”€ Reducer æœºåˆ¶ (operator.add)
â”‚   â”œâ”€ è‡ªå®šä¹‰ Reducer
â”‚   â””â”€ å¹¶è¡Œè·¯å¾„åŒæ­¥
â”‚
â”œâ”€ 5.2 Sub-graph (éš¾åº¦: â­â­â­â­)
â”‚   â”œâ”€ çŠ¶æ€éš”ç¦» (state_schema)
â”‚   â”œâ”€ è¾“å‡ºæ§åˆ¶ (output_schema)
â”‚   â”œâ”€ å­å›¾ç¼–è¯‘ä¸åµŒå¥—
â”‚   â””â”€ æ¨¡å—åŒ–è®¾è®¡åŸåˆ™
â”‚
â”œâ”€ 5.3 Map-Reduce (éš¾åº¦: â­â­â­â­â­)
â”‚   â”œâ”€ Send API åŠ¨æ€åˆ†å‘
â”‚   â”œâ”€ Map é˜¶æ®µ (å¹¶è¡Œå¤„ç†)
â”‚   â”œâ”€ Reduce é˜¶æ®µ (ç»“æœèšåˆ)
â”‚   â””â”€ å¤šå±‚ Map-Reduce
â”‚
â””â”€ 5.4 Research Assistant (éš¾åº¦: â­â­â­â­â­)
    â”œâ”€ å¤šæ¨¡å—ååŒ (Human-in-the-Loop + Sub-graph + Map-Reduce)
    â”œâ”€ RAG æ¨¡å¼ (æ£€ç´¢å¢å¼ºç”Ÿæˆ)
    â”œâ”€ ç»“æ„åŒ–è¾“å‡º (Pydantic)
    â””â”€ ç”Ÿäº§çº§æ¶æ„è®¾è®¡
```

### çŸ¥è¯†ä¾èµ–å…³ç³»

```mermaid
graph TB
    A[Module-3: çŠ¶æ€ç®¡ç†] --> B[Parallelization]
    A --> C[Sub-graph]
    B --> D[Map-Reduce]
    C --> D
    E[Module-4: Human-in-the-Loop] --> F[Research Assistant]
    D --> F
    C --> F

    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1f5
    style D fill:#f5e1ff
    style E fill:#e1ffe1
    style F fill:#ffe1e1
```

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µé¢„è§ˆ

### 1. Parallelization: æ€§èƒ½çš„"å€å¢å™¨"

**æ ¸å¿ƒæ€æƒ³:** è®©å¤šä¸ªèŠ‚ç‚¹åœ¨åŒä¸€æ—¶é—´æ­¥å†…å¹¶è¡Œæ‰§è¡Œ,å……åˆ†åˆ©ç”¨è®¡ç®—èµ„æºã€‚

#### Fan-out/Fan-in æ¨¡å¼

```
         START
           |
        [Node A]
         /   \
        /     \
   [Node B] [Node C]  â† Fan-out (æ‰‡å‡º)
        \     /
         \   /
        [Node D]       â† Fan-in (æ‰‡å…¥)
           |
          END
```

**å…¸å‹åº”ç”¨åœºæ™¯:**
```python
# åœºæ™¯:å¹¶è¡Œæ£€ç´¢å¤šä¸ªæ•°æ®æº
START â†’ prepare_query
         â”œâ†’ search_wikipedia â†’ aggregate
         â”œâ†’ search_web       â†’ aggregate
         â””â†’ search_database  â†’ aggregate
                                   â†“
                                  END
```

#### Reducer æœºåˆ¶ - è§£å†³å¹¶è¡Œå†²çª

**é—®é¢˜:** å¤šä¸ªå¹¶è¡ŒèŠ‚ç‚¹æ›´æ–°åŒä¸€çŠ¶æ€å­—æ®µæ—¶ä¼šå†²çª

```python
# âŒ é”™è¯¯:æ²¡æœ‰ Reducer
class State(TypedDict):
    results: list  # B å’Œ C åŒæ—¶å†™å…¥ä¼šæŠ¥é”™

# âœ… æ­£ç¡®:ä½¿ç”¨ Reducer
from operator import add
from typing import Annotated

class State(TypedDict):
    results: Annotated[list, add]  # è‡ªåŠ¨åˆå¹¶æ›´æ–°
```

**Reducer å·¥ä½œåŸç†:**
```python
# B èŠ‚ç‚¹è¿”å›
{"results": ["result_from_B"]}

# C èŠ‚ç‚¹è¿”å›
{"results": ["result_from_C"]}

# operator.add åˆå¹¶å
{"results": ["result_from_B", "result_from_C"]}
```

**å¸¸ç”¨ Reducer:**

| Reducer | é€‚ç”¨ç±»å‹ | ä½œç”¨ |
|---------|---------|------|
| `operator.add` | list | åˆ—è¡¨æ‹¼æ¥ |
| `add_messages` | list[BaseMessage] | æ¶ˆæ¯åˆå¹¶(å»é‡ã€æ›´æ–°) |
| è‡ªå®šä¹‰ | ä»»æ„ | å®Œå…¨è‡ªå®šä¹‰åˆå¹¶é€»è¾‘ |

**è‡ªå®šä¹‰ Reducer ç¤ºä¾‹:**
```python
def merge_with_priority(left, right):
    """æŒ‰ä¼˜å…ˆçº§åˆå¹¶ç»“æœ"""
    combined = (left if isinstance(left, list) else [left]) + \
               (right if isinstance(right, list) else [right])

    # æŒ‰ä¼˜å…ˆçº§æ’åº
    return sorted(combined, key=lambda x: x.get('priority', 0), reverse=True)

class State(TypedDict):
    results: Annotated[list, merge_with_priority]
```

---

### 2. Sub-graph: æ¨¡å—åŒ–çš„"ä¹é«˜ç§¯æœ¨"

**æ ¸å¿ƒæ€æƒ³:** å°†å¤æ‚å›¾åˆ†è§£ä¸ºç‹¬ç«‹çš„å­å›¾æ¨¡å—,æ¯ä¸ªå­å›¾æœ‰è‡ªå·±çš„çŠ¶æ€ç©ºé—´å’Œé€»è¾‘ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦å­å›¾?

**æ²¡æœ‰å­å›¾çš„å›°å¢ƒ:**
```python
# ä¸»å›¾çŠ¶æ€åŒ…å«æ‰€æœ‰å­—æ®µ(æ··ä¹±)
class State(TypedDict):
    # ä¸»æµç¨‹å­—æ®µ
    user_input: str
    final_result: str

    # å­ä»»åŠ¡ A çš„ä¸­é—´å˜é‡
    task_a_temp1: str
    task_a_temp2: int

    # å­ä»»åŠ¡ B çš„ä¸­é—´å˜é‡
    task_b_temp1: list
    task_b_temp2: dict

    # ... è¶Šæ¥è¶Šå¤šçš„å­—æ®µ,éš¾ä»¥ç»´æŠ¤
```

**ä½¿ç”¨å­å›¾çš„æ¸…æ™°æ¶æ„:**
```python
# ä¸»å›¾çŠ¶æ€(ç®€æ´)
class MainState(TypedDict):
    user_input: str
    task_a_result: str
    task_b_result: str
    final_result: str

# å­å›¾ A çŠ¶æ€(ç‹¬ç«‹)
class TaskAState(TypedDict):
    input: str
    temp1: str  # åªåœ¨å­å›¾å†…éƒ¨ä½¿ç”¨
    temp2: int  # ä¸ä¼šæ³„æ¼åˆ°ä¸»å›¾
    result: str

# å­å›¾ B çŠ¶æ€(ç‹¬ç«‹)
class TaskBState(TypedDict):
    input: str
    temp1: list  # ä¸ TaskA çš„ temp1 å®Œå…¨éš”ç¦»
    temp2: dict
    result: str
```

#### çŠ¶æ€éš”ç¦»æœºåˆ¶

**å®Œæ•´çŠ¶æ€ vs è¾“å‡ºçŠ¶æ€:**
```python
# å­å›¾å†…éƒ¨çŠ¶æ€(å®Œæ•´)
class SubGraphState(TypedDict):
    input: str       # è¾“å…¥
    temp1: str       # ä¸­é—´å˜é‡
    temp2: int       # ä¸­é—´å˜é‡
    result: str      # è¾“å‡º

# å­å›¾è¾“å‡ºçŠ¶æ€(åªè¿”å›éœ€è¦çš„)
class SubGraphOutput(TypedDict):
    result: str      # åªè¿”å›è¿™ä¸ªå­—æ®µ

# æ„å»ºå­å›¾
sub_graph = StateGraph(
    state_schema=SubGraphState,     # å†…éƒ¨ä½¿ç”¨å®Œæ•´çŠ¶æ€
    output_schema=SubGraphOutput    # åªè¾“å‡º result
)
```

**ç±»æ¯”:** å­å›¾å°±åƒä¸€ä¸ªå‡½æ•°
```python
# å‡½æ•°æœ‰å±€éƒ¨å˜é‡å’Œè¿”å›å€¼
def function(input_data):
    temp1 = process_step1(input_data)  # å±€éƒ¨å˜é‡
    temp2 = process_step2(temp1)       # å±€éƒ¨å˜é‡
    result = finalize(temp2)
    return result  # åªè¿”å›ç»“æœ,ä¸è¿”å› temp1/temp2

# å­å›¾ä¹Ÿæœ‰å†…éƒ¨çŠ¶æ€å’Œè¾“å‡ºçŠ¶æ€
# state_schema = å‡½æ•°çš„æ‰€æœ‰å±€éƒ¨å˜é‡
# output_schema = å‡½æ•°çš„ return è¯­å¥
```

#### å­å›¾åµŒå¥—ç¤ºä¾‹

```python
# å®šä¹‰å­å›¾
sub_builder = StateGraph(SubGraphState, output_schema=SubGraphOutput)
sub_builder.add_node("step1", step1_func)
sub_builder.add_node("step2", step2_func)
sub_builder.add_edge(START, "step1")
sub_builder.add_edge("step1", "step2")
sub_builder.add_edge("step2", END)
sub_graph = sub_builder.compile()

# å°†å­å›¾ä½œä¸ºèŠ‚ç‚¹æ·»åŠ åˆ°ä¸»å›¾
main_builder = StateGraph(MainState)
main_builder.add_node("normal_node", normal_func)
main_builder.add_node("sub_task", sub_graph)  # â­ å­å›¾ä¹Ÿæ˜¯èŠ‚ç‚¹
main_builder.add_edge(START, "normal_node")
main_builder.add_edge("normal_node", "sub_task")
main_builder.add_edge("sub_task", END)
```

---

### 3. Map-Reduce: å¤§è§„æ¨¡ä»»åŠ¡çš„"åˆ†æ²»ç­–ç•¥"

**æ ¸å¿ƒæ€æƒ³:** å°†å¤§ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªå°ä»»åŠ¡å¹¶è¡Œå¤„ç†,ç„¶åèšåˆç»“æœã€‚

#### ç»å…¸ Map-Reduce æµç¨‹

```
è¾“å…¥: å¤„ç† 100 ä¸ªæ–‡æ¡£

Map é˜¶æ®µ (å¹¶è¡Œ):
æ–‡æ¡£1-20  â†’ Worker 1 â†’ æ‘˜è¦1
æ–‡æ¡£21-40 â†’ Worker 2 â†’ æ‘˜è¦2
æ–‡æ¡£41-60 â†’ Worker 3 â†’ æ‘˜è¦3
æ–‡æ¡£61-80 â†’ Worker 4 â†’ æ‘˜è¦4
æ–‡æ¡£81-100â†’ Worker 5 â†’ æ‘˜è¦5

Reduce é˜¶æ®µ (èšåˆ):
æ‰€æœ‰æ‘˜è¦ â†’ æ•´åˆæˆæœ€ç»ˆæŠ¥å‘Š
```

#### Send API - åŠ¨æ€ä»»åŠ¡åˆ†å‘ â­

**ä¼ ç»Ÿå¹¶è¡Œ vs Send API:**

**ä¼ ç»Ÿæ–¹å¼(é™æ€,å›ºå®šæ•°é‡):**
```python
# åªèƒ½å¹¶è¡Œ 3 ä¸ªå›ºå®šèŠ‚ç‚¹
builder.add_edge("input", "task1")
builder.add_edge("input", "task2")
builder.add_edge("input", "task3")
# å¦‚æœæœ‰ 10 ä¸ªä»»åŠ¡æ€ä¹ˆåŠ?éœ€è¦æ·»åŠ  10 æ¡è¾¹?
```

**Send API(åŠ¨æ€,çµæ´»æ‰©å±•):**
```python
from langgraph.types import Send

def dynamic_dispatch(state):
    tasks = state["tasks"]  # å¯èƒ½æ˜¯ 3 ä¸ª,ä¹Ÿå¯èƒ½æ˜¯ 100 ä¸ª

    # è‡ªåŠ¨ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºå¹¶è¡ŒèŠ‚ç‚¹
    return [Send("process_task", {"task": t}) for t in tasks]

# æ— è®ºå¤šå°‘ä»»åŠ¡,éƒ½èƒ½è‡ªåŠ¨å¹¶è¡Œå¤„ç†
builder.add_conditional_edges("input", dynamic_dispatch, ["process_task"])
```

**Send API çš„é­”åŠ›:**
```python
Send("ç›®æ ‡èŠ‚ç‚¹å", {"å‘é€çš„çŠ¶æ€æ•°æ®"})
#    ^^^^^^^^^^  ^^^^^^^^^^^^^^^^^^^^
#    è¦æ‰§è¡Œçš„èŠ‚ç‚¹  ä¼ é€’ç»™è¯¥èŠ‚ç‚¹çš„çŠ¶æ€(å¯ä»¥æ˜¯éƒ¨åˆ†çŠ¶æ€)

# ç¤ºä¾‹:ç”Ÿæˆç¬‘è¯
subjects = ["animals", "technology", "sports"]

return [
    Send("generate_joke", {"subject": "animals"}),
    Send("generate_joke", {"subject": "technology"}),
    Send("generate_joke", {"subject": "sports"})
]
# ä¼šåˆ›å»º 3 ä¸ªå¹¶è¡Œçš„ generate_joke èŠ‚ç‚¹å®ä¾‹
```

#### å®Œæ•´ Map-Reduce ç¤ºä¾‹

```python
from langgraph.types import Send
import operator
from typing import Annotated

# çŠ¶æ€å®šä¹‰
class OverallState(TypedDict):
    topic: str                                  # ä¸»é¢˜
    subtopics: list[str]                        # å­ä¸»é¢˜(Map è¾“å…¥)
    results: Annotated[list, operator.add]      # ç»“æœ(Map è¾“å‡º,éœ€è¦ Reducer)
    final_summary: str                          # æœ€ç»ˆæ‘˜è¦(Reduce è¾“å‡º)

class SubtopicState(TypedDict):
    subtopic: str  # Map èŠ‚ç‚¹åªéœ€è¦å­ä¸»é¢˜

# Map é˜¶æ®µ:ç”Ÿæˆå­ä¸»é¢˜
def generate_subtopics(state: OverallState):
    topic = state["topic"]
    # LLM ç”Ÿæˆ 5 ä¸ªå­ä¸»é¢˜
    subtopics = llm_generate_subtopics(topic)
    return {"subtopics": subtopics}

# åŠ¨æ€åˆ†å‘
def dispatch_to_map(state: OverallState):
    return [
        Send("process_subtopic", {"subtopic": st})
        for st in state["subtopics"]
    ]

# Map é˜¶æ®µ:å¹¶è¡Œå¤„ç†æ¯ä¸ªå­ä¸»é¢˜
def process_subtopic(state: SubtopicState):
    subtopic = state["subtopic"]
    result = llm_analyze(subtopic)
    return {"results": [result]}  # æ³¨æ„æ˜¯åˆ—è¡¨

# Reduce é˜¶æ®µ:èšåˆæ‰€æœ‰ç»“æœ
def summarize_results(state: OverallState):
    all_results = "\n\n".join(state["results"])
    summary = llm_summarize(all_results)
    return {"final_summary": summary}

# æ„å»ºå›¾
builder = StateGraph(OverallState)
builder.add_node("generate_subtopics", generate_subtopics)
builder.add_node("process_subtopic", process_subtopic)
builder.add_node("summarize_results", summarize_results)

builder.add_edge(START, "generate_subtopics")
builder.add_conditional_edges(
    "generate_subtopics",
    dispatch_to_map,
    ["process_subtopic"]
)
builder.add_edge("process_subtopic", "summarize_results")
builder.add_edge("summarize_results", END)

graph = builder.compile()
```

---

### 4. Research Assistant: ç»¼åˆæ¡ˆä¾‹ - "äº¤å“ä¹å›¢"

**æ ¸å¿ƒæ€æƒ³:** å°†æ‰€æœ‰é«˜çº§æ¨¡å¼ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„ç”Ÿäº§çº§ç³»ç»Ÿã€‚

#### ç³»ç»Ÿæ¶æ„

```
é˜¶æ®µ 1: ç”Ÿæˆåˆ†æå¸ˆå›¢é˜Ÿ (Human-in-the-Loop + å­å›¾)
ç”¨æˆ·è¾“å…¥ä¸»é¢˜ â†’ [create_analysts] â†’ [human_feedback] â‡„ ç”¨æˆ·å®¡æ‰¹
                                          â†“
é˜¶æ®µ 2: å¹¶è¡Œè®¿è°ˆ (Map-Reduce + Sub-graph)
                      â”Œâ”€â†’ [åˆ†æå¸ˆ1-è®¿è°ˆå­å›¾] â†’ ç« èŠ‚1
                      â”œâ”€â†’ [åˆ†æå¸ˆ2-è®¿è°ˆå­å›¾] â†’ ç« èŠ‚2
åˆ†æå¸ˆåˆ—è¡¨ â†’ Send API â”¼â”€â†’ [åˆ†æå¸ˆ3-è®¿è°ˆå­å›¾] â†’ ç« èŠ‚3
                      â”œâ”€â†’ [åˆ†æå¸ˆ4-è®¿è°ˆå­å›¾] â†’ ç« èŠ‚4
                      â””â”€â†’ [åˆ†æå¸ˆ5-è®¿è°ˆå­å›¾] â†’ ç« èŠ‚5
                              â†“
é˜¶æ®µ 3: æŠ¥å‘Šæ±‡æ€» (Reduce)
æ‰€æœ‰ç« èŠ‚ â†’ [write_introduction]  â”
        â†’ [write_report]         â”œâ†’ [finalize_report] â†’ æœ€ç»ˆæŠ¥å‘Š
        â†’ [write_conclusion]     â”˜
```

#### è®¿è°ˆå­å›¾è¯¦è§£

æ¯ä¸ªè®¿è°ˆå­å›¾æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æµç¨‹:

```
è®¿è°ˆå­å›¾å†…éƒ¨æµç¨‹:
[generate_question] åˆ†æå¸ˆæé—®
    â†“
    â”œâ†’ [search_web]       â”
    â””â†’ [search_wikipedia] â”˜ å¹¶è¡Œæ£€ç´¢
              â†“
    [generate_answer] åŸºäºæ£€ç´¢ç»“æœå›ç­”
              â†“
    åˆ¤æ–­: ç»§ç»­æé—® or ç»“æŸè®¿è°ˆ?
    â”œâ”€ ç»§ç»­ â†’ å›åˆ° generate_question (æœ€å¤š N è½®)
    â””â”€ ç»“æŸ â†’ [save_interview] â†’ [write_section] â†’ è¿”å›ç« èŠ‚
```

#### å…³é”®æŠ€æœ¯ç‚¹

**1. ç»“æ„åŒ–è¾“å‡º (Pydantic):**
```python
from pydantic import BaseModel

class Analyst(BaseModel):
    name: str
    role: str
    affiliation: str
    description: str

    @property
    def persona(self) -> str:
        return f"{self.name}, {self.role} at {self.affiliation}. {self.description}"

# LLM è¿”å›ç¬¦åˆè¿™ä¸ªç»“æ„çš„æ•°æ®
structured_llm = llm.with_structured_output(Analyst)
analyst = structured_llm.invoke("Generate an AI expert")
```

**2. Human-in-the-Loop å¾ªç¯:**
```python
def should_continue(state):
    # å¦‚æœç”¨æˆ·æä¾›äº†åé¦ˆ,é‡æ–°ç”Ÿæˆåˆ†æå¸ˆ
    if state.get("human_feedback"):
        return "create_analysts"
    else:
        return END

# åœ¨ human_feedback å‰ä¸­æ–­
graph = builder.compile(
    interrupt_before=["human_feedback"],
    checkpointer=memory
)
```

**3. Send API åŠ¨æ€åˆ†å‘è®¿è°ˆ:**
```python
def initiate_interviews(state):
    analysts = state["analysts"]
    topic = state["topic"]

    # ä¸ºæ¯ä¸ªåˆ†æå¸ˆåˆ›å»ºç‹¬ç«‹çš„è®¿è°ˆå­å›¾å®ä¾‹
    return [
        Send("conduct_interview", {
            "analyst": analyst,
            "messages": [HumanMessage(f"Tell me about {topic}")]
        })
        for analyst in analysts
    ]
```

**4. çŠ¶æ€éš”ç¦»ä¸èšåˆ:**
```python
# ä¸»å›¾çŠ¶æ€
class ResearchState(TypedDict):
    topic: str
    analysts: list[Analyst]
    sections: Annotated[list, operator.add]  # æ”¶é›†æ‰€æœ‰ç« èŠ‚
    final_report: str

# è®¿è°ˆå­å›¾çŠ¶æ€(å®Œå…¨éš”ç¦»)
class InterviewState(MessagesState):
    analyst: Analyst
    context: Annotated[list, operator.add]  # æ£€ç´¢çš„æ–‡æ¡£
    interview: str
    sections: list  # åªæœ‰è¿™ä¸ªå­—æ®µä¼šè¿”å›ä¸»å›¾

# å­å›¾åªè¿”å› sections
interview_graph = StateGraph(
    InterviewState,
    output_schema=TypedDict("Output", {"sections": list})
)
```

---

## ğŸ—ºï¸ å­¦ä¹ è·¯çº¿å›¾

### åˆå­¦è€…è·¯å¾„ (3-5 å¤©)

**ç›®æ ‡:** æŒæ¡åŸºç¡€å¹¶è¡Œå’Œå­å›¾æ¨¡å¼

**Day 1-2: Parallelization**
- âœ… ç†è§£ Fan-out/Fan-in æ¨¡å¼
- âœ… æŒæ¡ `operator.add` Reducer
- âœ… å®ç°ç®€å•çš„å¹¶è¡Œæ£€ç´¢
- ğŸ¯ å®æˆ˜:æ„å»ºå¹¶è¡ŒæŸ¥è¯¢å¤šä¸ª API çš„ç³»ç»Ÿ

**Day 3-4: Sub-graph**
- âœ… ç†è§£çŠ¶æ€éš”ç¦»çš„å¿…è¦æ€§
- âœ… æŒæ¡ `state_schema` å’Œ `output_schema`
- âœ… å®ç°å­å›¾åµŒå¥—
- ğŸ¯ å®æˆ˜:å°†å¤æ‚æµç¨‹æ‹†åˆ†ä¸ºç‹¬ç«‹å­å›¾æ¨¡å—

**Day 5: ç»¼åˆç»ƒä¹ **
- ğŸ¯ é¡¹ç›®:æ„å»ºä¸€ä¸ªæ”¯æŒå¹¶è¡Œå’Œå­å›¾çš„é—®ç­”ç³»ç»Ÿ
- ğŸ“ å¤ä¹ :å·©å›º Reducerã€å­å›¾ç¼–è¯‘ç­‰æ ¸å¿ƒæ¦‚å¿µ

---

### è¿›é˜¶è€…è·¯å¾„ (4-6 å¤©)

**ç›®æ ‡:** ç²¾é€š Map-Reduce å’Œå¤æ‚æ¨¡å¼ç»„åˆ

**Day 1-2: Map-Reduce åŸºç¡€**
- âœ… æ·±å…¥ç†è§£ Send API
- âœ… å®ç°åŠ¨æ€ä»»åŠ¡åˆ†å‘
- âœ… æŒæ¡ Map å’Œ Reduce é˜¶æ®µè®¾è®¡
- ğŸ¯ å®æˆ˜:æ„å»ºæ‰¹é‡æ–‡æ¡£æ‘˜è¦ç³»ç»Ÿ

**Day 3-4: é«˜çº§ Map-Reduce**
- âœ… å­¦ä¹ å¤šå±‚ Map-Reduce
- âœ… æŒæ¡æ¡ä»¶æ€§åˆ†å‘
- âœ… ä¼˜åŒ–æ€§èƒ½å’Œèµ„æºä½¿ç”¨
- ğŸ¯ å®æˆ˜:å®ç°å¯é…ç½®çš„æ•°æ®å¤„ç†ç®¡é“

**Day 5-6: æ¨¡å¼ç»„åˆ**
- âœ… Sub-graph + Map-Reduce é›†æˆ
- âœ… Parallelization + Human-in-the-Loop
- ğŸ¯ å®æˆ˜:æ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

---

### ä¸“å®¶è·¯å¾„ (6-10 å¤©)

**ç›®æ ‡:** æ„å»ºç”Ÿäº§çº§çš„ä¼ä¸šåº”ç”¨

**Day 1-3: Research Assistant æ·±åº¦å­¦ä¹ **
- âœ… åˆ†æå®Œæ•´æ¶æ„è®¾è®¡
- âœ… ç†è§£æ¯ä¸ªæ¨¡å—çš„èŒè´£
- âœ… æŒæ¡ RAG æ¨¡å¼å®ç°
- âœ… å­¦ä¹ ç»“æ„åŒ–è¾“å‡ºæœ€ä½³å®è·µ
- ğŸ¯ å®æˆ˜:å¤ç° Research Assistant ç³»ç»Ÿ

**Day 4-6: æ¶æ„ä¼˜åŒ–**
- âœ… æ€§èƒ½ä¼˜åŒ–:å¹¶è¡Œåº¦æ§åˆ¶ã€ç¼“å­˜ç­–ç•¥
- âœ… é”™è¯¯å¤„ç†:å­å›¾å¤±è´¥æ¢å¤ã€é‡è¯•æœºåˆ¶
- âœ… å¯è§‚æµ‹æ€§:æ—¥å¿—ã€æŒ‡æ ‡ã€è¿½è¸ª
- ğŸ¯ å®æˆ˜:ä¸ºç³»ç»Ÿæ·»åŠ å®Œæ•´çš„ç›‘æ§å’Œå®¹é”™

**Day 7-10: å¤§å‹ç»¼åˆé¡¹ç›®**
- ğŸ¯ é¡¹ç›®é€‰é¡¹:
  - å¤šæ™ºèƒ½ä½“åä½œå¹³å°
  - ä¼ä¸šçº§çŸ¥è¯†ç®¡ç†ç³»ç»Ÿ
  - è‡ªåŠ¨åŒ–ç ”ç©¶åŠ©æ‰‹
  - å¤æ‚å·¥ä½œæµç¼–æ’å¼•æ“

---

## ğŸ’¡ å­¦ä¹ å»ºè®®

### 1. ä»ç®€å•åˆ°å¤æ‚çš„æ¸è¿›è·¯å¾„

**é˜¶æ®µ 1:å•ä¸€æ¨¡å¼æŒæ¡**
```python
# å…ˆå•ç‹¬æŒæ¡å¹¶è¡Œ
graph_parallel = build_parallel_graph()

# å†å•ç‹¬æŒæ¡å­å›¾
graph_subgraph = build_graph_with_subgraph()

# æœ€åå•ç‹¬æŒæ¡ Map-Reduce
graph_mapreduce = build_mapreduce_graph()
```

**é˜¶æ®µ 2:ä¸¤ä¸¤ç»„åˆ**
```python
# å¹¶è¡Œ + å­å›¾
graph = build_parallel_subgraph()

# Map-Reduce + å­å›¾
graph = build_mapreduce_with_subgraph()
```

**é˜¶æ®µ 3:å®Œæ•´é›†æˆ**
```python
# æ‰€æœ‰æ¨¡å¼ç»„åˆ
graph = build_research_assistant()
```

---

### 2. è°ƒè¯•æŠ€å·§

**å¯è§†åŒ–å›¾ç»“æ„:**
```python
from IPython.display import Image, display

# ç”Ÿæˆå›¾çš„ Mermaid å›¾è¡¨
display(Image(graph.get_graph().draw_mermaid_png()))
```

**è¿½è¸ªæ‰§è¡Œæµç¨‹:**
```python
for step in graph.stream(input_data, config, stream_mode="debug"):
    print(f"Step: {step['step']}")
    print(f"Node: {step['node']}")
    print(f"State: {step['state']}")
    print("=" * 50)
```

**å­å›¾è°ƒè¯•:**
```python
# å…ˆå•ç‹¬æµ‹è¯•å­å›¾
sub_graph_result = sub_graph.invoke(test_input)
print("Sub-graph output:", sub_graph_result)

# å†é›†æˆåˆ°ä¸»å›¾æµ‹è¯•
main_graph_result = main_graph.invoke(test_input)
```

---

### 3. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**æ§åˆ¶å¹¶è¡Œåº¦:**
```python
import asyncio

# é™åˆ¶æœ€å¤§å¹¶å‘æ•°
async def controlled_send(tasks, max_concurrent=10):
    semaphore = asyncio.Semaphore(max_concurrent)

    async def limited_task(task):
        async with semaphore:
            return await process(task)

    return await asyncio.gather(*[limited_task(t) for t in tasks])
```

**ç¼“å­˜é‡å¤è®¡ç®—:**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def expensive_operation(input_hash):
    return llm_call(input_hash)
```

**æ‰¹é‡å¤„ç†:**
```python
# å°†å°ä»»åŠ¡åˆå¹¶æ‰¹å¤„ç†
def batch_process(tasks, batch_size=10):
    for i in range(0, len(tasks), batch_size):
        batch = tasks[i:i+batch_size]
        yield llm_batch_invoke(batch)
```

---

### 4. æ¶æ„è®¾è®¡åŸåˆ™

**åŸåˆ™ 1:å•ä¸€èŒè´£**
```python
# âœ… æ¯ä¸ªå­å›¾åªåšä¸€ä»¶äº‹
create_analysts_graph = ...  # åªè´Ÿè´£ç”Ÿæˆåˆ†æå¸ˆ
conduct_interview_graph = ...  # åªè´Ÿè´£æ‰§è¡Œè®¿è°ˆ
write_report_graph = ...       # åªè´Ÿè´£æŠ¥å‘Šç”Ÿæˆ

# âŒ é¿å…å¤§è€Œå…¨çš„å­å›¾
everything_graph = ...  # åŒ…å«æ‰€æœ‰é€»è¾‘
```

**åŸåˆ™ 2:æœ€å°åŒ–çŠ¶æ€å…±äº«**
```python
# âœ… å­å›¾åªæ¥æ”¶å¿…éœ€çš„è¾“å…¥
class InterviewInput(TypedDict):
    analyst: Analyst  # åªéœ€è¦åˆ†æå¸ˆä¿¡æ¯

# âŒ ä¼ é€’æ•´ä¸ªä¸»çŠ¶æ€
class InterviewInput(TypedDict):
    analyst: Analyst
    all_analysts: list  # ä¸éœ€è¦
    topic: str           # ä¸éœ€è¦
    ...
```

**åŸåˆ™ 3:æ˜ç¡®çš„æ¥å£**
```python
# ä½¿ç”¨ output_schema æ˜ç¡®å­å›¾çš„è¾“å‡º
interview_graph = StateGraph(
    InterviewState,
    output_schema=TypedDict("Output", {
        "sections": list  # æ˜ç¡®åªè¿”å›ç« èŠ‚
    })
)
```

---

## ğŸ“Š çŸ¥è¯†ç‚¹é€ŸæŸ¥è¡¨

### Parallelization

| æ¦‚å¿µ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| Fan-out | ä¸€ä¸ªèŠ‚ç‚¹æ‰‡å‡ºåˆ°å¤šä¸ªèŠ‚ç‚¹ | `add_edge("A", "B"); add_edge("A", "C")` |
| Fan-in | å¤šä¸ªèŠ‚ç‚¹æ±‡èšåˆ°ä¸€ä¸ªèŠ‚ç‚¹ | `add_edge("B", "D"); add_edge("C", "D")` |
| Reducer | åˆå¹¶å¹¶è¡Œæ›´æ–° | `Annotated[list, operator.add]` |
| è‡ªå®šä¹‰ Reducer | è‡ªå®šä¹‰åˆå¹¶é€»è¾‘ | `def my_reducer(left, right): ...` |

### Sub-graph

| API | ä½œç”¨ | ç¤ºä¾‹ |
|-----|------|------|
| `state_schema` | å­å›¾å†…éƒ¨çŠ¶æ€ | `StateGraph(SubState)` |
| `output_schema` | å­å›¾è¾“å‡ºå­—æ®µ | `StateGraph(SubState, output_schema=Output)` |
| `compile()` | ç¼–è¯‘å­å›¾ | `sub_graph = builder.compile()` |
| åµŒå¥— | å­å›¾ä½œä¸ºèŠ‚ç‚¹ | `main_builder.add_node("sub", sub_graph)` |

### Map-Reduce

| API | ä½œç”¨ | ç¤ºä¾‹ |
|-----|------|------|
| `Send` | åŠ¨æ€ä»»åŠ¡åˆ†å‘ | `Send("node", {"data": value})` |
| Map é˜¶æ®µ | å¹¶è¡Œå¤„ç† | è¿”å›å¤šä¸ª `Send` å¯¹è±¡ |
| Reduce é˜¶æ®µ | ç»“æœèšåˆ | ä½¿ç”¨ Reducer æ”¶é›†æ‰€æœ‰ç»“æœ |

---

## é™„å½•: æœ¯è¯­è¡¨

### æ¶æ„æ¨¡å¼

- **Fan-out (æ‰‡å‡º)**: ä¸€ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºåˆ†å‘åˆ°å¤šä¸ªå¹¶è¡ŒèŠ‚ç‚¹
- **Fan-in (æ‰‡å…¥)**: å¤šä¸ªå¹¶è¡ŒèŠ‚ç‚¹çš„è¾“å‡ºæ±‡èšåˆ°ä¸€ä¸ªèŠ‚ç‚¹
- **Map-Reduce**: å°†å¤§ä»»åŠ¡åˆ†è§£ä¸ºå°ä»»åŠ¡å¹¶è¡Œå¤„ç†,ç„¶åèšåˆç»“æœ
- **RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)**: ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æ¨¡å¼,å…ˆæ£€ç´¢ç›¸å…³ä¿¡æ¯å†ç”Ÿæˆå›ç­”

### æŠ€æœ¯æœ¯è¯­

- **Reducer**: å®šä¹‰å¦‚ä½•åˆå¹¶å¤šä¸ªå¹¶è¡Œæ›´æ–°çš„å‡½æ•°
- **Send API**: LangGraph çš„åŠ¨æ€ä»»åŠ¡åˆ†å‘æœºåˆ¶
- **Sub-graph (å­å›¾)**: åµŒå…¥åœ¨ä¸»å›¾ä¸­çš„ç‹¬ç«‹å›¾æ¨¡å—
- **State Schema**: å®šä¹‰å›¾çš„å®Œæ•´çŠ¶æ€ç»“æ„
- **Output Schema**: å®šä¹‰å­å›¾è¾“å‡ºçš„å­—æ®µ(è¾“å‡ºçŠ¶æ€çš„å­é›†)
- **Structured Output**: ä½¿ç”¨ Pydantic æ¨¡å‹ç¡®ä¿ LLM è¾“å‡ºç¬¦åˆç‰¹å®šæ ¼å¼

### Python ç›¸å…³

- **TypedDict**: Python ç±»å‹æç¤º,å®šä¹‰å­—å…¸ç»“æ„
- **Annotated**: Python ç±»å‹æ³¨è§£,å¯é™„åŠ å…ƒæ•°æ®(å¦‚ Reducer)
- **Pydantic BaseModel**: æ•°æ®éªŒè¯åº“,ç”¨äºç»“æ„åŒ–æ•°æ®å®šä¹‰
- **operator.add**: Python å†…ç½®æ¨¡å—,æä¾›åŠ æ³•æ“ä½œç¬¦å‡½æ•°

---

## ğŸ“ å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å­¦ä¹ å,è¯·ç¡®è®¤ä½ èƒ½å¤Ÿ:

**Parallelization:**
- [ ] è§£é‡Š Fan-out å’Œ Fan-in çš„æ¦‚å¿µ
- [ ] ä½¿ç”¨ `operator.add` Reducer å¤„ç†å¹¶è¡Œæ›´æ–°
- [ ] ç¼–å†™è‡ªå®šä¹‰ Reducer å‡½æ•°
- [ ] ç†è§£å¹¶è¡ŒèŠ‚ç‚¹çš„åŒæ­¥æœºåˆ¶

**Sub-graph:**
- [ ] ç†è§£çŠ¶æ€éš”ç¦»çš„å¿…è¦æ€§å’Œå¥½å¤„
- [ ] åŒºåˆ† `state_schema` å’Œ `output_schema`
- [ ] å°†å­å›¾ä½œä¸ºèŠ‚ç‚¹åµŒå…¥ä¸»å›¾
- [ ] è®¾è®¡æ¨¡å—åŒ–çš„å›¾æ¶æ„

**Map-Reduce:**
- [ ] ä½¿ç”¨ Send API åŠ¨æ€åˆ†å‘ä»»åŠ¡
- [ ] å®ç°å®Œæ•´çš„ Map-Reduce æµç¨‹
- [ ] ç†è§£ Map å’Œ Reduce é˜¶æ®µçš„èŒè´£
- [ ] ä¼˜åŒ– Map-Reduce æ€§èƒ½

**Research Assistant:**
- [ ] ç†è§£å¤šæ¨¡å—ååŒçš„æ¶æ„è®¾è®¡
- [ ] å®ç° Human-in-the-Loop + Map-Reduce ç»„åˆ
- [ ] æŒæ¡ RAG æ¨¡å¼å®ç°
- [ ] ä½¿ç”¨ Pydantic å®šä¹‰ç»“æ„åŒ–è¾“å‡º

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬ç« å­¦ä¹ å,ä½ å°†å…·å¤‡æ„å»º**ä¼ä¸šçº§ã€å¯æ‰©å±•ã€é«˜æ€§èƒ½**çš„ AI ç³»ç»Ÿçš„èƒ½åŠ›ã€‚å»ºè®®ç»§ç»­å­¦ä¹ :

- **Module-6**: ç”Ÿäº§éƒ¨ç½²ä¸ç›‘æ§(å®¹é”™ã€æ—¥å¿—ã€æ€§èƒ½ä¼˜åŒ–)
- **Module-7**: å¤§å‹ç»¼åˆé¡¹ç›®å®æˆ˜

**æ¨èå®è·µé¡¹ç›®:**
1. **å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿ**(ä¸­çº§):å¤šä¸ª AI Agent ååŒå®Œæˆå¤æ‚ä»»åŠ¡
2. **ä¼ä¸šçŸ¥è¯†ç®¡ç†å¹³å°**(é«˜çº§):RAG + Map-Reduce å¤„ç†å¤§è§„æ¨¡æ–‡æ¡£
3. **è‡ªåŠ¨åŒ–ç ”ç©¶åŠ©æ‰‹**(ä¸“å®¶çº§):å¤ç°å®Œæ•´çš„ Research Assistant ç³»ç»Ÿ

**è¿›é˜¶é˜…è¯»:**
- [LangGraph é«˜çº§æ¨¡å¼æ–‡æ¡£](https://langchain-ai.github.io/langgraph/how-tos/)
- [åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡æ¨¡å¼](https://www.patterns.dev/)
- [MapReduce è®ºæ–‡ (Google)](https://research.google/pubs/pub62/)

---

**æœ¬ç« æ ¸å¿ƒä»·å€¼:** ä»ç®€å•çš„é¡ºåºæ‰§è¡Œè¿›åŒ–åˆ°å¤æ‚çš„å¹¶è¡Œã€æ¨¡å—åŒ–ã€å¯æ‰©å±•æ¶æ„,è®©ä½ çš„ AI ç³»ç»Ÿèƒ½å¤Ÿå¤„ç†çœŸå®ä¸–ç•Œçš„å¤æ‚éœ€æ±‚ã€‚æŒæ¡è¿™äº›é«˜çº§æ¨¡å¼,ä½ å°†ä¸å†æ˜¯"ä»£ç å·¥äºº",è€Œæ˜¯"æ¶æ„å¸ˆ"!

ğŸ¯ **å‡†å¤‡å¥½äº†å—?** è®©æˆ‘ä»¬å¼€å§‹ç¬¬ä¸€èŠ‚è¯¾:**5.1 Parallelization(å¹¶è¡Œæ‰§è¡Œ)** â€” å­¦ä¹ å¦‚ä½•è®©ä½ çš„ç³»ç»Ÿæ€§èƒ½ç¿»å€!
