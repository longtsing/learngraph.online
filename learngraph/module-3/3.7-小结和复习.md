# Module-3 å°ç»“å’Œå¤ä¹ ï¼šçŠ¶æ€ç®¡ç†ç²¾é€šæŒ‡å—

> **æ¥è‡ªå›¾çµå¥–è·å¾—è€…çš„æ€»ç»“å¯„è¯­**
>
> æ­å–œä½ å®Œæˆäº† Module-3 çš„å­¦ä¹ ï¼çŠ¶æ€ç®¡ç†æ˜¯ LangGraph çš„çµé­‚ï¼Œä¹Ÿæ˜¯æ„å»ºå¤æ‚ AI ç³»ç»Ÿçš„åŸºçŸ³ã€‚ä½ å·²ç»æŒæ¡äº†ä»åŸºç¡€çš„ State Schema è®¾è®¡ï¼Œåˆ°é«˜çº§çš„ Reducer å‡½æ•°ï¼›ä»å•ä¸€çŠ¶æ€åˆ°å¤šçŠ¶æ€æ¨¡å¼ï¼›ä»ç®€å•çš„æ¶ˆæ¯è¿‡æ»¤åˆ°æ™ºèƒ½çš„å¯¹è¯æ‘˜è¦ï¼›ä»¥åŠå¤–éƒ¨æ•°æ®åº“çš„æŒä¹…åŒ–å­˜å‚¨ã€‚
>
> åœ¨æˆ‘å‡ åå¹´çš„è®¡ç®—æœºç§‘å­¦ç ”ç©¶ç”Ÿæ¶¯ä¸­ï¼Œæˆ‘è§è¯äº†çŠ¶æ€ç®¡ç†ä»ç®€å•çš„å˜é‡åˆ°å¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿçš„æ¼”è¿›ã€‚LangGraph çš„çŠ¶æ€ç®¡ç†æœºåˆ¶ï¼Œä¼˜é›…åœ°ç»“åˆäº†å‡½æ•°å¼ç¼–ç¨‹çš„çº¯å‡½æ•°æ€æƒ³ã€ç±»å‹ç³»ç»Ÿçš„å®‰å…¨æ€§ä¿éšœï¼Œä»¥åŠé’ˆå¯¹ AI åº”ç”¨çš„åˆ›æ–°è®¾è®¡ã€‚è¿™ä¸ä»…æ˜¯ä¸€å¥—æŠ€æœ¯ï¼Œæ›´æ˜¯ä¸€ç§æ€ç»´æ–¹å¼ã€‚
>
> è®°ä½ï¼šä¼˜ç§€çš„æ¶æ„å¸ˆä¸æ˜¯å†™æœ€å¤šä»£ç çš„äººï¼Œè€Œæ˜¯è®¾è®¡æœ€ä¼˜çŠ¶æ€ç»“æ„çš„äººã€‚æœ¬ç« çš„çŸ¥è¯†å°†ä¼´éšä½ çš„æ•´ä¸ª LangGraph å¼€å‘ç”Ÿæ¶¯ï¼Œåå¤å®è·µï¼Œèä¼šè´¯é€šï¼Œä½ å°†èƒ½å¤Ÿæ„å»ºçœŸæ­£å…·æœ‰"æ™ºæ…§"çš„ AI ç³»ç»Ÿã€‚
>
> *â€” å‘æ‰€æœ‰è¿½æ±‚å“è¶Šçš„ä½ è‡´æ•¬*

---

## ğŸ“‹ æœ¬ç« æ ¸å¿ƒçŸ¥è¯†å›é¡¾

### å­¦ä¹ åœ°å›¾

```mermaid
graph TB
    A[3.1 State Schema] -->|å®šä¹‰æ•°æ®ç»“æ„| B[3.2 State Reducers]
    A -->|æ§åˆ¶å¯è§æ€§| C[3.3 Multiple Schemas]
    B -->|æ”¯æŒå¹¶å‘| D[å¹¶è¡ŒèŠ‚ç‚¹]
    C -->|ä¿æŠ¤éšç§| E[API æ¥å£]
    D -->|äº§ç”Ÿå¤§é‡æ¶ˆæ¯| F[3.4 Trim & Filter]
    E -->|éœ€è¦ä¼˜åŒ–| F
    F -->|è¿›ä¸€æ­¥å‹ç¼©| G[3.5 Summarization]
    G -->|æŒä¹…åŒ–| H[3.6 External Memory]
    H -->|å®Œæ•´ç³»ç»Ÿ| I[ç”Ÿäº§çº§èŠå¤©æœºå™¨äºº]

    style A fill:#e1f5ff
    style B fill:#ffe1e1
    style C fill:#fff4e1
    style F fill:#e1ffe1
    style G fill:#f0e1ff
    style H fill:#ffe1f0
    style I fill:#ffe6e6
```

### å…­å¤§æ ¸å¿ƒæŠ€æœ¯é€ŸæŸ¥è¡¨

| æŠ€æœ¯ | æ ¸å¿ƒé—®é¢˜ | è§£å†³æ–¹æ¡ˆ | å…³é”® API | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|---------|
| **State Schema** | å¦‚ä½•å®šä¹‰æ•°æ®ç»“æ„ï¼Ÿ | TypedDict/Dataclass/Pydantic | `class State(TypedDict)` | æ‰€æœ‰é¡¹ç›® |
| **Reducers** | å¹¶è¡ŒèŠ‚ç‚¹å¦‚ä½•åˆå¹¶çŠ¶æ€ï¼Ÿ | operator.add/add_messages | `Annotated[list, add]` | å¹¶è¡Œæ‰§è¡Œ |
| **Multiple Schemas** | å¦‚ä½•éšè—å†…éƒ¨æ•°æ®ï¼Ÿ | Private/Input/Output State | `input_schema=...` | API æœåŠ¡ |
| **Trim & Filter** | å¦‚ä½•æ§åˆ¶æ¶ˆæ¯æ•°é‡ï¼Ÿ | RemoveMessage/trim_messages | `trim_messages(max_tokens)` | é•¿å¯¹è¯ |
| **Summarization** | å¦‚ä½•å‹ç¼©å†å²ï¼Ÿ | LLM ç”Ÿæˆæ‘˜è¦ | `RemoveMessage` + æ‘˜è¦ | è¶…é•¿å¯¹è¯ |
| **External Memory** | å¦‚ä½•æŒä¹…åŒ–çŠ¶æ€ï¼Ÿ | SqliteSaver/PostgresSaver | `SqliteSaver(conn)` | ç”Ÿäº§ç¯å¢ƒ |

---

## ğŸ¯ å¤ä¹ é¢˜ç›®åˆ—è¡¨

æœ¬å¤ä¹ åŒ…å« 15 é“ç»¼åˆæ€§é—®é¢˜ï¼Œåˆ†ä¸ºä¸‰ä¸ªéš¾åº¦çº§åˆ«ï¼š

### åŸºç¡€ç†è§£é¢˜ï¼ˆç¬¬ 1-5 é¢˜ï¼‰â­
æµ‹è¯•å¯¹æ ¸å¿ƒæ¦‚å¿µçš„ç†è§£

### ä»£ç å®ç°é¢˜ï¼ˆç¬¬ 6-10 é¢˜ï¼‰â­â­
æµ‹è¯•å®é™…ç¼–ç¨‹èƒ½åŠ›

### æ¶æ„è®¾è®¡é¢˜ï¼ˆç¬¬ 11-15 é¢˜ï¼‰â­â­â­
æµ‹è¯•ç³»ç»Ÿè®¾è®¡å’Œæœ€ä½³å®è·µ

---

## ğŸ“š è¯¦ç»†é—®ç­”è§£æ

### é—®é¢˜ 1ï¼šState Schema ä¸‰ç§å®šä¹‰æ–¹å¼çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### æ ¸å¿ƒåŒºåˆ«

ä¸‰ç§æ–¹å¼çš„æœ¬è´¨åŒºåˆ«åœ¨äº**ç±»å‹æ£€æŸ¥çš„æ—¶æœº**å’Œ**åŠŸèƒ½ä¸°å¯Œåº¦**ï¼š

| ç‰¹æ€§ | TypedDict | Dataclass | Pydantic |
|------|-----------|-----------|----------|
| **ç±»å‹æ£€æŸ¥æ—¶æœº** | é™æ€ï¼ˆIDE/mypyï¼‰ | é™æ€ï¼ˆIDE/mypyï¼‰ | é™æ€+è¿è¡Œæ—¶ |
| **è®¿é—®è¯­æ³•** | `state["key"]` | `state.key` | `state.key` |
| **è¿è¡Œæ—¶éªŒè¯** | âŒ æ—  | âŒ æ—  | âœ… è‡ªåŠ¨éªŒè¯ |
| **æ€§èƒ½** | æœ€å¿«ï¼ˆ0 å¼€é”€ï¼‰ | å¿«ï¼ˆè½»å¾®å¼€é”€ï¼‰ | ç¨æ…¢ï¼ˆéªŒè¯å¼€é”€ï¼‰ |
| **é»˜è®¤å€¼** | âŒ | âœ… | âœ… |
| **è‡ªå®šä¹‰æ–¹æ³•** | âŒ | âœ… | âœ… |
| **å¤æ‚éªŒè¯** | âŒ | âŒ | âœ… (validators) |

#### ä»£ç å¯¹æ¯”

```python
# TypedDict - æœ€è½»é‡
from typing_extensions import TypedDict
from typing import Literal

class TypedDictState(TypedDict):
    name: str
    mood: Literal["happy", "sad"]

# ä½¿ç”¨
state = {"name": "Alice", "mood": "angry"}  # âŒ IDE è­¦å‘Šï¼Œä½†è¿è¡Œä¸æŠ¥é”™
print(state["name"])  # å­—å…¸è®¿é—®

# Dataclass - æ›´é¢å‘å¯¹è±¡
from dataclasses import dataclass, field

@dataclass
class DataclassState:
    name: str
    mood: Literal["happy", "sad"]
    count: int = 0  # é»˜è®¤å€¼
    tags: list = field(default_factory=list)  # å¯å˜é»˜è®¤å€¼

    def is_happy(self):  # è‡ªå®šä¹‰æ–¹æ³•
        return self.mood == "happy"

# ä½¿ç”¨
state = DataclassState(name="Alice", mood="happy")
print(state.name)  # ç‚¹å·è®¿é—®
print(state.is_happy())  # True

# Pydantic - æœ€ä¸¥æ ¼
from pydantic import BaseModel, field_validator

class PydanticState(BaseModel):
    name: str
    mood: str

    @field_validator('mood')
    @classmethod
    def validate_mood(cls, v):
        if v not in ["happy", "sad"]:
            raise ValueError(f"Invalid mood: {v}")
        return v

# ä½¿ç”¨
try:
    state = PydanticState(name="Alice", mood="angry")
except ValueError as e:
    print(f"éªŒè¯å¤±è´¥: {e}")  # âœ… è¿è¡Œæ—¶æ•è·é”™è¯¯
```

#### é€‰æ‹©å†³ç­–æ ‘

```python
def choose_state_type(project):
    """é€‰æ‹©åˆé€‚çš„çŠ¶æ€ç±»å‹"""

    # 1. éœ€è¦è¿è¡Œæ—¶éªŒè¯ï¼Ÿï¼ˆå¤–éƒ¨è¾“å…¥ã€å…³é”®æ•°æ®ï¼‰
    if project.needs_runtime_validation:
        return "Pydantic BaseModel"

    # 2. éœ€è¦é»˜è®¤å€¼æˆ–è‡ªå®šä¹‰æ–¹æ³•ï¼Ÿ
    if project.needs_defaults or project.needs_methods:
        return "Dataclass"

    # 3. è¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Ÿï¼ˆå¤§å¤šæ•°æƒ…å†µï¼‰
    return "TypedDict"
```

#### å®é™…é¡¹ç›®å»ºè®®

```python
# å°å‹é¡¹ç›®ï¼ˆ< 5 ä¸ªèŠ‚ç‚¹ï¼‰
class State(TypedDict):
    messages: list
    user_id: str

# ä¸­å‹é¡¹ç›®ï¼ˆ5-20 ä¸ªèŠ‚ç‚¹ï¼‰
@dataclass
class State:
    messages: list = field(default_factory=list)
    user_id: str = ""
    config: dict = field(default_factory=dict)

# å¤§å‹é¡¹ç›®ï¼ˆ> 20 ä¸ªèŠ‚ç‚¹ï¼‰
class State(BaseModel):
    messages: list
    user_id: str
    config: AppConfig  # åµŒå¥— Pydantic æ¨¡å‹

    @field_validator('user_id')
    @classmethod
    def validate_user_id(cls, v):
        if not v or len(v) < 3:
            raise ValueError("Invalid user_id")
        return v
```

#### å¸¸è§é™·é˜±

```python
# é™·é˜± 1: TypedDict ä¸åšè¿è¡Œæ—¶æ£€æŸ¥
class State(TypedDict):
    age: int

state = {"age": "not_an_int"}  # âŒ ç±»å‹é”™è¯¯ä½†ä¸æŠ¥é”™
# è§£å†³ï¼šä½¿ç”¨ Pydantic æˆ–æ‰‹åŠ¨éªŒè¯

# é™·é˜± 2: Dataclass å¯å˜é»˜è®¤å€¼
@dataclass
class State:
    tags: list = []  # âŒ å±é™©ï¼æ‰€æœ‰å®ä¾‹å…±äº«åŒä¸€ä¸ªåˆ—è¡¨

# è§£å†³ï¼š
@dataclass
class State:
    tags: list = field(default_factory=list)  # âœ…

# é™·é˜± 3: Pydantic æ€§èƒ½å¼€é”€
class State(BaseModel):
    data: list[dict]  # æ·±å±‚åµŒå¥—

    @field_validator('data')
    @classmethod
    def validate_data(cls, v):
        # æ¯æ¬¡åˆ›å»ºå®ä¾‹éƒ½ä¼šéªŒè¯ï¼
        for item in v:
            complex_validation(item)  # å¯èƒ½å¾ˆæ…¢
        return v

# è§£å†³ï¼šåªåœ¨å¿…è¦æ—¶éªŒè¯ï¼Œæˆ–ä½¿ç”¨ç¼“å­˜
```

#### å…³é”®æ´å¯Ÿ

> **TypedDict æ˜¯ LangGraph çš„é»˜è®¤é€‰æ‹©**ï¼Œå› ä¸ºå®ƒï¼š
> - æ€§èƒ½æœ€ä¼˜ï¼ˆé›¶è¿è¡Œæ—¶å¼€é”€ï¼‰
> - è¯­æ³•ç®€æ´ï¼ˆæœ€å°‘æ ·æ¿ä»£ç ï¼‰
> - ä¸ LangGraph çš„è®¾è®¡å“²å­¦ä¸€è‡´ï¼ˆä¿¡ä»»å¼€å‘è€…ï¼‰
>
> åªæœ‰åœ¨**æ˜ç¡®éœ€è¦è¿è¡Œæ—¶éªŒè¯æˆ–å¤æ‚åŠŸèƒ½**æ—¶ï¼Œæ‰é€‰æ‹© Dataclass æˆ– Pydanticã€‚

</details>

---

### é—®é¢˜ 2ï¼šReducer å‡½æ•°çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå¹¶è¡ŒèŠ‚ç‚¹å¿…é¡»ä½¿ç”¨ Reducerï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### Reducer çš„æœ¬è´¨

Reducer æ˜¯ä¸€ä¸ª**çŠ¶æ€åˆå¹¶å‡½æ•°**ï¼Œç­¾åä¸ºï¼š

```python
def reducer(left: T, right: T) -> T:
    """
    left: å½“å‰çŠ¶æ€ä¸­çš„æ—§å€¼
    right: èŠ‚ç‚¹è¿”å›çš„æ–°å€¼
    è¿”å›: åˆå¹¶åçš„å€¼
    """
    pass
```

#### ä¸ºä»€ä¹ˆéœ€è¦ Reducerï¼Ÿ

**åœºæ™¯ï¼šå¹¶è¡ŒèŠ‚ç‚¹çš„çŠ¶æ€å†²çª**

```python
# å›¾ç»“æ„
START â†’ node_1 â†’ â”¬â†’ node_2 â†’ END
                 â””â†’ node_3 â†’ END

# æ‰§è¡Œæµç¨‹
class State(TypedDict):
    count: int

def node_1(state):
    return {"count": 1}

def node_2(state):
    return {"count": state["count"] + 1}  # æœŸæœ› 2

def node_3(state):
    return {"count": state["count"] + 1}  # æœŸæœ› 2

# é—®é¢˜ï¼šnode_2 å’Œ node_3 å¹¶è¡Œæ‰§è¡Œ
# ä¸¤è€…éƒ½è¿”å› {"count": 2}
# LangGraph ä¸çŸ¥é“è¯¥ä¿ç•™å“ªä¸ª â†’ InvalidUpdateError âŒ
```

**è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ Reducer**

```python
from operator import add
from typing import Annotated

class State(TypedDict):
    count: Annotated[list[int], add]  # â­ å…³é”®ï¼

def node_1(state):
    return {"count": [1]}

def node_2(state):
    return {"count": [2]}  # è¿”å›åˆ—è¡¨

def node_3(state):
    return {"count": [2]}  # è¿”å›åˆ—è¡¨

# LangGraph æ‰§è¡Œæµç¨‹ï¼š
# 1. node_1 è¿”å› [1] â†’ state["count"] = [1]
# 2. node_2 è¿”å› [2]
# 3. node_3 è¿”å› [2]
# 4. Reducer åˆå¹¶ï¼š[1] + [2] + [2] = [1, 2, 2] âœ…
```

#### Reducer å·¥ä½œæœºåˆ¶è¯¦è§£

```python
# LangGraph å†…éƒ¨ä¼ªä»£ç 
def apply_node_update(current_state, node_return, reducer):
    """åº”ç”¨èŠ‚ç‚¹æ›´æ–°"""
    for key, new_value in node_return.items():
        old_value = current_state[key]

        # å¦‚æœæœ‰ Reducerï¼Œä½¿ç”¨ Reducer åˆå¹¶
        if has_reducer(key):
            current_state[key] = reducer(old_value, new_value)
        else:
            # æ²¡æœ‰ Reducerï¼Œç›´æ¥è¦†ç›–
            current_state[key] = new_value

    return current_state
```

#### å¸¸ç”¨ Reducer å¯¹æ¯”

```python
from operator import add
import operator

# 1. operator.add - åˆ—è¡¨æ‹¼æ¥
class State(TypedDict):
    results: Annotated[list, add]

# node_1 è¿”å› [1, 2]
# node_2 è¿”å› [3]
# åˆå¹¶: [1, 2] + [3] = [1, 2, 3]

# 2. add_messages - æ¶ˆæ¯ç®¡ç†
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

# è‡ªåŠ¨å¤„ç†ï¼š
# - è¿½åŠ æ–°æ¶ˆæ¯
# - åŸºäº ID æ›´æ–°æ¶ˆæ¯
# - RemoveMessage åˆ é™¤æ¶ˆæ¯

# 3. max/min - æ•°å€¼èšåˆ
class State(TypedDict):
    max_score: Annotated[int, max]
    min_score: Annotated[int, min]

# node_1 è¿”å› {"max_score": 10}
# node_2 è¿”å› {"max_score": 15}
# åˆå¹¶: max(10, 15) = 15

# 4. è‡ªå®šä¹‰ Reducer - å»é‡
def unique_add(left: list, right: list) -> list:
    """å»é‡åè¿½åŠ """
    if not left:
        left = []
    if not right:
        right = []
    return list(set(left + right))

class State(TypedDict):
    tags: Annotated[list[str], unique_add]
```

#### å®æˆ˜æ¡ˆä¾‹ï¼šå¹¶è¡Œ API è°ƒç”¨

```python
from typing import Annotated
from operator import add

class APIState(TypedDict):
    query: str
    results: Annotated[list[dict], add]  # â­ Reducer

def call_api_1(state: APIState):
    """è°ƒç”¨ API 1"""
    result = {"source": "API1", "data": fetch_api_1(state["query"])}
    return {"results": [result]}

def call_api_2(state: APIState):
    """è°ƒç”¨ API 2"""
    result = {"source": "API2", "data": fetch_api_2(state["query"])}
    return {"results": [result]}

def call_api_3(state: APIState):
    """è°ƒç”¨ API 3"""
    result = {"source": "API3", "data": fetch_api_3(state["query"])}
    return {"results": [result]}

# æ„å»ºå›¾
builder = StateGraph(APIState)
builder.add_node("api1", call_api_1)
builder.add_node("api2", call_api_2)
builder.add_node("api3", call_api_3)

# å¹¶è¡Œè°ƒç”¨
builder.add_edge(START, "api1")
builder.add_edge(START, "api2")
builder.add_edge(START, "api3")
builder.add_edge("api1", END)
builder.add_edge("api2", END)
builder.add_edge("api3", END)

graph = builder.compile()

# æ‰§è¡Œ
result = graph.invoke({"query": "weather", "results": []})
# result["results"] = [
#     {"source": "API1", "data": ...},
#     {"source": "API2", "data": ...},
#     {"source": "API3", "data": ...}
# ] âœ… ä¸‰ä¸ªç»“æœè‡ªåŠ¨åˆå¹¶
```

#### é«˜çº§è‡ªå®šä¹‰ Reducer

```python
# ç¤ºä¾‹ 1ï¼šé™åˆ¶åˆ—è¡¨å¤§å°
def limited_add(max_size: int):
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºé™åˆ¶å¤§å°çš„ Reducer"""
    def reducer(left: list | None, right: list | None) -> list:
        if not left:
            left = []
        if not right:
            right = []
        combined = left + right
        return combined[-max_size:]  # åªä¿ç•™æœ€å N ä¸ª
    return reducer

class State(TypedDict):
    recent_messages: Annotated[list, limited_add(10)]

# ç¤ºä¾‹ 2ï¼šå¸¦ä¼˜å…ˆçº§çš„åˆå¹¶
from dataclasses import dataclass

@dataclass
class PriorityItem:
    value: str
    priority: int

def priority_merge(left: list[PriorityItem], right: list[PriorityItem]) -> list[PriorityItem]:
    """æŒ‰ä¼˜å…ˆçº§åˆå¹¶å¹¶æ’åº"""
    combined = (left or []) + (right or [])
    return sorted(combined, key=lambda x: x.priority, reverse=True)

class State(TypedDict):
    tasks: Annotated[list[PriorityItem], priority_merge]

# ç¤ºä¾‹ 3ï¼šæ™ºèƒ½å»é‡ï¼ˆä¿ç•™æœ€æ–°ï¼‰
def smart_unique(left: list[dict], right: list[dict]) -> list[dict]:
    """åŸºäº ID å»é‡ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬"""
    seen = {}
    for item in (left or []) + (right or []):
        item_id = item.get("id")
        if item_id:
            seen[item_id] = item  # åæ¥çš„è¦†ç›–å‰é¢çš„
    return list(seen.values())

class State(TypedDict):
    items: Annotated[list[dict], smart_unique]
```

#### å¸¸è§é”™è¯¯

```python
# é”™è¯¯ 1ï¼šå¿˜è®°è¿”å›åˆ—è¡¨
class State(TypedDict):
    results: Annotated[list, add]

def node(state):
    return {"results": "value"}  # âŒ åº”è¯¥è¿”å› ["value"]

# é”™è¯¯ 2ï¼šReducer ä¸å¤„ç† None
def bad_reducer(left: list, right: list) -> list:
    return left + right  # âŒ å¦‚æœ left æˆ– right æ˜¯ None ä¼šæŠ¥é”™

# æ­£ç¡®ï¼š
def good_reducer(left: list | None, right: list | None) -> list:
    if not left:
        left = []
    if not right:
        right = []
    return left + right  # âœ…

# é”™è¯¯ 3ï¼šReducer ä¿®æ”¹åŸå¯¹è±¡
def bad_reducer(left: list, right: list) -> list:
    left.extend(right)  # âŒ ä¿®æ”¹äº†åŸå¯¹è±¡
    return left

# æ­£ç¡®ï¼š
def good_reducer(left: list, right: list) -> list:
    return left + right  # âœ… åˆ›å»ºæ–°å¯¹è±¡
```

#### å…³é”®æ´å¯Ÿ

> **Reducer çš„ä¸‰ä¸ªæ ¸å¿ƒä½œç”¨ï¼š**
> 1. **è§£å†³å¹¶å‘å†²çª**ï¼šå¤šä¸ªèŠ‚ç‚¹å¯ä»¥å®‰å…¨åœ°æ›´æ–°åŒä¸€å­—æ®µ
> 2. **å®šä¹‰åˆå¹¶è¯­ä¹‰**ï¼šæ˜ç¡®æŒ‡å®šå¦‚ä½•ç»„åˆå¤šä¸ªæ›´æ–°
> 3. **ä¿æŒçŠ¶æ€ä¸€è‡´æ€§**ï¼šç¡®ä¿çŠ¶æ€æ›´æ–°çš„å¯é¢„æµ‹æ€§
>
> **ä½•æ—¶å¿…é¡»ä½¿ç”¨ Reducerï¼Ÿ**
> - å›¾ä¸­å­˜åœ¨å¹¶è¡ŒèŠ‚ç‚¹
> - å¤šä¸ªèŠ‚ç‚¹å¯èƒ½æ›´æ–°åŒä¸€å­—æ®µ
> - éœ€è¦è¿½åŠ è€Œéè¦†ç›–ï¼ˆå¦‚æ¶ˆæ¯å†å²ï¼‰

</details>

---

### é—®é¢˜ 3ï¼šadd_messages Reducer çš„ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿå„è‡ªçš„å®ç°åŸç†ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½

`add_messages` æ˜¯ LangGraph æœ€å¼ºå¤§çš„å†…ç½® Reducerï¼Œæä¾›ä¸‰å¤§åŠŸèƒ½ï¼š

1. **æ¶ˆæ¯è¿½åŠ ï¼ˆAppendï¼‰**
2. **æ¶ˆæ¯ä¿®æ”¹ï¼ˆUpdateï¼‰**
3. **æ¶ˆæ¯åˆ é™¤ï¼ˆRemoveï¼‰**

#### åŠŸèƒ½ 1ï¼šæ¶ˆæ¯è¿½åŠ 

**åŸç†ï¼š** å°†æ–°æ¶ˆæ¯è¿½åŠ åˆ°ç°æœ‰æ¶ˆæ¯åˆ—è¡¨æœ«å°¾

```python
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage

# åˆå§‹çŠ¶æ€
messages = [
    HumanMessage("Hi", id="1"),
    AIMessage("Hello!", id="2")
]

# è¿½åŠ æ–°æ¶ˆæ¯
new_message = HumanMessage("How are you?", id="3")
result = add_messages(messages, new_message)

# ç»“æœ
# [
#     HumanMessage("Hi", id="1"),
#     AIMessage("Hello!", id="2"),
#     HumanMessage("How are you?", id="3")  # âœ… è¿½åŠ æˆåŠŸ
# ]
```

**åœ¨ LangGraph ä¸­ä½¿ç”¨ï¼š**

```python
from langgraph.graph import MessagesState

class State(MessagesState):
    pass  # è‡ªåŠ¨åŒ…å« messages: Annotated[list, add_messages]

def chat_node(state: State):
    # ç”Ÿæˆå›å¤
    response = llm.invoke(state["messages"])
    return {"messages": [response]}  # âœ… è‡ªåŠ¨è¿½åŠ 
```

#### åŠŸèƒ½ 2ï¼šæ¶ˆæ¯ä¿®æ”¹ï¼ˆåŸºäº IDï¼‰

**åŸç†ï¼š** å¦‚æœæ–°æ¶ˆæ¯çš„ ID ä¸ç°æœ‰æ¶ˆæ¯ç›¸åŒï¼Œåˆ™è¦†ç›–æ—§æ¶ˆæ¯

```python
# åˆå§‹çŠ¶æ€
messages = [
    HumanMessage("I like cats", id="msg_1"),
    AIMessage("Great!", id="msg_2")
]

# ä¿®æ”¹æ¶ˆæ¯ï¼ˆç›¸åŒ IDï¼‰
updated_message = HumanMessage("I like dogs", id="msg_1")  # â­ ç›¸åŒ ID
result = add_messages(messages, updated_message)

# ç»“æœ
# [
#     HumanMessage("I like dogs", id="msg_1"),  # âœ… å†…å®¹è¢«æ›´æ–°
#     AIMessage("Great!", id="msg_2")
# ]
```

**åº”ç”¨åœºæ™¯ï¼šç”¨æˆ·ç¼–è¾‘æ¶ˆæ¯**

```python
class ChatState(MessagesState):
    edit_mode: bool = False

def handle_edit(state: ChatState):
    """å¤„ç†ç”¨æˆ·ç¼–è¾‘"""
    if state["edit_mode"]:
        # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        last_user_msg = [m for m in state["messages"] if isinstance(m, HumanMessage)][-1]

        # åˆ›å»ºç¼–è¾‘åçš„æ¶ˆæ¯ï¼ˆä½¿ç”¨ç›¸åŒ IDï¼‰
        edited_msg = HumanMessage(
            content=state["edited_content"],
            id=last_user_msg.id  # â­ ç›¸åŒ ID = è¦†ç›–
        )

        return {"messages": [edited_msg], "edit_mode": False}

    return {}
```

#### åŠŸèƒ½ 3ï¼šæ¶ˆæ¯åˆ é™¤

**åŸç†ï¼š** ä½¿ç”¨ `RemoveMessage` æ ‡è®°è¦åˆ é™¤çš„æ¶ˆæ¯

```python
from langchain_core.messages import RemoveMessage

# åˆå§‹çŠ¶æ€
messages = [
    HumanMessage("Msg 1", id="1"),
    HumanMessage("Msg 2", id="2"),
    HumanMessage("Msg 3", id="3"),
    HumanMessage("Msg 4", id="4")
]

# åˆ é™¤å‰ä¸¤æ¡æ¶ˆæ¯
delete_ops = [
    RemoveMessage(id="1"),
    RemoveMessage(id="2")
]
result = add_messages(messages, delete_ops)

# ç»“æœ
# [
#     HumanMessage("Msg 3", id="3"),
#     HumanMessage("Msg 4", id="4")
# ] âœ… å‰ä¸¤æ¡è¢«åˆ é™¤
```

**å®æˆ˜ï¼šæ»‘åŠ¨çª—å£å¯¹è¯**

```python
from langgraph.graph import MessagesState
from langchain_core.messages import RemoveMessage

class WindowState(MessagesState):
    window_size: int = 10  # åªä¿ç•™æœ€è¿‘ 10 æ¡

def sliding_window_node(state: WindowState):
    """å®ç°æ»‘åŠ¨çª—å£"""
    messages = state["messages"]
    window_size = state["window_size"]

    # å¦‚æœè¶…è¿‡çª—å£å¤§å°
    if len(messages) > window_size:
        # è®¡ç®—éœ€è¦åˆ é™¤çš„æ•°é‡
        num_to_delete = len(messages) - window_size

        # åˆ›å»ºåˆ é™¤æ“ä½œ
        delete_ops = [
            RemoveMessage(id=m.id)
            for m in messages[:num_to_delete]
        ]

        return {"messages": delete_ops}

    return {}
```

#### ä¸‰å¤§åŠŸèƒ½ç»„åˆä½¿ç”¨

**å®æˆ˜æ¡ˆä¾‹ï¼šæ™ºèƒ½æ¶ˆæ¯ç®¡ç†å™¨**

```python
from langchain_core.messages import RemoveMessage, HumanMessage, AIMessage
from langgraph.graph import MessagesState

class SmartChatState(MessagesState):
    max_messages: int = 20
    keep_system: bool = True

def smart_message_manager(state: SmartChatState):
    """æ™ºèƒ½ç®¡ç†æ¶ˆæ¯ï¼š
    1. ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
    2. åˆ é™¤æ—§å¯¹è¯
    3. æ”¯æŒæ¶ˆæ¯ç¼–è¾‘
    """
    messages = state["messages"]
    max_msgs = state["max_messages"]

    updates = []

    # 1. å¦‚æœæœ‰ç¼–è¾‘è¯·æ±‚
    if state.get("edit_last"):
        last_user = [m for m in messages if isinstance(m, HumanMessage)][-1]
        edited = HumanMessage(
            content=state["edit_content"],
            id=last_user.id  # â­ åŠŸèƒ½ 2ï¼šä¿®æ”¹
        )
        updates.append(edited)

    # 2. å¦‚æœè¶…è¿‡æœ€å¤§æ¶ˆæ¯æ•°
    if len(messages) > max_msgs:
        # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œæ™®é€šæ¶ˆæ¯
        from langchain_core.messages import SystemMessage
        system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
        other_msgs = [m for m in messages if not isinstance(m, SystemMessage)]

        # åˆ é™¤æ—§æ¶ˆæ¯ï¼ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼‰
        if state["keep_system"]:
            num_to_delete = len(other_msgs) - (max_msgs - len(system_msgs))
            if num_to_delete > 0:
                delete_ops = [
                    RemoveMessage(id=m.id)  # â­ åŠŸèƒ½ 3ï¼šåˆ é™¤
                    for m in other_msgs[:num_to_delete]
                ]
                updates.extend(delete_ops)
        else:
            # åˆ é™¤æœ€æ—§çš„æ¶ˆæ¯
            num_to_delete = len(messages) - max_msgs
            delete_ops = [
                RemoveMessage(id=m.id)
                for m in messages[:num_to_delete]
            ]
            updates.extend(delete_ops)

    # 3. è¿½åŠ æ–°æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if state.get("new_user_message"):
        new_msg = HumanMessage(state["new_user_message"])  # â­ åŠŸèƒ½ 1ï¼šè¿½åŠ 
        updates.append(new_msg)

    return {"messages": updates} if updates else {}
```

#### add_messages å†…éƒ¨å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
def add_messages(left: list[BaseMessage], right: list[BaseMessage] | BaseMessage) -> list[BaseMessage]:
    """
    LangGraph çš„ add_messages å®ç°åŸç†ï¼ˆç®€åŒ–ï¼‰
    """
    # 1. è§„èŒƒåŒ–è¾“å…¥
    if not isinstance(right, list):
        right = [right]

    # 2. å¤åˆ¶å·¦ä¾§æ¶ˆæ¯
    result = list(left) if left else []

    # 3. å¤„ç†å³ä¾§æ¶ˆæ¯
    for msg in right:
        if isinstance(msg, RemoveMessage):
            # åŠŸèƒ½ 3ï¼šåˆ é™¤æ¶ˆæ¯
            result = [m for m in result if m.id != msg.id]
        else:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ›´æ–°æ“ä½œ
            existing_index = None
            for i, existing_msg in enumerate(result):
                if existing_msg.id == msg.id:
                    existing_index = i
                    break

            if existing_index is not None:
                # åŠŸèƒ½ 2ï¼šæ›´æ–°æ¶ˆæ¯ï¼ˆç›¸åŒ IDï¼‰
                result[existing_index] = msg
            else:
                # åŠŸèƒ½ 1ï¼šè¿½åŠ æ¶ˆæ¯ï¼ˆæ–° IDï¼‰
                result.append(msg)

    return result
```

#### æœ€ä½³å®è·µ

```python
# âœ… æ¨èï¼šä½¿ç”¨ MessagesState
from langgraph.graph import MessagesState

class State(MessagesState):
    # è‡ªåŠ¨æ‹¥æœ‰ messages å­—æ®µ
    user_id: str

# âŒ ä¸æ¨èï¼šæ‰‹åŠ¨å®šä¹‰
from typing import Annotated
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]  # å¤šå†™ä»£ç 
    user_id: str

# âœ… æ¨èï¼šè¿”å›åˆ—è¡¨
def node(state):
    return {"messages": [new_message]}  # å•ä¸ªæ¶ˆæ¯ä¹Ÿç”¨åˆ—è¡¨

# âŒ ä¸æ¨èï¼šè¿”å›å•ä¸ªæ¶ˆæ¯
def node(state):
    return {"messages": new_message}  # è™½ç„¶å¯ä»¥ï¼Œä½†ä¸ä¸€è‡´

# âœ… æ¨èï¼šæ‰¹é‡åˆ é™¤ä½¿ç”¨åˆ—è¡¨æ¨å¯¼
delete_ops = [RemoveMessage(id=m.id) for m in old_messages]
return {"messages": delete_ops}

# âŒ ä¸æ¨èï¼šå¾ªç¯è¿½åŠ 
for msg in old_messages:
    # å¤šæ¬¡æ›´æ–°ï¼Œæ•ˆç‡ä½
    return {"messages": [RemoveMessage(id=msg.id)]}
```

#### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# æŠ€å·§ 1ï¼šæ‰¹é‡æ“ä½œä¼˜äºå¤šæ¬¡æ“ä½œ
# âŒ ä½æ•ˆ
for i in range(10):
    graph.invoke({"messages": [HumanMessage(f"Msg {i}")]}, config)

# âœ… é«˜æ•ˆ
all_messages = [HumanMessage(f"Msg {i}") for i in range(10)]
graph.invoke({"messages": all_messages}, config)

# æŠ€å·§ 2ï¼šä½¿ç”¨ç”Ÿæˆå™¨å»¶è¿Ÿè®¡ç®—
def get_delete_ops(messages, cutoff):
    """ä½¿ç”¨ç”Ÿæˆå™¨é¿å…åˆ›å»ºå¤§åˆ—è¡¨"""
    for msg in messages:
        if msg.timestamp < cutoff:
            yield RemoveMessage(id=msg.id)

# ä½¿ç”¨
delete_ops = list(get_delete_ops(state["messages"], cutoff))
return {"messages": delete_ops}
```

#### å…³é”®æ´å¯Ÿ

> **add_messages æ˜¯çŠ¶æ€ç®¡ç†çš„ç‘å£«å†›åˆ€**
>
> 1. **è¿½åŠ **ï¼šæ”¯æŒå¯¹è¯å†å²çš„è‡ªç„¶å¢é•¿
> 2. **ä¿®æ”¹**ï¼šå…è®¸ç”¨æˆ·ç¼–è¾‘å’Œ AI é‡æ–°ç”Ÿæˆ
> 3. **åˆ é™¤**ï¼šå®ç°æ»‘åŠ¨çª—å£å’Œå†…å­˜ä¼˜åŒ–
>
> è¿™ä¸‰ä¸ªåŠŸèƒ½è¦†ç›–äº†èŠå¤©åº”ç”¨çš„æ‰€æœ‰æ¶ˆæ¯ç®¡ç†éœ€æ±‚ï¼Œæ˜¯ LangGraph æœ€å¸¸ç”¨çš„ Reducerã€‚

</details>

---

### é—®é¢˜ 4ï¼šMultiple Schemas å¦‚ä½•å®ç°çŠ¶æ€çš„å¯è§æ€§æ§åˆ¶ï¼Ÿä¸‰ç§æ¨¡å¼çš„ä½œç”¨ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### å¯è§æ€§æ§åˆ¶çš„æœ¬è´¨

Multiple Schemas é€šè¿‡**ç±»å‹æ³¨è§£**å’Œ**StateGraph é…ç½®**å®ç°ä¸‰å±‚å¯è§æ€§æ§åˆ¶ï¼š

```
InputState (è¾“å…¥å±‚) â†’ InternalState (å†…éƒ¨å±‚) â†’ OutputState (è¾“å‡ºå±‚)
    ç”¨æˆ·å¯è§           èŠ‚ç‚¹å†…éƒ¨ä½¿ç”¨            ç”¨æˆ·å¯è§
```

#### ä¸‰ç§æ¨¡å¼è¯¦è§£

##### æ¨¡å¼ 1ï¼šPrivate Stateï¼ˆç§æœ‰çŠ¶æ€ï¼‰

**ä½œç”¨ï¼š** èŠ‚ç‚¹é—´ä¼ é€’ç§æœ‰æ•°æ®ï¼Œä¸æš´éœ²ç»™ç”¨æˆ·

**å®ç°åŸç†ï¼š**

```python
# 1. å®šä¹‰å…¬å¼€çŠ¶æ€å’Œç§æœ‰çŠ¶æ€
class PublicState(TypedDict):
    result: int

class PrivateState(TypedDict):
    intermediate: int  # ç§æœ‰å­—æ®µ

# 2. ä½¿ç”¨ç±»å‹æ³¨è§£æ§åˆ¶å¯è§æ€§
def node_1(state: PublicState) -> PrivateState:
    """
    è¾“å…¥ï¼šPublicStateï¼ˆåªèƒ½è®¿é—® resultï¼‰
    è¾“å‡ºï¼šPrivateStateï¼ˆè¿”å›ç§æœ‰æ•°æ®ï¼‰
    """
    result = state["result"]
    intermediate = result * 2
    return {"intermediate": intermediate}

def node_2(state: PrivateState) -> PublicState:
    """
    è¾“å…¥ï¼šPrivateStateï¼ˆå¯ä»¥è®¿é—® intermediateï¼‰
    è¾“å‡ºï¼šPublicStateï¼ˆè¿”å›å…¬å¼€æ•°æ®ï¼‰
    """
    final = state["intermediate"] + 10
    return {"result": final}

# 3. æ„å»ºå›¾ï¼ˆä½¿ç”¨ PublicState ä½œä¸ºä¸»çŠ¶æ€ï¼‰
builder = StateGraph(PublicState)
builder.add_node("node_1", node_1)
builder.add_node("node_2", node_2)
builder.add_edge(START, "node_1")
builder.add_edge("node_1", "node_2")
builder.add_edge("node_2", END)

graph = builder.compile()

# 4. æ‰§è¡Œ
result = graph.invoke({"result": 5})
# result = {"result": 20}  â† intermediate è¢«éšè—
```

**æ‰§è¡Œæµç¨‹ï¼š**

```
ç”¨æˆ·è¾“å…¥: {"result": 5}
    â†“
node_1: è¯»å– result=5, è¿”å› {"intermediate": 10}
    â†“
å†…éƒ¨çŠ¶æ€: {"result": 5, "intermediate": 10}  # LangGraph å†…éƒ¨åˆå¹¶
    â†“
node_2: è¯»å– intermediate=10, è¿”å› {"result": 20}
    â†“
å†…éƒ¨çŠ¶æ€: {"result": 20, "intermediate": 10}
    â†“
è¾“å‡ºè¿‡æ»¤: {"result": 20}  â† intermediate è¢«è¿‡æ»¤æ‰
```

**å…³é”®æœºåˆ¶ï¼š**

```python
# LangGraph å†…éƒ¨ä¼ªä»£ç 
def execute_node(node, state, node_input_type, node_output_type):
    # 1. è¾“å…¥è¿‡æ»¤ï¼šåªä¼ é€’èŠ‚ç‚¹å£°æ˜çš„è¾“å…¥ç±»å‹å­—æ®µ
    filtered_input = filter_by_type(state, node_input_type)

    # 2. æ‰§è¡ŒèŠ‚ç‚¹
    node_output = node(filtered_input)

    # 3. åˆå¹¶è¾“å‡ºï¼šå°†èŠ‚ç‚¹è¿”å›çš„å­—æ®µåˆå¹¶åˆ°å†…éƒ¨çŠ¶æ€
    internal_state.update(node_output)

    return internal_state
```

##### æ¨¡å¼ 2ï¼šInput Schemaï¼ˆè¾“å…¥æ¨¡å¼ï¼‰

**ä½œç”¨ï¼š** é™å®šç”¨æˆ·å¿…é¡»æä¾›çš„è¾“å…¥å­—æ®µ

**å®ç°åŸç†ï¼š**

```python
# 1. å®šä¹‰è¾“å…¥æ¨¡å¼
class InputState(TypedDict):
    question: str  # ç”¨æˆ·åªéœ€æä¾›é—®é¢˜

# 2. å®šä¹‰å†…éƒ¨æ¨¡å¼ï¼ˆå®Œæ•´çŠ¶æ€ï¼‰
class InternalState(TypedDict):
    question: str
    answer: str
    confidence: float
    processing_time: float  # å†…éƒ¨ç»Ÿè®¡

# 3. æ„å»ºå›¾æ—¶æŒ‡å®š input_schema
graph = StateGraph(
    InternalState,           # å†…éƒ¨ä½¿ç”¨å®Œæ•´çŠ¶æ€
    input_schema=InputState  # â­ é™å®šè¾“å…¥
)

# 4. ç”¨æˆ·è°ƒç”¨æ—¶åªèƒ½æä¾› InputState å®šä¹‰çš„å­—æ®µ
result = graph.invoke({"question": "What is AI?"})
# âœ… åˆæ³•

result = graph.invoke({
    "question": "What is AI?",
    "answer": "..."  # âŒ é”™è¯¯ï¼šä¸åœ¨ InputState ä¸­
})
```

**è¾“å…¥éªŒè¯æœºåˆ¶ï¼š**

```python
# LangGraph å†…éƒ¨ä¼ªä»£ç 
def invoke(user_input, config):
    # 1. éªŒè¯è¾“å…¥å­—æ®µ
    if hasattr(graph, 'input_schema'):
        validate_input(user_input, graph.input_schema)
        # åªä¿ç•™ input_schema å®šä¹‰çš„å­—æ®µ
        user_input = filter_by_schema(user_input, graph.input_schema)

    # 2. åˆå§‹åŒ–å†…éƒ¨çŠ¶æ€
    internal_state = initialize_state(graph.state_schema)
    internal_state.update(user_input)

    # 3. æ‰§è¡Œå›¾
    final_state = execute_graph(internal_state)

    return final_state
```

##### æ¨¡å¼ 3ï¼šOutput Schemaï¼ˆè¾“å‡ºæ¨¡å¼ï¼‰

**ä½œç”¨ï¼š** è¿‡æ»¤è¾“å‡ºå­—æ®µï¼Œåªè¿”å›ç”¨æˆ·éœ€è¦çš„æ•°æ®

**å®ç°åŸç†ï¼š**

```python
# 1. å®šä¹‰è¾“å‡ºæ¨¡å¼
class OutputState(TypedDict):
    answer: str       # ç”¨æˆ·éœ€è¦
    confidence: float # ç”¨æˆ·éœ€è¦

# 2. å®šä¹‰å†…éƒ¨æ¨¡å¼
class InternalState(TypedDict):
    question: str
    answer: str
    confidence: float
    processing_time: float  # å†…éƒ¨ç»Ÿè®¡ï¼Œä¸è¿”å›

# 3. æ„å»ºå›¾æ—¶æŒ‡å®š output_schema
graph = StateGraph(
    InternalState,
    output_schema=OutputState  # â­ é™å®šè¾“å‡º
)

# 4. æ‰§è¡Œå›¾
result = graph.invoke({"question": "What is AI?"})
# result = {
#     "answer": "...",
#     "confidence": 0.95
# }  â† processing_time è¢«è¿‡æ»¤æ‰
```

**è¾“å‡ºè¿‡æ»¤æœºåˆ¶ï¼š**

```python
# LangGraph å†…éƒ¨ä¼ªä»£ç 
def invoke(user_input, config):
    # æ‰§è¡Œå›¾
    final_state = execute_graph(user_input)

    # è¾“å‡ºè¿‡æ»¤
    if hasattr(graph, 'output_schema'):
        return filter_by_schema(final_state, graph.output_schema)

    return final_state
```

#### ä¸‰ç§æ¨¡å¼ç»„åˆä½¿ç”¨

**å®Œæ•´ç¤ºä¾‹ï¼šé—®ç­”ç³»ç»Ÿ**

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

# 1. å®šä¹‰ä¸‰ç§çŠ¶æ€
class InputState(TypedDict):
    """ç”¨æˆ·è¾“å…¥ï¼šåªéœ€æä¾›é—®é¢˜"""
    question: str

class OutputState(TypedDict):
    """ç”¨æˆ·è¾“å‡ºï¼šè¿”å›ç­”æ¡ˆå’Œç½®ä¿¡åº¦"""
    answer: str
    confidence: float

class InternalState(TypedDict):
    """å†…éƒ¨çŠ¶æ€ï¼šå®Œæ•´çš„å¤„ç†æ•°æ®"""
    question: str
    answer: str
    confidence: float
    retrieved_docs: list      # ç§æœ‰ï¼šæ£€ç´¢çš„æ–‡æ¡£
    llm_calls: int           # ç§æœ‰ï¼šLLM è°ƒç”¨æ¬¡æ•°
    processing_time: float   # ç§æœ‰ï¼šå¤„ç†æ—¶é—´

# 2. å®šä¹‰èŠ‚ç‚¹
def retrieve_node(state: InputState):
    """æ£€ç´¢èŠ‚ç‚¹ï¼šåªéœ€è¦ question"""
    docs = vector_db.search(state["question"])
    return {
        "retrieved_docs": docs,
        "llm_calls": 0
    }

def generate_node(state: InternalState) -> OutputState:
    """ç”ŸæˆèŠ‚ç‚¹ï¼š
    è¾“å…¥ï¼šå¯ä»¥è®¿é—®æ‰€æœ‰å†…éƒ¨å­—æ®µ
    è¾“å‡ºï¼šåªè¿”å› OutputState å­—æ®µ
    """
    import time
    start = time.time()

    # ä½¿ç”¨æ£€ç´¢çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
    answer = llm.invoke({
        "question": state["question"],
        "context": state["retrieved_docs"]
    })

    return {
        "answer": answer.content,
        "confidence": 0.95,
        "llm_calls": state["llm_calls"] + 1,
        "processing_time": time.time() - start
    }

# 3. æ„å»ºå›¾
builder = StateGraph(
    InternalState,              # å†…éƒ¨å®Œæ•´çŠ¶æ€
    input_schema=InputState,    # é™å®šè¾“å…¥
    output_schema=OutputState   # é™å®šè¾“å‡º
)

builder.add_node("retrieve", retrieve_node)
builder.add_node("generate", generate_node)
builder.add_edge(START, "retrieve")
builder.add_edge("retrieve", "generate")
builder.add_edge("generate", END)

graph = builder.compile()

# 4. ä½¿ç”¨
result = graph.invoke({"question": "What is LangGraph?"})
print(result)
# {
#     "answer": "LangGraph is a library for building...",
#     "confidence": 0.95
# }
# â† retrieved_docs, llm_calls, processing_time éƒ½è¢«éšè—
```

#### å¯è§æ€§æ§åˆ¶çš„å±‚æ¬¡ç»“æ„

```python
# å®Œæ•´çš„å¯è§æ€§å±‚æ¬¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     User (External World)       â”‚
â”‚  åªçœ‹åˆ° InputState/OutputState   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
      InputState (è¾“å…¥éªŒè¯)
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LangGraph Internal           â”‚
â”‚  å®Œæ•´çš„ InternalState           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ node_1 (InputState)     â”‚  â”‚
â”‚  â”‚    â†“                    â”‚  â”‚
â”‚  â”‚ [PrivateState created]  â”‚  â”‚
â”‚  â”‚    â†“                    â”‚  â”‚
â”‚  â”‚ node_2 (PrivateState)   â”‚  â”‚
â”‚  â”‚    â†“                    â”‚  â”‚
â”‚  â”‚ [PublicState updated]   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
      OutputState (è¾“å‡ºè¿‡æ»¤)
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     User (External World)       â”‚
â”‚  åªçœ‹åˆ° OutputState             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å®æˆ˜åº”ç”¨åœºæ™¯

**åœºæ™¯ 1ï¼šAPI æœåŠ¡**

```python
class APIInput(TypedDict):
    """API è¯·æ±‚"""
    user_id: str
    query: str

class APIOutput(TypedDict):
    """API å“åº”"""
    result: str
    status: str

class InternalState(TypedDict):
    """å†…éƒ¨å¤„ç†"""
    user_id: str
    query: str
    result: str
    status: str
    api_key: str        # ç§æœ‰ï¼šä¸è¿”å›
    rate_limit: int     # ç§æœ‰ï¼šä¸è¿”å›
    cost: float         # ç§æœ‰ï¼šä¸è¿”å›

graph = StateGraph(
    InternalState,
    input_schema=APIInput,
    output_schema=APIOutput
)
# ç”¨æˆ·æ°¸è¿œçœ‹ä¸åˆ° api_key, rate_limit, cost
```

**åœºæ™¯ 2ï¼šå¤šç§Ÿæˆ·ç³»ç»Ÿ**

```python
class TenantInput(TypedDict):
    tenant_id: str
    request: str

class TenantOutput(TypedDict):
    response: str

class InternalState(TypedDict):
    tenant_id: str
    request: str
    response: str
    tenant_config: dict    # ç§æœ‰ï¼šç§Ÿæˆ·é…ç½®
    usage_stats: dict      # ç§æœ‰ï¼šä½¿ç”¨ç»Ÿè®¡
    internal_errors: list  # ç§æœ‰ï¼šé”™è¯¯æ—¥å¿—

# æ¯ä¸ªç§Ÿæˆ·åªèƒ½çœ‹åˆ°è‡ªå·±çš„è¾“å…¥è¾“å‡ºï¼Œ
# çœ‹ä¸åˆ°å…¶ä»–ç§Ÿæˆ·çš„æ•°æ®å’Œç³»ç»Ÿå†…éƒ¨ä¿¡æ¯
```

#### å…³é”®æ´å¯Ÿ

> **Multiple Schemas å®ç°äº†çŠ¶æ€çš„"æœ€å°æƒé™åŸåˆ™"ï¼š**
>
> 1. **è¾“å…¥å±‚**ï¼šç”¨æˆ·åªéœ€æä¾›å¿…éœ€ä¿¡æ¯
> 2. **å†…éƒ¨å±‚**ï¼šç³»ç»Ÿå¯ä»¥ä½¿ç”¨å®Œæ•´ä¿¡æ¯
> 3. **è¾“å‡ºå±‚**ï¼šç”¨æˆ·åªèƒ½çœ‹åˆ°å…è®¸çš„ä¿¡æ¯
>
> è¿™ä¸ä»…æé«˜äº†å®‰å…¨æ€§ï¼Œè¿˜ä½¿ API æ›´æ¸…æ™°ã€æ›´æ˜“ç»´æŠ¤ã€‚

</details>

---

### é—®é¢˜ 5ï¼šæ¶ˆæ¯ç®¡ç†çš„ä¸‰ç§æŠ€æœ¯ï¼ˆRemoveMessageã€Filterã€Trimï¼‰å„è‡ªçš„é€‚ç”¨åœºæ™¯ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### ä¸‰ç§æŠ€æœ¯å¯¹æ¯”

| æŠ€æœ¯ | ä¿®æ”¹çŠ¶æ€ | Token æ§åˆ¶ | å¤æ‚åº¦ | å†å²ä¿ç•™ | é€‚ç”¨åœºæ™¯ |
|------|---------|-----------|--------|---------|---------|
| **RemoveMessage** | âœ… æ°¸ä¹…åˆ é™¤ | é—´æ¥æ§åˆ¶ | ä½ | âŒ éƒ¨åˆ†ä¸¢å¤± | ç¡®å®šä¸éœ€è¦çš„å†å² |
| **æ¶ˆæ¯è¿‡æ»¤** | âŒ ä¸ä¿®æ”¹ | æŒ‰æ•°é‡æ§åˆ¶ | ä½ | âœ… å®Œæ•´ä¿ç•™ | ç®€å•çš„"æœ€è¿‘ N æ¡" |
| **Token è£å‰ª** | âŒ ä¸ä¿®æ”¹ | æŒ‰ Token æ§åˆ¶ | ä¸­ | âœ… å®Œæ•´ä¿ç•™ | éœ€è¦ç²¾ç¡®æˆæœ¬æ§åˆ¶ |

#### æŠ€æœ¯ 1ï¼šRemoveMessage - æ°¸ä¹…åˆ é™¤

**åŸç†ï¼š** ä»çŠ¶æ€ä¸­æ°¸ä¹…åˆ é™¤æ¶ˆæ¯

**ä»£ç ç¤ºä¾‹ï¼š**

```python
from langchain_core.messages import RemoveMessage
from langgraph.graph import MessagesState

def filter_old_messages(state: MessagesState):
    """åˆ é™¤é™¤æœ€å 2 æ¡å¤–çš„æ‰€æœ‰æ¶ˆæ¯"""
    messages = state["messages"]

    # åˆ›å»ºåˆ é™¤æ“ä½œ
    delete_ops = [
        RemoveMessage(id=m.id)
        for m in messages[:-2]  # é™¤æœ€å 2 æ¡
    ]

    return {"messages": delete_ops}
```

**é€‚ç”¨åœºæ™¯ï¼š**

```python
# âœ… åœºæ™¯ 1ï¼šå®šæœŸæ¸…ç†æ—§å¯¹è¯
def cleanup_old_conversations(state):
    """åˆ é™¤ 7 å¤©å‰çš„æ¶ˆæ¯"""
    from datetime import datetime, timedelta

    cutoff = datetime.now() - timedelta(days=7)
    messages = state["messages"]

    delete_ops = [
        RemoveMessage(id=m.id)
        for m in messages
        if hasattr(m, 'timestamp') and m.timestamp < cutoff
    ]

    return {"messages": delete_ops}

# âœ… åœºæ™¯ 2ï¼šåˆ é™¤æ•æ„Ÿä¿¡æ¯
def remove_sensitive_messages(state):
    """åˆ é™¤åŒ…å«æ•æ„Ÿè¯çš„æ¶ˆæ¯"""
    sensitive_keywords = ["password", "credit_card", "ssn"]
    messages = state["messages"]

    delete_ops = []
    for msg in messages:
        if any(keyword in msg.content.lower() for keyword in sensitive_keywords):
            delete_ops.append(RemoveMessage(id=msg.id))

    return {"messages": delete_ops}

# âœ… åœºæ™¯ 3ï¼šå®ç°æ»‘åŠ¨çª—å£
def sliding_window(state, window_size=10):
    """åªä¿ç•™æœ€è¿‘ N æ¡æ¶ˆæ¯"""
    messages = state["messages"]

    if len(messages) > window_size:
        delete_ops = [
            RemoveMessage(id=m.id)
            for m in messages[:-window_size]
        ]
        return {"messages": delete_ops}

    return {}
```

**ä¼˜ç¼ºç‚¹ï¼š**

```python
# ä¼˜ç‚¹
âœ… æ°¸ä¹…å‡å°‘çŠ¶æ€å¤§å°ï¼ˆèŠ‚çœå†…å­˜ï¼‰
âœ… å®ç°ç®€å•ï¼ˆæ— éœ€é¢å¤–é…ç½®ï¼‰
âœ… é€‚åˆç¡®å®šä¸éœ€è¦çš„å†å²

# ç¼ºç‚¹
âŒ æ•°æ®ä¸å¯æ¢å¤ï¼ˆä¸€æ—¦åˆ é™¤æ— æ³•æ‰¾å›ï¼‰
âŒ å¯èƒ½ä¸¢å¤±é‡è¦ä¸Šä¸‹æ–‡
âŒ ä¸é€‚åˆéœ€è¦å›æº¯çš„åœºæ™¯
```

#### æŠ€æœ¯ 2ï¼šæ¶ˆæ¯è¿‡æ»¤ - ä¸ä¿®æ”¹çŠ¶æ€

**åŸç†ï¼š** ä¼ é€’æ¶ˆæ¯å­é›†ç»™ LLMï¼ŒçŠ¶æ€ä¿æŒå®Œæ•´

**ä»£ç ç¤ºä¾‹ï¼š**

```python
def chat_node(state: MessagesState):
    """åªä¼ é€’æœ€å 5 æ¡æ¶ˆæ¯ç»™ LLM"""
    # 1. è¿‡æ»¤æ¶ˆæ¯
    recent_messages = state["messages"][-5:]

    # 2. è°ƒç”¨ LLMï¼ˆåªçœ‹åˆ° 5 æ¡ï¼‰
    response = llm.invoke(recent_messages)

    # 3. è¿”å›å›å¤ï¼ˆè¿½åŠ åˆ°å®Œæ•´å†å²ï¼‰
    return {"messages": [response]}

# çŠ¶æ€ä»ç„¶ä¿ç•™æ‰€æœ‰å†å²æ¶ˆæ¯ï¼
```

**é€‚ç”¨åœºæ™¯ï¼š**

```python
# âœ… åœºæ™¯ 1ï¼šç®€å•çš„"æœ€è¿‘ N æ¡"é€»è¾‘
def chat_with_recent_context(state):
    """åªä½¿ç”¨æœ€è¿‘ 10 æ¡æ¶ˆæ¯"""
    return {"messages": [llm.invoke(state["messages"][-10:])]}

# âœ… åœºæ™¯ 2ï¼šæŒ‰æ¶ˆæ¯ç±»å‹è¿‡æ»¤
def chat_with_user_messages_only(state):
    """åªä¼ é€’ç”¨æˆ·æ¶ˆæ¯ï¼ˆå¿½ç•¥ç³»ç»Ÿæ¶ˆæ¯ï¼‰"""
    from langchain_core.messages import HumanMessage

    user_messages = [
        m for m in state["messages"]
        if isinstance(m, HumanMessage)
    ]

    return {"messages": [llm.invoke(user_messages)]}

# âœ… åœºæ™¯ 3ï¼šæŒ‰æ—¶é—´çª—å£è¿‡æ»¤
def chat_with_recent_time_window(state):
    """åªä½¿ç”¨æœ€è¿‘ 5 åˆ†é’Ÿçš„æ¶ˆæ¯"""
    from datetime import datetime, timedelta

    cutoff = datetime.now() - timedelta(minutes=5)
    recent_messages = [
        m for m in state["messages"]
        if hasattr(m, 'timestamp') and m.timestamp > cutoff
    ]

    return {"messages": [llm.invoke(recent_messages)]}

# âœ… åœºæ™¯ 4ï¼šæŒ‰å†…å®¹é•¿åº¦è¿‡æ»¤
def chat_with_short_messages(state):
    """åªä½¿ç”¨çŸ­æ¶ˆæ¯ï¼ˆ< 100 å­—ç¬¦ï¼‰"""
    short_messages = [
        m for m in state["messages"]
        if len(m.content) < 100
    ]

    return {"messages": [llm.invoke(short_messages)]}
```

**ä¼˜ç¼ºç‚¹ï¼š**

```python
# ä¼˜ç‚¹
âœ… çŠ¶æ€å®Œæ•´ä¿ç•™ï¼ˆå¯ç”¨äºæ—¥å¿—ã€åˆ†æï¼‰
âœ… å®ç°ç®€å•ï¼ˆæ— éœ€ Reducerï¼‰
âœ… çµæ´»ï¼ˆå¯ä»¥éšæ—¶è°ƒæ•´è¿‡æ»¤è§„åˆ™ï¼‰
âœ… å¯é€†ï¼ˆä¸ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰

# ç¼ºç‚¹
âŒ LLM ç¼ºä¹å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½å›ç­”ä¸è¿è´¯ï¼‰
âŒ ç²—ç³™çš„æ§åˆ¶ï¼ˆä¸è€ƒè™‘ Token æ•°é‡ï¼‰
âŒ éœ€è¦æ‰‹åŠ¨ç®¡ç†è¿‡æ»¤é€»è¾‘
```

#### æŠ€æœ¯ 3ï¼šToken è£å‰ª - ç²¾ç¡®æ§åˆ¶

**åŸç†ï¼š** åŸºäº Token æ•°é‡æ™ºèƒ½æˆªæ–­æ¶ˆæ¯

**ä»£ç ç¤ºä¾‹ï¼š**

```python
from langchain_core.messages import trim_messages
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4")

def chat_with_token_limit(state: MessagesState):
    """è£å‰ªåˆ°æœ€å¤š 1000 tokens"""
    # 1. è£å‰ªæ¶ˆæ¯
    trimmed_messages = trim_messages(
        state["messages"],
        max_tokens=1000,           # æœ€å¤§ Token æ•°
        strategy="last",           # ä¿ç•™æœ€åçš„æ¶ˆæ¯
        token_counter=model,       # ä½¿ç”¨æ¨¡å‹çš„ tokenizer
        allow_partial=False        # ä¸å…è®¸æˆªæ–­å•æ¡æ¶ˆæ¯
    )

    # 2. è°ƒç”¨ LLM
    response = model.invoke(trimmed_messages)

    return {"messages": [response]}
```

**é«˜çº§å‚æ•°è¯¦è§£ï¼š**

```python
# å‚æ•° 1: max_tokens - æœ€å¤§ Token æ•°
trim_messages(messages, max_tokens=500)  # ä¸¥æ ¼æ§åˆ¶æˆæœ¬

# å‚æ•° 2: strategy - è£å‰ªç­–ç•¥
trim_messages(messages, strategy="last")   # ä¿ç•™æœ€åçš„æ¶ˆæ¯ï¼ˆé»˜è®¤ï¼‰
trim_messages(messages, strategy="first")  # ä¿ç•™æœ€å¼€å§‹çš„æ¶ˆæ¯

# å‚æ•° 3: token_counter - Token è®¡æ•°å™¨
trim_messages(messages, token_counter=model)  # ä½¿ç”¨æ¨¡å‹çš„ tokenizer
trim_messages(messages, token_counter=lambda x: len(x.split()))  # ç®€å•æŒ‰å•è¯æ•°

# å‚æ•° 4: allow_partial - æ˜¯å¦å…è®¸æˆªæ–­å•æ¡æ¶ˆæ¯
trim_messages(messages, allow_partial=False)  # ä¿ç•™å®Œæ•´æ¶ˆæ¯
trim_messages(messages, allow_partial=True)   # å¯ä»¥æˆªæ–­æœ€åä¸€æ¡

# å‚æ•° 5: include_system - æ˜¯å¦åŒ…å«ç³»ç»Ÿæ¶ˆæ¯
trim_messages(messages, include_system=True)   # åŒ…å«ç³»ç»Ÿæ¶ˆæ¯ï¼ˆé»˜è®¤ï¼‰
trim_messages(messages, include_system=False)  # æ’é™¤ç³»ç»Ÿæ¶ˆæ¯
```

**é€‚ç”¨åœºæ™¯ï¼š**

```python
# âœ… åœºæ™¯ 1ï¼šç²¾ç¡®æ§åˆ¶æˆæœ¬
def cost_optimized_chat(state):
    """ç¡®ä¿æ¯æ¬¡è°ƒç”¨ä¸è¶…è¿‡ 1000 tokensï¼ˆçº¦ $0.03ï¼‰"""
    trimmed = trim_messages(
        state["messages"],
        max_tokens=1000,
        token_counter=model
    )
    return {"messages": [model.invoke(trimmed)]}

# âœ… åœºæ™¯ 2ï¼šå……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çª—å£
def max_context_chat(state):
    """ä½¿ç”¨æ¥è¿‘ä¸Šä¸‹æ–‡çª—å£çš„æœ€å¤§ tokens"""
    # GPT-4: 8192 tokens
    # é¢„ç•™ 1000 tokens ç»™å›å¤
    max_input_tokens = 8192 - 1000

    trimmed = trim_messages(
        state["messages"],
        max_tokens=max_input_tokens,
        token_counter=model
    )
    return {"messages": [model.invoke(trimmed)]}

# âœ… åœºæ™¯ 3ï¼šä¿ç•™ç³»ç»ŸæŒ‡ä»¤ + æœ€è¿‘å¯¹è¯
def chat_with_system_prompt(state):
    """ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼Œè£å‰ªå¯¹è¯å†å²"""
    from langchain_core.messages import SystemMessage

    # åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯
    system_messages = [m for m in state["messages"] if isinstance(m, SystemMessage)]
    chat_messages = [m for m in state["messages"] if not isinstance(m, SystemMessage)]

    # è®¡ç®—ç³»ç»Ÿæ¶ˆæ¯çš„ tokens
    system_tokens = sum(model.get_num_tokens(m.content) for m in system_messages)

    # è£å‰ªå¯¹è¯å†å²ï¼ˆå‡å»ç³»ç»Ÿæ¶ˆæ¯å ç”¨çš„ tokensï¼‰
    trimmed_chat = trim_messages(
        chat_messages,
        max_tokens=2000 - system_tokens,
        token_counter=model
    )

    # ç»„åˆï¼šç³»ç»Ÿæ¶ˆæ¯ + è£å‰ªåçš„å¯¹è¯
    final_messages = system_messages + trimmed_chat
    return {"messages": [model.invoke(final_messages)]}
```

**ä¼˜ç¼ºç‚¹ï¼š**

```python
# ä¼˜ç‚¹
âœ… ç²¾ç¡®æ§åˆ¶ Token æˆæœ¬
âœ… è‡ªåŠ¨å¤„ç†ä¸åŒé•¿åº¦çš„æ¶ˆæ¯
âœ… å……åˆ†åˆ©ç”¨ä¸Šä¸‹æ–‡çª—å£
âœ… çŠ¶æ€å®Œæ•´ä¿ç•™

# ç¼ºç‚¹
âŒ å®ç°å¤æ‚åº¦è¾ƒé«˜
âŒ éœ€è¦æ¨¡å‹çš„ tokenizer
âŒ æœ‰è®¡ç®—å¼€é”€ï¼ˆtoken è®¡æ•°ï¼‰
```

#### ä¸‰ç§æŠ€æœ¯ç»„åˆä½¿ç”¨

**å®æˆ˜æ¡ˆä¾‹ï¼šæ™ºèƒ½å®¢æœæœºå™¨äºº**

```python
from langchain_core.messages import RemoveMessage, trim_messages, SystemMessage
from langgraph.graph import MessagesState
from datetime import datetime, timedelta

class CustomerServiceState(MessagesState):
    max_messages: int = 50
    max_tokens: int = 2000

def intelligent_message_management(state: CustomerServiceState):
    """
    å¤šå±‚æ¶ˆæ¯ç®¡ç†ç­–ç•¥ï¼š
    1. åˆ é™¤è¶…è¿‡ 24 å°æ—¶çš„æ¶ˆæ¯ï¼ˆRemoveMessageï¼‰
    2. ä¿ç•™ç³»ç»ŸæŒ‡ä»¤ï¼ˆFilterï¼‰
    3. Token è£å‰ªå¯¹è¯å†å²ï¼ˆTrimï¼‰
    """
    messages = state["messages"]
    updates = []

    # ç¬¬ 1 å±‚ï¼šåˆ é™¤è¿‡æœŸæ¶ˆæ¯
    cutoff_time = datetime.now() - timedelta(hours=24)
    expired_messages = [
        m for m in messages
        if hasattr(m, 'timestamp') and m.timestamp < cutoff_time
    ]

    if expired_messages:
        delete_ops = [RemoveMessage(id=m.id) for m in expired_messages]
        updates.extend(delete_ops)
        # æ›´æ–°æ¶ˆæ¯åˆ—è¡¨
        messages = [m for m in messages if m not in expired_messages]

    # ç¬¬ 2 å±‚ï¼šåˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯å’Œå¯¹è¯æ¶ˆæ¯
    system_messages = [m for m in messages if isinstance(m, SystemMessage)]
    chat_messages = [m for m in messages if not isinstance(m, SystemMessage)]

    # ç¬¬ 3 å±‚ï¼šToken è£å‰ª
    if chat_messages:
        # è®¡ç®—ç³»ç»Ÿæ¶ˆæ¯å ç”¨çš„ tokens
        system_tokens = sum(
            model.get_num_tokens(m.content)
            for m in system_messages
        )

        # è£å‰ªå¯¹è¯å†å²
        available_tokens = state["max_tokens"] - system_tokens
        trimmed_chat = trim_messages(
            chat_messages,
            max_tokens=available_tokens,
            token_counter=model,
            strategy="last"
        )

        # ç»„åˆæ¶ˆæ¯
        final_messages = system_messages + trimmed_chat
    else:
        final_messages = system_messages

    # è°ƒç”¨ LLM
    response = model.invoke(final_messages)
    updates.append(response)

    return {"messages": updates}
```

#### é€‰æ‹©å†³ç­–æµç¨‹å›¾

```mermaid
graph TD
    A[éœ€è¦ç®¡ç†æ¶ˆæ¯] --> B{å†å²éœ€è¦ä¿ç•™?}
    B -->|ä¸éœ€è¦| C[ä½¿ç”¨ RemoveMessage]
    B -->|éœ€è¦| D{Token æ•°é‡é‡è¦?}
    D -->|ä¸é‡è¦| E[ä½¿ç”¨æ¶ˆæ¯è¿‡æ»¤]
    D -->|é‡è¦| F{æ¶ˆæ¯é•¿åº¦å˜åŒ–å¤§?}
    F -->|å¦| E
    F -->|æ˜¯| G[ä½¿ç”¨ Token è£å‰ª]

    C --> H[å®šæœŸæ¸…ç†/æ»‘åŠ¨çª—å£]
    E --> I[ç®€å•è§„åˆ™/å¿«é€ŸåŸå‹]
    G --> J[ç”Ÿäº§ç¯å¢ƒ/æˆæœ¬ä¼˜åŒ–]
```

#### æ€§èƒ½å¯¹æ¯”

å‡è®¾ 10 è½®å¯¹è¯ï¼Œæ¯è½®å¹³å‡ 100 tokensï¼š

| æ–¹æ¡ˆ | çŠ¶æ€å¤§å° | Token ä½¿ç”¨ | å“åº”å»¶è¿Ÿ | ä¸Šä¸‹æ–‡å®Œæ•´æ€§ |
|------|---------|-----------|---------|-------------|
| æ— ç®¡ç† | 1000 tokens | 1000 tokens | é«˜ | 100% |
| RemoveMessage (ä¿ç•™ 2 æ¡) | 200 tokens | 200 tokens | ä½ | 20% |
| Filter (æœ€å 5 æ¡) | 1000 tokens | 500 tokens | ä¸­ | 50% |
| Trim (500 tokens) | 1000 tokens | 500 tokens | ä¸­ | åŠ¨æ€ |

#### å…³é”®æ´å¯Ÿ

> **é€‰æ‹©å»ºè®®ï¼š**
>
> 1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨æ¶ˆæ¯è¿‡æ»¤ï¼ˆç®€å•å¿«é€Ÿï¼‰
> 2. **å°è§„æ¨¡åº”ç”¨**ï¼šRemoveMessage + è¿‡æ»¤ç»„åˆ
> 3. **ç”Ÿäº§ç¯å¢ƒ**ï¼šToken è£å‰ªï¼ˆç²¾ç¡®æ§åˆ¶ï¼‰
> 4. **ä¼ä¸šçº§**ï¼šä¸‰ç§æŠ€æœ¯ç»„åˆä½¿ç”¨
>
> **æ ¸å¿ƒåŸåˆ™ï¼š** æ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©ï¼Œä¸è¦è¿‡åº¦ä¼˜åŒ–ã€‚

</details>

---

### é—®é¢˜ 6ï¼šå¦‚ä½•å®ç°ä¸€ä¸ªæ”¯æŒæ¶ˆæ¯æ‘˜è¦çš„èŠå¤©æœºå™¨äººï¼Ÿå…³é”®æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### ç³»ç»Ÿæ¶æ„

æ¶ˆæ¯æ‘˜è¦ç³»ç»ŸåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   1. æ‘˜è¦è§¦å‘æœºåˆ¶ï¼ˆä½•æ—¶æ‘˜è¦ï¼‰           â”‚
â”‚      - åŸºäºæ¶ˆæ¯æ•°é‡                    â”‚
â”‚      - åŸºäº Token æ•°é‡                 â”‚
â”‚      - åŸºäºæ—¶é—´çª—å£                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   2. æ‘˜è¦ç”Ÿæˆé€»è¾‘ï¼ˆå¦‚ä½•æ‘˜è¦ï¼‰           â”‚
â”‚      - é¦–æ¬¡æ‘˜è¦ vs å¢é‡æ‘˜è¦            â”‚
â”‚      - ç³»ç»Ÿæç¤ºè¯è®¾è®¡                   â”‚
â”‚      - LLM è°ƒç”¨                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   3. æ¶ˆæ¯ä¿®å‰ªç­–ç•¥ï¼ˆä¿ç•™ä»€ä¹ˆï¼‰           â”‚
â”‚      - åˆ é™¤æ—§æ¶ˆæ¯                      â”‚
â”‚      - ä¿ç•™æœ€è¿‘ N æ¡                   â”‚
â”‚      - ä¿ç•™æ‘˜è¦                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å®Œæ•´å®ç°ä»£ç 

```python
from typing_extensions import TypedDict
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, RemoveMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END

# æ­¥éª¤ 1ï¼šå®šä¹‰çŠ¶æ€
class ChatState(MessagesState):
    summary: str  # å¯¹è¯æ‘˜è¦

# æ­¥éª¤ 2ï¼šåˆå§‹åŒ–æ¨¡å‹
model = ChatOpenAI(model="gpt-4", temperature=0)

# æ­¥éª¤ 3ï¼šå®šä¹‰å¯¹è¯èŠ‚ç‚¹
def call_model(state: ChatState):
    """
    å¯¹è¯èŠ‚ç‚¹ï¼šå¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶ç”Ÿæˆå›å¤
    """
    # è·å–æ‘˜è¦
    summary = state.get("summary", "")

    # å¦‚æœæœ‰æ‘˜è¦ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡
    if summary:
        # æ„å»ºç³»ç»Ÿæ¶ˆæ¯åŒ…å«æ‘˜è¦
        system_message = SystemMessage(
            content=f"Summary of conversation earlier: {summary}"
        )
        messages = [system_message] + state["messages"]
    else:
        messages = state["messages"]

    # è°ƒç”¨ LLM
    response = model.invoke(messages)

    return {"messages": [response]}

# æ­¥éª¤ 4ï¼šå®šä¹‰æ‘˜è¦èŠ‚ç‚¹
def summarize_conversation(state: ChatState):
    """
    æ‘˜è¦èŠ‚ç‚¹ï¼šç”Ÿæˆ/æ›´æ–°å¯¹è¯æ‘˜è¦å¹¶åˆ é™¤æ—§æ¶ˆæ¯
    """
    # 1. è·å–ç°æœ‰æ‘˜è¦
    summary = state.get("summary", "")

    # 2. åˆ›å»ºæ‘˜è¦æç¤ºè¯
    if summary:
        # å¢é‡æ‘˜è¦ï¼šæ‰©å±•ç°æœ‰æ‘˜è¦
        summary_message = (
            f"This is summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )
    else:
        # é¦–æ¬¡æ‘˜è¦ï¼šåˆ›å»ºæ–°æ‘˜è¦
        summary_message = "Create a summary of the conversation above:"

    # 3. è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

    # 4. åˆ é™¤æ—§æ¶ˆæ¯ï¼Œåªä¿ç•™æœ€å 2 æ¡
    delete_messages = [
        RemoveMessage(id=m.id)
        for m in state["messages"][:-2]
    ]

    # 5. è¿”å›æ›´æ–°
    return {
        "summary": response.content,
        "messages": delete_messages
    }

# æ­¥éª¤ 5ï¼šå®šä¹‰æ¡ä»¶å‡½æ•°ï¼ˆæ‘˜è¦è§¦å‘ï¼‰
def should_continue(state: ChatState):
    """
    å†³å®šæ˜¯å¦éœ€è¦æ‘˜è¦
    """
    messages = state["messages"]

    # å¦‚æœæ¶ˆæ¯è¶…è¿‡ 6 æ¡ï¼Œè§¦å‘æ‘˜è¦
    if len(messages) > 6:
        return "summarize_conversation"

    # å¦åˆ™ç»“æŸ
    return END

# æ­¥éª¤ 6ï¼šæ„å»ºå›¾
from langgraph.checkpoint.memory import MemorySaver

# åˆ›å»ºå›¾
workflow = StateGraph(ChatState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("conversation", call_model)
workflow.add_node("summarize_conversation", summarize_conversation)

# æ·»åŠ è¾¹
workflow.add_edge(START, "conversation")
workflow.add_conditional_edges("conversation", should_continue)
workflow.add_edge("summarize_conversation", END)

# ç¼–è¯‘ï¼ˆå¸¦å†…å­˜ï¼‰
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)

# æ­¥éª¤ 7ï¼šä½¿ç”¨ç¤ºä¾‹
config = {"configurable": {"thread_id": "1"}}

# ç¬¬ 1-6 è½®å¯¹è¯ï¼šæ­£å¸¸ç´¯ç§¯
for i in range(6):
    output = graph.invoke({
        "messages": [HumanMessage(f"Message {i+1}")]
    }, config)
    print(f"Round {i+1}: {len(output['messages'])} messages")

# ç¬¬ 7 è½®å¯¹è¯ï¼šè§¦å‘æ‘˜è¦
output = graph.invoke({
    "messages": [HumanMessage("Message 7")]
}, config)

# æŸ¥çœ‹æ‘˜è¦
summary = graph.get_state(config).values.get("summary", "")
print(f"\nSummary created: {summary}")
print(f"Messages remaining: {len(output['messages'])}")
```

#### å…³é”®æ­¥éª¤è¯¦è§£

##### æ­¥éª¤ 1ï¼šè®¾è®¡çŠ¶æ€ç»“æ„

```python
class ChatState(MessagesState):
    summary: str  # â­ å…³é”®å­—æ®µ
```

**è®¾è®¡è¦ç‚¹ï¼š**
- ç»§æ‰¿ `MessagesState` è‡ªåŠ¨è·å¾— `messages` å­—æ®µ
- æ·»åŠ  `summary` å­—æ®µå­˜å‚¨æ‘˜è¦
- å¯ä»¥æ‰©å±•å…¶ä»–å­—æ®µï¼ˆå¦‚ `summary_count`, `last_summary_time` ç­‰ï¼‰

##### æ­¥éª¤ 2ï¼šå®ç°å¢é‡æ‘˜è¦

```python
# é¦–æ¬¡æ‘˜è¦ï¼ˆæ— ç°æœ‰æ‘˜è¦ï¼‰
if not summary:
    prompt = "Create a summary of the conversation above:"

# å¢é‡æ‘˜è¦ï¼ˆæœ‰ç°æœ‰æ‘˜è¦ï¼‰
else:
    prompt = f"""
    This is summary of the conversation to date: {summary}

    Extend the summary by taking into account the new messages above:
    """
```

**å¢é‡æ‘˜è¦çš„ä¼˜åŠ¿ï¼š**
```python
# é¦–æ¬¡æ‘˜è¦ï¼ˆ7 æ¡æ¶ˆæ¯ï¼‰
summary_1 = "User is Lance, likes 49ers..."

# å¢é‡æ‘˜è¦ï¼ˆæ–°å¢ 5 æ¡æ¶ˆæ¯ï¼‰
# åªéœ€è¦æ‘˜è¦æ–°æ¶ˆæ¯ï¼Œä¸éœ€è¦é‡æ–°æ‘˜è¦å…¨éƒ¨
prompt = f"Current summary: {summary_1}\nExtend with new messages..."
summary_2 = "User is Lance, likes 49ers and Nick Bosa, asked about salaries..."
```

##### æ­¥éª¤ 3ï¼šæ™ºèƒ½æ¶ˆæ¯ä¿®å‰ª

```python
# ä¿ç•™æœ€å 2 æ¡æ¶ˆæ¯
delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
```

**ä¸ºä»€ä¹ˆä¿ç•™ 2 æ¡ï¼Ÿ**
```
æœ€å 2 æ¡é€šå¸¸æ˜¯ï¼š
- æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
- æœ€æ–°çš„ AI å›å¤

è¿™ç¡®ä¿å¯¹è¯çš„è¿ç»­æ€§å’Œä¸Šä¸‹æ–‡ã€‚
```

**å¯é…ç½®çš„ä¿ç•™ç­–ç•¥ï¼š**

```python
class ChatState(MessagesState):
    summary: str
    keep_last_n: int = 2  # å¯é…ç½®

def summarize_conversation(state: ChatState):
    keep_n = state.get("keep_last_n", 2)
    delete_messages = [
        RemoveMessage(id=m.id)
        for m in state["messages"][:-keep_n]
    ]
    # ...
```

#### é«˜çº§ä¼˜åŒ–æŠ€å·§

##### ä¼˜åŒ– 1ï¼šå¤šå±‚æ‘˜è¦

```python
class AdvancedChatState(MessagesState):
    short_term_summary: str  # æœ€è¿‘ 10 è½®
    long_term_summary: str   # å®Œæ•´å†å²

def should_summarize(state):
    msg_count = len(state["messages"])

    if msg_count > 20:
        return "long_term_summary"  # é•¿æœŸæ‘˜è¦
    elif msg_count > 6:
        return "short_term_summary"  # çŸ­æœŸæ‘˜è¦
    return END

# ä½¿ç”¨æ—¶ï¼š
# short_term_summary: æä¾›è¿‘æœŸä¸Šä¸‹æ–‡
# long_term_summary: æä¾›èƒŒæ™¯ä¿¡æ¯
```

##### ä¼˜åŒ– 2ï¼šç»“æ„åŒ–æ‘˜è¦

```python
from pydantic import BaseModel

class StructuredSummary(BaseModel):
    """ç»“æ„åŒ–æ‘˜è¦"""
    user_info: str          # ç”¨æˆ·ä¿¡æ¯
    topics: list[str]       # è®¨è®ºçš„ä¸»é¢˜
    key_facts: list[str]    # å…³é”®äº‹å®
    pending_questions: list[str]  # å¾…è§£ç­”çš„é—®é¢˜

def structured_summarize(state: ChatState):
    """ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦"""
    prompt = """
    Summarize this conversation with the following structure:
    - User profile (name, interests, etc.)
    - Topics discussed
    - Key facts mentioned
    - Any pending questions
    """

    messages = state["messages"] + [HumanMessage(prompt)]
    response = model.with_structured_output(StructuredSummary).invoke(messages)

    # å°†ç»“æ„åŒ–æ‘˜è¦è½¬ä¸º JSON å­˜å‚¨
    summary_json = response.json()

    # ...
```

##### ä¼˜åŒ– 3ï¼šé€‰æ‹©æ€§æ‘˜è¦

```python
def intelligent_should_summarize(state: ChatState):
    """
    æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦æ‘˜è¦ï¼š
    - æ¶ˆæ¯æ•°é‡
    - Token æ•°é‡
    - è¯é¢˜è½¬æ¢
    """
    messages = state["messages"]

    # æ¡ä»¶ 1ï¼šæ¶ˆæ¯æ•°é‡
    if len(messages) > 6:
        # æ¡ä»¶ 2ï¼šToken æ•°é‡
        total_tokens = sum(model.get_num_tokens(m.content) for m in messages)
        if total_tokens > 2000:
            # æ¡ä»¶ 3ï¼šæ˜¯å¦æœ‰è¯é¢˜è½¬æ¢
            if detect_topic_change(messages):
                return "summarize_conversation"

    return END

def detect_topic_change(messages):
    """æ£€æµ‹è¯é¢˜æ˜¯å¦è½¬æ¢ï¼ˆç®€å•å®ç°ï¼‰"""
    if len(messages) < 2:
        return False

    last_content = messages[-1].content
    prev_content = messages[-2].content

    # ä½¿ç”¨å…³é”®è¯ç›¸ä¼¼åº¦æˆ– LLM åˆ¤æ–­
    similarity = calculate_similarity(last_content, prev_content)
    return similarity < 0.3  # ç›¸ä¼¼åº¦ä½è¡¨ç¤ºè¯é¢˜è½¬æ¢
```

##### ä¼˜åŒ– 4ï¼šæ‘˜è¦è´¨é‡æ§åˆ¶

```python
def summarize_with_quality_check(state: ChatState):
    """å¸¦è´¨é‡æ£€æŸ¥çš„æ‘˜è¦ç”Ÿæˆ"""
    # ç”Ÿæˆæ‘˜è¦
    summary_prompt = "Create a comprehensive summary..."
    messages = state["messages"] + [HumanMessage(summary_prompt)]
    summary_response = model.invoke(messages)
    summary = summary_response.content

    # è´¨é‡æ£€æŸ¥ 1ï¼šé•¿åº¦æ£€æŸ¥
    if len(summary.split()) < 10:
        # æ‘˜è¦å¤ªçŸ­ï¼Œé‡æ–°ç”Ÿæˆ
        retry_prompt = "Create a MORE DETAILED summary..."
        summary = model.invoke([HumanMessage(retry_prompt)]).content

    # è´¨é‡æ£€æŸ¥ 2ï¼šå®Œæ•´æ€§æ£€æŸ¥
    if not contains_key_info(summary, state["messages"]):
        # æ‘˜è¦ç¼ºå°‘å…³é”®ä¿¡æ¯ï¼Œé‡æ–°ç”Ÿæˆ
        detailed_prompt = "Include: user name, main topics, key facts..."
        summary = model.invoke([HumanMessage(detailed_prompt)]).content

    # ...
    return {"summary": summary, "messages": delete_messages}

def contains_key_info(summary: str, messages: list) -> bool:
    """æ£€æŸ¥æ‘˜è¦æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯"""
    # æå–ç”¨æˆ·å
    user_names = extract_names(messages)
    for name in user_names:
        if name.lower() not in summary.lower():
            return False

    # æ£€æŸ¥ä¸»è¦è¯é¢˜
    topics = extract_topics(messages)
    topic_covered = sum(1 for topic in topics if topic in summary)
    return topic_covered >= len(topics) * 0.7  # 70% çš„è¯é¢˜è¢«è¦†ç›–
```

#### å®Œæ•´çš„ç”Ÿäº§çº§å®ç°

```python
class ProductionChatState(MessagesState):
    """ç”Ÿäº§çº§èŠå¤©çŠ¶æ€"""
    summary: str
    summary_count: int = 0      # æ‘˜è¦æ¬¡æ•°
    total_messages: int = 0     # æ€»æ¶ˆæ¯æ•°
    last_summary_at: int = 0    # ä¸Šæ¬¡æ‘˜è¦æ—¶çš„æ¶ˆæ¯æ•°
    summary_strategy: str = "adaptive"  # æ‘˜è¦ç­–ç•¥

def adaptive_summarize(state: ProductionChatState):
    """è‡ªé€‚åº”æ‘˜è¦ç­–ç•¥"""
    msg_count = len(state["messages"])
    total_msgs = state["total_messages"]
    strategy = state["summary_strategy"]

    if strategy == "aggressive":
        # æ¿€è¿›ç­–ç•¥ï¼šé¢‘ç¹æ‘˜è¦ï¼Œæœ€å°åŒ–å†…å­˜
        keep_last = 2
    elif strategy == "balanced":
        # å¹³è¡¡ç­–ç•¥ï¼šé€‚åº¦æ‘˜è¦
        keep_last = 5
    elif strategy == "conservative":
        # ä¿å®ˆç­–ç•¥ï¼šå°½é‡ä¿ç•™å†å²
        keep_last = 10
    else:  # adaptive
        # è‡ªé€‚åº”ï¼šæ ¹æ®å¯¹è¯é•¿åº¦åŠ¨æ€è°ƒæ•´
        if total_msgs < 20:
            keep_last = 5
        elif total_msgs < 50:
            keep_last = 3
        else:
            keep_last = 2

    # ç”Ÿæˆæ‘˜è¦
    summary = generate_summary(state)

    # åˆ é™¤æ—§æ¶ˆæ¯
    delete_ops = [
        RemoveMessage(id=m.id)
        for m in state["messages"][:-keep_last]
    ]

    return {
        "summary": summary,
        "messages": delete_ops,
        "summary_count": state["summary_count"] + 1,
        "last_summary_at": total_msgs
    }
```

#### å…³é”®æ´å¯Ÿ

> **æ¶ˆæ¯æ‘˜è¦ç³»ç»Ÿçš„æ ¸å¿ƒè¦ç´ ï¼š**
>
> 1. **è§¦å‘æœºåˆ¶**ï¼šä½•æ—¶æ‘˜è¦ï¼ˆæ¶ˆæ¯æ•°/Token/æ—¶é—´ï¼‰
> 2. **æ‘˜è¦ç­–ç•¥**ï¼šå¦‚ä½•æ‘˜è¦ï¼ˆé¦–æ¬¡/å¢é‡/ç»“æ„åŒ–ï¼‰
> 3. **ä¿®å‰ªç­–ç•¥**ï¼šä¿ç•™ä»€ä¹ˆï¼ˆæœ€è¿‘ N æ¡ï¼‰
> 4. **è´¨é‡æ§åˆ¶**ï¼šå¦‚ä½•ç¡®ä¿æ‘˜è¦è´¨é‡
>
> **æœ€ä½³å®è·µï¼š**
> - ä»ç®€å•å¼€å§‹ï¼ˆæ¶ˆæ¯æ•°é‡è§¦å‘ï¼‰
> - é€æ­¥ä¼˜åŒ–ï¼ˆToken æ§åˆ¶ã€ç»“æ„åŒ–ï¼‰
> - ç›‘æ§æ•ˆæœï¼ˆæ‘˜è¦è´¨é‡ã€æˆæœ¬èŠ‚çœï¼‰
> - æ ¹æ®ä¸šåŠ¡è°ƒæ•´ï¼ˆä¸åŒåœºæ™¯ä¸åŒç­–ç•¥ï¼‰

</details>

---

I'll continue with questions 7-15 in the next response to complete the review document. Let me know when you're ready to continue.
### é—®é¢˜ 7ï¼šå¦‚ä½•é…ç½®å¤–éƒ¨æ•°æ®åº“å®ç°è·¨ä¼šè¯çš„æŒä¹…åŒ–å­˜å‚¨ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### æŒä¹…åŒ–å­˜å‚¨çš„æ ¸å¿ƒæ¦‚å¿µ

**Checkpointer** æ˜¯ LangGraph çš„çŠ¶æ€æŒä¹…åŒ–æœºåˆ¶ï¼Œè´Ÿè´£ï¼š
1. ä¿å­˜æ¯ä¸€æ­¥çš„çŠ¶æ€å¿«ç…§
2. æ”¯æŒçŠ¶æ€æ¢å¤å’Œæ—¶é—´æ—…è¡Œ
3. å®ç°è·¨ä¼šè¯çš„å¯¹è¯è®°å¿†

**ä¸‰ç§ Checkpointer å¯¹æ¯”ï¼š**

| ç±»å‹ | å­˜å‚¨ä½ç½® | æŒä¹…åŒ– | å¹¶å‘ | é€‚ç”¨åœºæ™¯ |
|------|---------|--------|------|---------|
| `MemorySaver` | è¿›ç¨‹å†…å­˜ | âŒ | å•è¿›ç¨‹ | å¼€å‘æµ‹è¯• |
| `SqliteSaver` | SQLite æ–‡ä»¶ | âœ… | å•æœº | ä¸­å°åº”ç”¨ |
| `PostgresSaver` | PostgreSQL | âœ… | åˆ†å¸ƒå¼ | ä¼ä¸šåº”ç”¨ |

#### æ–¹æ¡ˆ 1ï¼šMemorySaverï¼ˆå¼€å‘ç¯å¢ƒï¼‰

**ä»£ç ç¤ºä¾‹ï¼š**

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph

# åˆ›å»ºå†…å­˜ checkpointer
memory = MemorySaver()

# ç¼–è¯‘å›¾æ—¶æŒ‡å®š
graph = workflow.compile(checkpointer=memory)

# ä½¿ç”¨ï¼ˆçŠ¶æ€åªåœ¨è¿›ç¨‹è¿è¡ŒæœŸé—´å­˜åœ¨ï¼‰
config = {"configurable": {"thread_id": "1"}}
graph.invoke({"messages": [...]}, config)
```

**ç‰¹ç‚¹ï¼š**
- âœ… ç®€å•å¿«é€Ÿï¼Œæ— éœ€é…ç½®
- âœ… å¼€å‘è°ƒè¯•æ–¹ä¾¿
- âŒ è¿›ç¨‹é‡å¯åæ•°æ®ä¸¢å¤±
- âŒ ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒ

#### æ–¹æ¡ˆ 2ï¼šSqliteSaverï¼ˆç”Ÿäº§å°è§„æ¨¡ï¼‰

**å®Œæ•´é…ç½®æ­¥éª¤ï¼š**

```python
import sqlite3
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, MessagesState

# æ­¥éª¤ 1ï¼šåˆ›å»º SQLite è¿æ¥
# æ–¹å¼ Aï¼šå†…å­˜æ•°æ®åº“ï¼ˆä¸´æ—¶æµ‹è¯•ï¼‰
conn = sqlite3.connect(":memory:", check_same_thread=False)

# æ–¹å¼ Bï¼šæ–‡ä»¶æ•°æ®åº“ï¼ˆæŒä¹…åŒ–ï¼‰
conn = sqlite3.connect("chatbot.db", check_same_thread=False)

# æ­¥éª¤ 2ï¼šåˆ›å»º SqliteSaver
memory = SqliteSaver(conn)
# SqliteSaver ä¼šè‡ªåŠ¨åˆ›å»ºå¿…è¦çš„è¡¨ï¼š
# - checkpointsï¼šå­˜å‚¨çŠ¶æ€å¿«ç…§
# - checkpoint_writesï¼šå­˜å‚¨å†™å…¥æ“ä½œ

# æ­¥éª¤ 3ï¼šç¼–è¯‘å›¾
class State(MessagesState):
    summary: str

workflow = StateGraph(State)
# ... æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ ...

graph = workflow.compile(checkpointer=memory)

# æ­¥éª¤ 4ï¼šä½¿ç”¨
config = {"configurable": {"thread_id": "user_123"}}

# ç¬¬ä¸€æ¬¡å¯¹è¯
graph.invoke({"messages": [HumanMessage("Hi")]}, config)

# ç¨‹åºé‡å¯å...
# é‡æ–°è¿æ¥æ•°æ®åº“
conn = sqlite3.connect("chatbot.db", check_same_thread=False)
memory = SqliteSaver(conn)
graph = workflow.compile(checkpointer=memory)

# ç»§ç»­ä¹‹å‰çš„å¯¹è¯ï¼ˆçŠ¶æ€è‡ªåŠ¨æ¢å¤ï¼ï¼‰
result = graph.invoke({"messages": [HumanMessage("ç»§ç»­")]}, config)
# âœ… èƒ½å¤Ÿè®¿é—®ä¹‹å‰çš„å¯¹è¯å†å²
```

**SQLite æ•°æ®åº“ç»“æ„ï¼š**

```sql
-- checkpoints è¡¨
CREATE TABLE checkpoints (
    thread_id TEXT,              -- å¯¹è¯çº¿ç¨‹ ID
    checkpoint_ns TEXT,          -- å‘½åç©ºé—´
    checkpoint_id TEXT,          -- æ£€æŸ¥ç‚¹ IDï¼ˆUUIDï¼‰
    parent_checkpoint_id TEXT,   -- çˆ¶æ£€æŸ¥ç‚¹ ID
    type TEXT,                   -- ç±»å‹
    checkpoint BLOB,             -- åºåˆ—åŒ–çš„çŠ¶æ€æ•°æ®
    metadata BLOB,               -- å…ƒæ•°æ®
    created_at TIMESTAMP,        -- åˆ›å»ºæ—¶é—´
    PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id)
);

-- checkpoint_writes è¡¨
CREATE TABLE checkpoint_writes (
    thread_id TEXT,
    checkpoint_ns TEXT,
    checkpoint_id TEXT,
    task_id TEXT,
    idx INTEGER,
    channel TEXT,
    type TEXT,
    value BLOB,
    PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id, task_id, idx)
);
```

**é«˜çº§é…ç½®ï¼š**

```python
# é…ç½® 1ï¼šè®¾ç½®æ•°æ®åº“è·¯å¾„
import os

DB_DIR = "data/checkpoints"
os.makedirs(DB_DIR, exist_ok=True)

db_path = os.path.join(DB_DIR, "chatbot.db")
conn = sqlite3.connect(db_path, check_same_thread=False)

# é…ç½® 2ï¼šå¯ç”¨ WAL æ¨¡å¼ï¼ˆæé«˜å¹¶å‘æ€§èƒ½ï¼‰
conn.execute("PRAGMA journal_mode=WAL")

# é…ç½® 3ï¼šè®¾ç½®ç¼“å­˜å¤§å°ï¼ˆæé«˜æ€§èƒ½ï¼‰
conn.execute("PRAGMA cache_size=10000")  # 10000 pages (~40MB)

memory = SqliteSaver(conn)

# é…ç½® 4ï¼šå®šæœŸå¤‡ä»½
import shutil
from datetime import datetime

def backup_database():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = f"backups/chatbot_{timestamp}.db"
    shutil.copy("data/checkpoints/chatbot.db", backup_path)
    print(f"Backup created: {backup_path}")

# æ¯å¤©å¤‡ä»½ä¸€æ¬¡
backup_database()
```

#### æ–¹æ¡ˆ 3ï¼šPostgresSaverï¼ˆä¼ä¸šçº§ï¼‰

**é…ç½®æ­¥éª¤ï¼š**

```python
from langgraph.checkpoint.postgres import PostgresSaver
import asyncpg

# æ­¥éª¤ 1ï¼šåˆ›å»º PostgreSQL è¿æ¥
DATABASE_URL = "postgresql://user:password@localhost:5432/langgraph"

# å¼‚æ­¥ç‰ˆæœ¬
async def setup_postgres():
    pool = await asyncpg.create_pool(DATABASE_URL)
    memory = PostgresSaver(pool)
    return memory

# åŒæ­¥ç‰ˆæœ¬ï¼ˆä½¿ç”¨ psycopg2ï¼‰
from langgraph.checkpoint.postgres import PostgresSaver
import psycopg2

conn = psycopg2.connect(DATABASE_URL)
memory = PostgresSaver(conn)

# æ­¥éª¤ 2ï¼šåˆå§‹åŒ–æ•°æ®åº“è¡¨
# PostgresSaver ä¼šè‡ªåŠ¨åˆ›å»ºå¿…è¦çš„è¡¨
graph = workflow.compile(checkpointer=memory)

# æ­¥éª¤ 3ï¼šä½¿ç”¨
config = {"configurable": {"thread_id": "user_123"}}
graph.invoke({"messages": [...]}, config)
```

**PostgreSQL ä¼˜åŠ¿ï¼š**

```python
# 1. æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²
# å¤šä¸ªæœåŠ¡å™¨å®ä¾‹å…±äº«åŒä¸€ä¸ªæ•°æ®åº“
server_1 = create_app(PostgresSaver(conn))
server_2 = create_app(PostgresSaver(conn))
# ä¸¤ä¸ªæœåŠ¡å™¨å¯ä»¥è®¿é—®ç›¸åŒçš„å¯¹è¯çŠ¶æ€

# 2. é«˜å¹¶å‘æ”¯æŒ
# PostgreSQL çš„ MVCC æœºåˆ¶æ”¯æŒé«˜å¹¶å‘è¯»å†™

# 3. äº‹åŠ¡æ”¯æŒ
with conn:
    with conn.cursor() as cur:
        # åŸå­æ€§æ“ä½œ
        cur.execute("UPDATE checkpoints SET ...")
        cur.execute("INSERT INTO checkpoint_writes ...")

# 4. é«˜çº§æŸ¥è¯¢
# å¯ä»¥ä½¿ç”¨ SQL æŸ¥è¯¢åˆ†æå¯¹è¯æ•°æ®
conn.execute("""
    SELECT thread_id, COUNT(*) as checkpoint_count
    FROM checkpoints
    GROUP BY thread_id
    ORDER BY checkpoint_count DESC
    LIMIT 10
""")
```

#### Thread ç®¡ç†è¯¦è§£

**Thread çš„ä½œç”¨ï¼š**

Threadï¼ˆçº¿ç¨‹ï¼‰æ˜¯ä¸€ç»„ç›¸å…³çŠ¶æ€å¿«ç…§çš„é›†åˆï¼Œç”¨äºï¼š
1. éš”ç¦»ä¸åŒç”¨æˆ·çš„å¯¹è¯
2. æ”¯æŒåŒä¸€ç”¨æˆ·çš„å¤šä¸ªä¼šè¯
3. å®ç°å¯¹è¯çš„åˆ†ç»„å’Œç®¡ç†

**Thread ID è®¾è®¡æ¨¡å¼ï¼š**

```python
# æ¨¡å¼ 1ï¼šåŸºäºç”¨æˆ· ID
def get_user_config(user_id: str):
    return {"configurable": {"thread_id": f"user_{user_id}"}}

# æ¨¡å¼ 2ï¼šåŸºäºä¼šè¯ ID
import uuid
def create_session():
    session_id = str(uuid.uuid4())
    return {"configurable": {"thread_id": session_id}}

# æ¨¡å¼ 3ï¼šç»„åˆæ¨¡å¼ï¼ˆç”¨æˆ· + åœºæ™¯ï¼‰
def get_context_config(user_id: str, context: str):
    thread_id = f"{user_id}_{context}"
    return {"configurable": {"thread_id": thread_id}}

# ä½¿ç”¨ç¤ºä¾‹ï¼š
config = get_context_config("alice", "project_a")
# thread_id = "alice_project_a"
```

**å¤šçº¿ç¨‹ç®¡ç†ï¼š**

```python
class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""

    def __init__(self, checkpointer):
        self.checkpointer = checkpointer
        self.graph = workflow.compile(checkpointer=checkpointer)

    def start_conversation(self, user_id: str, context: str = "default"):
        """å¼€å§‹æ–°å¯¹è¯"""
        thread_id = f"{user_id}_{context}_{uuid.uuid4()}"
        config = {"configurable": {"thread_id": thread_id}}
        return config

    def get_conversation(self, thread_id: str):
        """è·å–å¯¹è¯çŠ¶æ€"""
        config = {"configurable": {"thread_id": thread_id}}
        return self.graph.get_state(config)

    def list_user_conversations(self, user_id: str):
        """åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰å¯¹è¯"""
        # æŸ¥è¯¢æ•°æ®åº“
        query = """
            SELECT DISTINCT thread_id, created_at
            FROM checkpoints
            WHERE thread_id LIKE ?
            ORDER BY created_at DESC
        """
        results = conn.execute(query, (f"{user_id}_%",)).fetchall()
        return results

    def delete_conversation(self, thread_id: str):
        """åˆ é™¤å¯¹è¯"""
        conn.execute("DELETE FROM checkpoints WHERE thread_id = ?", (thread_id,))
        conn.execute("DELETE FROM checkpoint_writes WHERE thread_id = ?", (thread_id,))
        conn.commit()
```

#### çŠ¶æ€æŸ¥è¯¢å’Œæ¢å¤

**API è¯¦è§£ï¼š**

```python
# 1. get_stateï¼šè·å–å½“å‰çŠ¶æ€
config = {"configurable": {"thread_id": "user_123"}}
state_snapshot = graph.get_state(config)

# StateSnapshot å¯¹è±¡åŒ…å«ï¼š
state_snapshot.values         # å½“å‰çŠ¶æ€çš„æ‰€æœ‰æ•°æ®
state_snapshot.next          # ä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„èŠ‚ç‚¹
state_snapshot.config        # é…ç½®ä¿¡æ¯
state_snapshot.metadata      # å…ƒæ•°æ®ï¼ˆæ­¥æ•°ã€æ¥æºç­‰ï¼‰
state_snapshot.created_at    # åˆ›å»ºæ—¶é—´
state_snapshot.parent_config # çˆ¶çŠ¶æ€é…ç½®

# 2. get_state_historyï¼šè·å–å†å²çŠ¶æ€
history = graph.get_state_history(config)

# éå†å†å²
for state in history:
    print(f"Step {state.metadata['step']}: {state.values}")

# 3. update_stateï¼šæ‰‹åŠ¨æ›´æ–°çŠ¶æ€
new_state = {"messages": [HumanMessage("Manual update")]}
graph.update_state(config, new_state)

# 4. æ—¶é—´æ—…è¡Œï¼šå›æº¯åˆ°å†å²çŠ¶æ€
# è·å–å†å²çŠ¶æ€åˆ—è¡¨
history_list = list(graph.get_state_history(config))

# é€‰æ‹©ç‰¹å®šå†å²çŠ¶æ€
past_state = history_list[5]  # ç¬¬ 5 ä¸ªå†å²çŠ¶æ€

# ä»è¯¥çŠ¶æ€ç»§ç»­æ‰§è¡Œ
result = graph.invoke(
    {"messages": [HumanMessage("Continue from past")]},
    past_state.config  # ä½¿ç”¨å†å²çŠ¶æ€çš„ config
)
```

#### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# æŠ€å·§ 1ï¼šæ‰¹é‡æ’å…¥
def bulk_save_conversations(conversations):
    """æ‰¹é‡ä¿å­˜å¤šä¸ªå¯¹è¯"""
    conn = sqlite3.connect("chatbot.db")
    cursor = conn.cursor()

    for conv in conversations:
        cursor.execute("INSERT INTO checkpoints (...) VALUES (...)")

    conn.commit()  # ä¸€æ¬¡æ€§æäº¤

# æŠ€å·§ 2ï¼šå®šæœŸæ¸…ç†
def cleanup_old_checkpoints(days=30):
    """åˆ é™¤è¶…è¿‡ N å¤©çš„æ£€æŸ¥ç‚¹"""
    from datetime import datetime, timedelta

    cutoff = datetime.now() - timedelta(days=days)
    conn.execute(
        "DELETE FROM checkpoints WHERE created_at < ?",
        (cutoff,)
    )
    conn.commit()

# æŠ€å·§ 3ï¼šç´¢å¼•ä¼˜åŒ–
def create_indexes():
    """åˆ›å»ºç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½"""
    conn.execute("CREATE INDEX IF NOT EXISTS idx_thread_id ON checkpoints(thread_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON checkpoints(created_at)")
    conn.commit()

# æŠ€å·§ 4ï¼šè¿æ¥æ± 
from contextlib import contextmanager

class ConnectionPool:
    def __init__(self, db_path, pool_size=5):
        self.db_path = db_path
        self.pool = [
            sqlite3.connect(db_path, check_same_thread=False)
            for _ in range(pool_size)
        ]
        self.available = self.pool.copy()

    @contextmanager
    def get_connection(self):
        conn = self.available.pop()
        try:
            yield conn
        finally:
            self.available.append(conn)

# ä½¿ç”¨
pool = ConnectionPool("chatbot.db")
with pool.get_connection() as conn:
    memory = SqliteSaver(conn)
    # ...
```

#### ç›‘æ§å’Œè°ƒè¯•

```python
# 1. æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡
def get_database_stats():
    """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
    stats = {}

    # æ€»æ£€æŸ¥ç‚¹æ•°
    result = conn.execute("SELECT COUNT(*) FROM checkpoints").fetchone()
    stats["total_checkpoints"] = result[0]

    # æ´»è·ƒçº¿ç¨‹æ•°
    result = conn.execute("SELECT COUNT(DISTINCT thread_id) FROM checkpoints").fetchone()
    stats["active_threads"] = result[0]

    # æ•°æ®åº“å¤§å°
    import os
    stats["db_size_mb"] = os.path.getsize("chatbot.db") / (1024 * 1024)

    return stats

# 2. å¯¼å‡ºå¯¹è¯å†å²
def export_conversation(thread_id: str, output_path: str):
    """å¯¼å‡ºå¯¹è¯åˆ° JSON æ–‡ä»¶"""
    import json

    config = {"configurable": {"thread_id": thread_id}}
    history = list(graph.get_state_history(config))

    data = []
    for state in history:
        data.append({
            "step": state.metadata.get("step"),
            "values": state.values,
            "timestamp": state.created_at
        })

    with open(output_path, "w") as f:
        json.dump(data, f, indent=2)

# 3. æ—¥å¿—è®°å½•
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def logged_invoke(graph, input_data, config):
    """å¸¦æ—¥å¿—çš„å›¾æ‰§è¡Œ"""
    thread_id = config["configurable"]["thread_id"]
    logger.info(f"Starting execution for thread: {thread_id}")

    try:
        result = graph.invoke(input_data, config)
        logger.info(f"Execution completed for thread: {thread_id}")
        return result
    except Exception as e:
        logger.error(f"Execution failed for thread {thread_id}: {e}")
        raise
```

#### å…³é”®æ´å¯Ÿ

> **æŒä¹…åŒ–å­˜å‚¨çš„æœ€ä½³å®è·µï¼š**
>
> 1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨ MemorySaverï¼Œå¿«é€Ÿè¿­ä»£
> 2. **æµ‹è¯•é˜¶æ®µ**ï¼šä½¿ç”¨ SqliteSaverï¼ˆå†…å­˜æ¨¡å¼ï¼‰ï¼ŒéªŒè¯é€»è¾‘
> 3. **ç”Ÿäº§é˜¶æ®µ**ï¼šä½¿ç”¨ SqliteSaverï¼ˆæ–‡ä»¶æ¨¡å¼ï¼‰æˆ– PostgresSaver
> 4. **ä¼ä¸šé˜¶æ®µ**ï¼šä½¿ç”¨ PostgresSaver + åˆ†å¸ƒå¼æ¶æ„
>
> **Thread ID è®¾è®¡åŸåˆ™ï¼š**
> - ç¡®ä¿å”¯ä¸€æ€§ï¼ˆé¿å…å†²çªï¼‰
> - æœ‰æ„ä¹‰ï¼ˆä¾¿äºæŸ¥è¯¢å’Œè°ƒè¯•ï¼‰
> - å¯æ‰©å±•ï¼ˆæ”¯æŒæœªæ¥éœ€æ±‚ï¼‰

</details>

---

### é—®é¢˜ 8ï¼šè®¾è®¡ä¸€ä¸ªå®Œæ•´çš„å®¢æœèŠå¤©æœºå™¨äººï¼Œéœ€è¦è€ƒè™‘å“ªäº›çŠ¶æ€ç®¡ç†è¦ç‚¹ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### ç³»ç»Ÿéœ€æ±‚åˆ†æ

ä¸€ä¸ªç”Ÿäº§çº§å®¢æœèŠå¤©æœºå™¨äººéœ€è¦è€ƒè™‘ï¼š

```
1. çŠ¶æ€ç®¡ç†
   â”œâ”€ ç”¨æˆ·ä¿¡æ¯ï¼ˆå§“åã€IDã€æƒé™ï¼‰
   â”œâ”€ å¯¹è¯å†å²ï¼ˆæ¶ˆæ¯ã€æ‘˜è¦ï¼‰
   â”œâ”€ ä¼šè¯ä¸Šä¸‹æ–‡ï¼ˆé—®é¢˜ç±»å‹ã€ä¼˜å…ˆçº§ï¼‰
   â””â”€ ç³»ç»ŸçŠ¶æ€ï¼ˆç»Ÿè®¡ã€æ—¥å¿—ï¼‰

2. æ€§èƒ½ä¼˜åŒ–
   â”œâ”€ æ¶ˆæ¯è£å‰ªï¼ˆToken æ§åˆ¶ï¼‰
   â”œâ”€ æ™ºèƒ½æ‘˜è¦ï¼ˆé•¿å¯¹è¯æ”¯æŒï¼‰
   â””â”€ ç¼“å­˜æœºåˆ¶ï¼ˆå¸¸è§é—®é¢˜ï¼‰

3. æ•°æ®æŒä¹…åŒ–
   â”œâ”€ PostgreSQLï¼ˆçŠ¶æ€å­˜å‚¨ï¼‰
   â”œâ”€ Redisï¼ˆç¼“å­˜å±‚ï¼‰
   â””â”€ S3ï¼ˆå†å²å½’æ¡£ï¼‰

4. å®‰å…¨ä¸éšç§
   â”œâ”€ æ•°æ®åŠ å¯†
   â”œâ”€ è®¿é—®æ§åˆ¶
   â””â”€ æ•æ„Ÿä¿¡æ¯è„±æ•
```

#### å®Œæ•´æ¶æ„è®¾è®¡

```python
from typing_extensions import TypedDict
from typing import Annotated, Literal
from datetime import datetime
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.postgres import PostgresSaver
from langchain_openai import ChatOpenAI
import operator

# ============= çŠ¶æ€å®šä¹‰ =============

class CustomerInfo(TypedDict):
    """å®¢æˆ·ä¿¡æ¯"""
    user_id: str
    name: str
    email: str
    tier: Literal["basic", "premium", "enterprise"]  # å®¢æˆ·ç­‰çº§
    language: str  # ç”¨æˆ·è¯­è¨€

class IssueInfo(TypedDict):
    """é—®é¢˜ä¿¡æ¯"""
    category: Literal["technical", "billing", "general"]
    priority: Literal["low", "medium", "high", "urgent"]
    status: Literal["open", "in_progress", "resolved"]
    tags: list[str]

class SystemMetrics(TypedDict):
    """ç³»ç»ŸæŒ‡æ ‡"""
    message_count: int
    llm_calls: int
    total_tokens: int
    session_start: datetime
    last_activity: datetime

class CustomerServiceState(MessagesState):
    """å®¢æœæœºå™¨äººçŠ¶æ€"""
    # å®¢æˆ·ä¿¡æ¯
    customer: CustomerInfo

    # é—®é¢˜ä¿¡æ¯
    issue: IssueInfo

    # å¯¹è¯ç®¡ç†
    summary: str  # å¯¹è¯æ‘˜è¦
    short_term_memory: list[str]  # çŸ­æœŸè®°å¿†ï¼ˆæœ€è¿‘ 3 è½®ï¼‰

    # ç³»ç»ŸæŒ‡æ ‡
    metrics: SystemMetrics

    # åŠŸèƒ½æ ‡å¿—
    needs_human: bool  # æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥
    satisfaction_score: float  # æ»¡æ„åº¦è¯„åˆ†

# ============= èŠ‚ç‚¹å®ç° =============

# åˆå§‹åŒ–æ¨¡å‹
model = ChatOpenAI(model="gpt-4", temperature=0)

def route_message(state: CustomerServiceState) -> Literal["handle_technical", "handle_billing", "handle_general"]:
    """
    è·¯ç”±èŠ‚ç‚¹ï¼šæ ¹æ®é—®é¢˜ç±»å‹åˆ†å‘
    """
    last_message = state["messages"][-1].content

    # ä½¿ç”¨ LLM åˆ†ç±»
    classifier_prompt = f"""
    Classify this customer message into one category:
    - technical: Technical issues, bugs, errors
    - billing: Payment, invoices, subscriptions
    - general: General questions, feedback

    Message: {last_message}

    Return only the category name.
    """

    category = model.invoke([HumanMessage(classifier_prompt)]).content.strip().lower()

    # æ›´æ–°çŠ¶æ€
    state["issue"]["category"] = category

    return f"handle_{category}"

def handle_technical(state: CustomerServiceState):
    """
    æŠ€æœ¯æ”¯æŒèŠ‚ç‚¹
    """
    # 1. å‡†å¤‡ä¸Šä¸‹æ–‡
    customer = state["customer"]
    summary = state.get("summary", "")

    system_prompt = f"""
    You are a technical support specialist.
    Customer: {customer['name']} ({customer['tier']} tier)
    Previous conversation summary: {summary}

    Provide clear, step-by-step technical solutions.
    """

    # 2. å‡†å¤‡æ¶ˆæ¯ï¼ˆåŒ…å«æ‘˜è¦ + æœ€è¿‘æ¶ˆæ¯ï¼‰
    messages = [SystemMessage(system_prompt)]

    # æ·»åŠ çŸ­æœŸè®°å¿†
    if state.get("short_term_memory"):
        for memory in state["short_term_memory"]:
            messages.append(SystemMessage(f"Recent context: {memory}"))

    # æ·»åŠ æœ€è¿‘æ¶ˆæ¯
    messages.extend(state["messages"][-5:])

    # 3. è°ƒç”¨ LLM
    response = model.invoke(messages)

    # 4. æ›´æ–°æŒ‡æ ‡
    metrics = state["metrics"]
    metrics["llm_calls"] += 1
    metrics["message_count"] += 1

    return {
        "messages": [response],
        "metrics": metrics,
        "issue": {"status": "in_progress"}
    }

def handle_billing(state: CustomerServiceState):
    """
    è´¦å•æ”¯æŒèŠ‚ç‚¹
    """
    customer = state["customer"]

    # æ£€æŸ¥å®¢æˆ·æƒé™
    if customer["tier"] == "basic":
        response = AIMessage(
            "For billing inquiries, please contact our billing department at billing@company.com"
        )
    else:
        # Premium/Enterprise å®¢æˆ·ç›´æ¥å¤„ç†
        system_prompt = "You are a billing specialist with access to account information..."
        messages = [SystemMessage(system_prompt)] + state["messages"][-3:]
        response = model.invoke(messages)

    return {"messages": [response]}

def handle_general(state: CustomerServiceState):
    """
    ä¸€èˆ¬å’¨è¯¢èŠ‚ç‚¹
    """
    # ä½¿ç”¨ç®€åŒ–çš„ä¸Šä¸‹æ–‡ï¼ˆèŠ‚çœ Tokenï¼‰
    messages = state["messages"][-3:]
    response = model.invoke(messages)

    return {"messages": [response]}

def check_escalation(state: CustomerServiceState) -> Literal["escalate", "summarize", "end"]:
    """
    æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§
    """
    issue = state["issue"]
    messages = state["messages"]

    # æ¡ä»¶ 1ï¼šé«˜ä¼˜å…ˆçº§é—®é¢˜
    if issue["priority"] == "urgent":
        return "escalate"

    # æ¡ä»¶ 2ï¼šå¯¹è¯è½®æ•°è¿‡å¤š
    if len(messages) > 10:
        return "summarize"

    # æ¡ä»¶ 3ï¼šæ£€æµ‹å®¢æˆ·æƒ…ç»ª
    last_message = messages[-1].content
    if detect_frustration(last_message):
        return "escalate"

    # æ¡ä»¶ 4ï¼šé—®é¢˜å·²è§£å†³
    if issue["status"] == "resolved":
        return "end"

    return "end"

def escalate_to_human(state: CustomerServiceState):
    """
    å‡çº§åˆ°äººå·¥å®¢æœ
    """
    customer = state["customer"]
    issue = state["issue"]

    # ç”Ÿæˆå‡çº§æŠ¥å‘Š
    report = f"""
    Escalation Request:
    - Customer: {customer['name']} ({customer['user_id']})
    - Issue: {issue['category']} - {issue['priority']}
    - Summary: {state.get('summary', 'No summary available')}
    - Messages: {len(state['messages'])}
    """

    # é€šçŸ¥ç³»ç»Ÿ
    notify_human_agent(report)

    # å›å¤å®¢æˆ·
    response = AIMessage(
        "I've escalated your issue to a human agent who will assist you shortly."
    )

    return {
        "messages": [response],
        "needs_human": True
    }

def summarize_conversation(state: CustomerServiceState):
    """
    å¯¹è¯æ‘˜è¦èŠ‚ç‚¹
    """
    messages = state["messages"]
    current_summary = state.get("summary", "")

    # ç”Ÿæˆæ‘˜è¦
    if current_summary:
        prompt = f"Current summary: {current_summary}\n\nUpdate with new messages:"
    else:
        prompt = "Summarize this customer service conversation:"

    summary_messages = messages + [HumanMessage(prompt)]
    summary = model.invoke(summary_messages).content

    # æ›´æ–°çŸ­æœŸè®°å¿†
    last_turn = f"User: {messages[-2].content} | Agent: {messages[-1].content}"
    short_term = state.get("short_term_memory", [])
    short_term.append(last_turn)
    if len(short_term) > 3:
        short_term = short_term[-3:]  # åªä¿ç•™æœ€è¿‘ 3 è½®

    # åˆ é™¤æ—§æ¶ˆæ¯
    from langchain_core.messages import RemoveMessage
    delete_ops = [RemoveMessage(id=m.id) for m in messages[:-4]]

    return {
        "summary": summary,
        "short_term_memory": short_term,
        "messages": delete_ops
    }

# ============= è¾…åŠ©å‡½æ•° =============

def detect_frustration(text: str) -> bool:
    """æ£€æµ‹å®¢æˆ·æƒ…ç»ª"""
    frustration_keywords = [
        "angry", "frustrated", "terrible", "worst",
        "unacceptable", "ridiculous", "cancel"
    ]
    return any(keyword in text.lower() for keyword in frustration_keywords)

def notify_human_agent(report: str):
    """é€šçŸ¥äººå·¥å®¢æœ"""
    # å‘é€åˆ°é˜Ÿåˆ—ã€Slackã€é‚®ä»¶ç­‰
    print(f"[ESCALATION] {report}")

# ============= æ„å»ºå›¾ =============

workflow = StateGraph(CustomerServiceState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("route", route_message)
workflow.add_node("handle_technical", handle_technical)
workflow.add_node("handle_billing", handle_billing)
workflow.add_node("handle_general", handle_general)
workflow.add_node("check_escalation", check_escalation)
workflow.add_node("escalate", escalate_to_human)
workflow.add_node("summarize", summarize_conversation)

# æ·»åŠ è¾¹
workflow.add_edge(START, "route")
workflow.add_conditional_edges("route", lambda s: s["issue"]["category"], {
    "technical": "handle_technical",
    "billing": "handle_billing",
    "general": "handle_general"
})

# æ‰€æœ‰å¤„ç†èŠ‚ç‚¹éƒ½åˆ°æ£€æŸ¥èŠ‚ç‚¹
workflow.add_edge("handle_technical", "check_escalation")
workflow.add_edge("handle_billing", "check_escalation")
workflow.add_edge("handle_general", "check_escalation")

# æ¡ä»¶åˆ†æ”¯
workflow.add_conditional_edges("check_escalation", check_escalation, {
    "escalate": "escalate",
    "summarize": "summarize",
    "end": END
})

workflow.add_edge("escalate", END)
workflow.add_edge("summarize", END)

# ç¼–è¯‘ï¼ˆä½¿ç”¨ PostgreSQLï¼‰
import asyncpg
pool = asyncpg.create_pool("postgresql://user:pass@localhost/customer_service")
memory = PostgresSaver(pool)

graph = workflow.compile(checkpointer=memory)

# ============= ä½¿ç”¨ç¤ºä¾‹ =============

def start_customer_session(user_id: str, name: str, tier: str):
    """å¼€å§‹å®¢æœä¼šè¯"""
    config = {"configurable": {"thread_id": f"customer_{user_id}"}}

    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "customer": {
            "user_id": user_id,
            "name": name,
            "email": f"{user_id}@example.com",
            "tier": tier,
            "language": "en"
        },
        "issue": {
            "category": "general",
            "priority": "medium",
            "status": "open",
            "tags": []
        },
        "metrics": {
            "message_count": 0,
            "llm_calls": 0,
            "total_tokens": 0,
            "session_start": datetime.now(),
            "last_activity": datetime.now()
        },
        "needs_human": False,
        "satisfaction_score": 0.0,
        "messages": []
    }

    return config, initial_state

# ä½¿ç”¨
config, initial_state = start_customer_session("user123", "Alice", "premium")

# ç¬¬ä¸€è½®å¯¹è¯
result = graph.invoke({
    **initial_state,
    "messages": [HumanMessage("My app keeps crashing")]
}, config)

# ç¬¬äºŒè½®å¯¹è¯
result = graph.invoke({
    "messages": [HumanMessage("I tried restarting but it didn't help")]
}, config)

# æŸ¥çœ‹çŠ¶æ€
state = graph.get_state(config)
print(f"Issue category: {state.values['issue']['category']}")
print(f"Status: {state.values['issue']['status']}")
print(f"Needs human: {state.values['needs_human']}")
```

#### å…³é”®è®¾è®¡è¦ç‚¹

**1. çŠ¶æ€åˆ†å±‚è®¾è®¡**

```python
# âœ… å¥½çš„è®¾è®¡ï¼šåˆ†å±‚æ¸…æ™°
class State(MessagesState):
    customer: CustomerInfo      # å®¢æˆ·å±‚
    issue: IssueInfo           # ä¸šåŠ¡å±‚
    summary: str               # å¯¹è¯å±‚
    metrics: SystemMetrics     # ç³»ç»Ÿå±‚

# âŒ ä¸å¥½çš„è®¾è®¡ï¼šå¹³é“ºæ‰€æœ‰å­—æ®µ
class State(MessagesState):
    user_id: str
    user_name: str
    issue_category: str
    issue_priority: str
    message_count: int
    # ... éš¾ä»¥ç»´æŠ¤
```

**2. æ™ºèƒ½è·¯ç”±**

```python
# æ ¹æ®é—®é¢˜ç±»å‹ã€å®¢æˆ·ç­‰çº§ã€ç´§æ€¥ç¨‹åº¦ç­‰å¤šç»´åº¦è·¯ç”±
def intelligent_route(state):
    # ç´§æ€¥é—®é¢˜ç›´æ¥å‡çº§
    if state["issue"]["priority"] == "urgent":
        return "escalate"

    # VIP å®¢æˆ·ä¼˜å…ˆå¤„ç†
    if state["customer"]["tier"] == "enterprise":
        return "vip_handler"

    # æŠ€æœ¯é—®é¢˜
    if state["issue"]["category"] == "technical":
        return "technical_support"

    # é»˜è®¤å¤„ç†
    return "general_handler"
```

**3. å¤šçº§æ‘˜è¦**

```python
class State(MessagesState):
    # çŸ­æœŸè®°å¿†ï¼šæœ€è¿‘ 3 è½®å¯¹è¯
    short_term_memory: list[str]

    # ä¸­æœŸæ‘˜è¦ï¼šæœ€è¿‘ 10 è½®çš„æ‘˜è¦
    medium_term_summary: str

    # é•¿æœŸæ‘˜è¦ï¼šæ•´ä¸ªä¼šè¯çš„æ‘˜è¦
    long_term_summary: str

# ä½¿ç”¨æ—¶åˆ†å±‚æä¾›ä¸Šä¸‹æ–‡
def prepare_context(state):
    context = []

    # 1. é•¿æœŸèƒŒæ™¯
    if state.get("long_term_summary"):
        context.append(f"Session background: {state['long_term_summary']}")

    # 2. ä¸­æœŸä¸Šä¸‹æ–‡
    if state.get("medium_term_summary"):
        context.append(f"Recent discussion: {state['medium_term_summary']}")

    # 3. çŸ­æœŸç»†èŠ‚
    if state.get("short_term_memory"):
        context.extend(state["short_term_memory"])

    return context
```

**4. ç›‘æ§å’Œåˆ†æ**

```python
class Metrics(TypedDict):
    # æ€§èƒ½æŒ‡æ ‡
    message_count: int
    llm_calls: int
    total_tokens: int
    avg_response_time: float

    # ä¸šåŠ¡æŒ‡æ ‡
    resolved_issues: int
    escalated_issues: int
    satisfaction_score: float

def update_metrics(state: CustomerServiceState):
    """æ›´æ–°æŒ‡æ ‡"""
    metrics = state["metrics"]

    # è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
    prometheus_client.gauge("customer_service_messages", metrics["message_count"])
    prometheus_client.gauge("customer_service_llm_calls", metrics["llm_calls"])

    # è®°å½•åˆ°æ•°æ®åº“
    analytics_db.insert({
        "user_id": state["customer"]["user_id"],
        "session_id": state["thread_id"],
        "metrics": metrics,
        "timestamp": datetime.now()
    })
```

#### å…³é”®æ´å¯Ÿ

> **å®¢æœæœºå™¨äººçš„çŠ¶æ€ç®¡ç†è¦ç‚¹ï¼š**
>
> 1. **ç»“æ„åŒ–çŠ¶æ€**ï¼šåˆ†å±‚ç®¡ç†ï¼ˆå®¢æˆ·/é—®é¢˜/å¯¹è¯/ç³»ç»Ÿï¼‰
> 2. **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®å¤šç»´åº¦åŠ¨æ€åˆ†å‘
> 3. **å¤šçº§è®°å¿†**ï¼šçŸ­æœŸ/ä¸­æœŸ/é•¿æœŸæ‘˜è¦
> 4. **æ€§èƒ½ä¼˜åŒ–**ï¼šToken æ§åˆ¶ã€ç¼“å­˜ã€æ‰¹å¤„ç†
> 5. **ç›‘æ§åˆ†æ**ï¼šå®æ—¶æŒ‡æ ‡ã€ç”¨æˆ·åé¦ˆ
> 6. **äººå·¥æ¥å…¥**ï¼šå‡çº§æœºåˆ¶ã€ä¸Šä¸‹æ–‡ä¼ é€’

</details>

---

### é—®é¢˜ 9ï¼šå¦‚ä½•ä¼˜åŒ– LangGraph çŠ¶æ€ç®¡ç†çš„ Token ä½¿ç”¨å’Œæˆæœ¬ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### Token æˆæœ¬åˆ†æ

**å…¸å‹èŠå¤©æœºå™¨äººçš„ Token æ¶ˆè€—ï¼š**

```
è½®æ¬¡ 1: è¾“å…¥ 50 + è¾“å‡º 100 = 150 tokens
è½®æ¬¡ 2: è¾“å…¥ 200 + è¾“å‡º 100 = 300 tokens  (ç´¯ç§¯å†å²)
è½®æ¬¡ 3: è¾“å…¥ 400 + è¾“å‡º 100 = 500 tokens
...
è½®æ¬¡ 10: è¾“å…¥ 2000 + è¾“å‡º 100 = 2100 tokens

æ€»æ¶ˆè€—: çº¦ 12,000 tokens
æˆæœ¬: $0.36 (æŒ‰ GPT-4 $0.03/1K tokens)
```

**ä¼˜åŒ–åï¼š**

```
è½®æ¬¡ 1-6: åŒä¸Š (çº¦ 6,000 tokens)
è½®æ¬¡ 7: è§¦å‘æ‘˜è¦ + è£å‰ª
   - æ‘˜è¦ç”Ÿæˆ: 500 tokens
   - åç»­æ¯è½®: 300 tokens (æ‘˜è¦ + æœ€è¿‘ 2 æ¡)

æ€»æ¶ˆè€—: çº¦ 7,500 tokens
æˆæœ¬: $0.225
èŠ‚çœ: 37.5%
```

#### ä¼˜åŒ–ç­–ç•¥ 1ï¼šæ™ºèƒ½æ¶ˆæ¯è£å‰ª

```python
from langchain_core.messages import trim_messages, SystemMessage

def token_optimized_chat(state: MessagesState):
    """Token ä¼˜åŒ–çš„å¯¹è¯èŠ‚ç‚¹"""
    messages = state["messages"]

    # 1. åˆ†ç¦»ç³»ç»Ÿæ¶ˆæ¯
    system_msgs = [m for m in messages if isinstance(m, SystemMessage)]
    chat_msgs = [m for m in messages if not isinstance(m, SystemMessage)]

    # 2. è®¡ç®—ç³»ç»Ÿæ¶ˆæ¯çš„ tokens
    system_tokens = sum(model.get_num_tokens(m.content) for m in system_msgs)

    # 3. è£å‰ªå¯¹è¯æ¶ˆæ¯
    # GPT-4: 8192 tokens ä¸Šä¸‹æ–‡
    # é¢„ç•™: 1000 tokens ç”¨äºå›å¤, 500 tokens ç¼“å†²
    available_tokens = 8192 - 1000 - 500 - system_tokens

    trimmed_chat = trim_messages(
        chat_msgs,
        max_tokens=available_tokens,
        strategy="last",
        token_counter=model,
        allow_partial=False
    )

    # 4. ç»„åˆå¹¶è°ƒç”¨
    final_messages = system_msgs + trimmed_chat
    response = model.invoke(final_messages)

    return {"messages": [response]}
```

#### ä¼˜åŒ–ç­–ç•¥ 2ï¼šåˆ†å±‚æ‘˜è¦ç³»ç»Ÿ

```python
class TokenOptimizedState(MessagesState):
    """Token ä¼˜åŒ–çš„çŠ¶æ€"""
    # ä¸‰å±‚æ‘˜è¦
    micro_summary: str      # æœ€è¿‘ 5 è½® (çº¦ 50 tokens)
    macro_summary: str      # æœ€è¿‘ 20 è½® (çº¦ 100 tokens)
    full_summary: str       # å®Œæ•´å†å² (çº¦ 150 tokens)

    # æ‘˜è¦å…ƒæ•°æ®
    last_micro_at: int = 0
    last_macro_at: int = 0
    last_full_at: int = 0

def should_summarize(state: TokenOptimizedState):
    """æ™ºèƒ½æ‘˜è¦è§¦å‘"""
    msg_count = len(state["messages"])

    # æ¯ 5 è½®æ›´æ–° micro
    if msg_count - state["last_micro_at"] >= 5:
        return "micro_summary"

    # æ¯ 20 è½®æ›´æ–° macro
    if msg_count - state["last_macro_at"] >= 20:
        return "macro_summary"

    # æ¯ 50 è½®æ›´æ–° full
    if msg_count - state["last_full_at"] >= 50:
        return "full_summary"

    return "continue"

def create_micro_summary(state: TokenOptimizedState):
    """åˆ›å»ºå¾®æ‘˜è¦ï¼ˆæœ€è¿‘ 5 è½®ï¼‰"""
    recent_messages = state["messages"][-10:]  # 5 è½® = 10 æ¡æ¶ˆæ¯

    prompt = "Briefly summarize the last 5 conversation turns (max 50 tokens):"
    summary = model.invoke(recent_messages + [HumanMessage(prompt)]).content

    # åˆ é™¤å·²æ‘˜è¦çš„æ¶ˆæ¯ï¼ˆä¿ç•™æœ€å 2 æ¡ï¼‰
    delete_ops = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]

    return {
        "micro_summary": summary,
        "messages": delete_ops,
        "last_micro_at": len(state["messages"])
    }

def prepare_context(state: TokenOptimizedState):
    """å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨åˆ†å±‚æ‘˜è¦ï¼‰"""
    context = []

    # 1. å®Œæ•´èƒŒæ™¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if state.get("full_summary"):
        context.append(SystemMessage(f"Background: {state['full_summary']}"))

    # 2. ä¸­æœŸä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
    if state.get("macro_summary"):
        context.append(SystemMessage(f"Recent: {state['macro_summary']}"))

    # 3. æœ€è¿‘ç»†èŠ‚ï¼ˆå¦‚æœæœ‰ï¼‰
    if state.get("micro_summary"):
        context.append(SystemMessage(f"Latest: {state['micro_summary']}"))

    # 4. å½“å‰æ¶ˆæ¯ï¼ˆæœ€å 2 æ¡ï¼‰
    context.extend(state["messages"][-2:])

    return context

# Token ä½¿ç”¨å¯¹æ¯”
# æ— æ‘˜è¦: 50 è½® Ã— å¹³å‡ 100 tokens = 5000 tokens
# åˆ†å±‚æ‘˜è¦: 150 (full) + 100 (macro) + 50 (micro) + 200 (æœ€è¿‘ 2 æ¡) = 500 tokens
# èŠ‚çœ: 90%ï¼
```

#### ä¼˜åŒ–ç­–ç•¥ 3ï¼šç¼“å­˜å¸¸è§é—®é¢˜

```python
from functools import lru_cache
import hashlib

class CachedState(MessagesState):
    """å¸¦ç¼“å­˜çš„çŠ¶æ€"""
    cache_hits: int = 0
    cache_misses: int = 0

# ä½¿ç”¨ LRU ç¼“å­˜
@lru_cache(maxsize=100)
def cached_llm_call(message_hash: str, message_content: str):
    """ç¼“å­˜ LLM è°ƒç”¨"""
    response = model.invoke([HumanMessage(message_content)])
    return response.content

def smart_chat_with_cache(state: CachedState):
    """å¸¦ç¼“å­˜çš„å¯¹è¯"""
    last_message = state["messages"][-1].content

    # è®¡ç®—æ¶ˆæ¯å“ˆå¸Œ
    message_hash = hashlib.md5(last_message.encode()).hexdigest()

    # å°è¯•ä»ç¼“å­˜è·å–
    try:
        cached_response = cached_llm_call(message_hash, last_message)
        response = AIMessage(cached_response)

        return {
            "messages": [response],
            "cache_hits": state["cache_hits"] + 1
        }
    except:
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸è°ƒç”¨
        response = model.invoke(state["messages"])

        return {
            "messages": [response],
            "cache_misses": state["cache_misses"] + 1
        }

# ç¼“å­˜æ•ˆæœ
# å‡è®¾ 20% çš„é—®é¢˜æ˜¯é‡å¤çš„
# æˆæœ¬èŠ‚çœ: 20% Ã— $0.03/1K = $0.006/1K tokens
```

#### ä¼˜åŒ–ç­–ç•¥ 4ï¼šåŠ¨æ€æ¨¡å‹é€‰æ‹©

```python
from langchain_openai import ChatOpenAI

# å¤šä¸ªæ¨¡å‹å®ä¾‹
gpt4 = ChatOpenAI(model="gpt-4", temperature=0)           # $0.03/1K (å¼º)
gpt35 = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)  # $0.002/1K (å¿«)
gpt4_mini = ChatOpenAI(model="gpt-4o-mini", temperature=0) # $0.015/1K (å¹³è¡¡)

def select_model(state: MessagesState):
    """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
    last_message = state["messages"][-1].content

    # ç®€å•é—®é¢˜ â†’ GPT-3.5
    if is_simple_query(last_message):
        return gpt35

    # å¤æ‚é—®é¢˜ â†’ GPT-4
    if is_complex_query(last_message):
        return gpt4

    # é»˜è®¤ â†’ GPT-4 Mini
    return gpt4_mini

def is_simple_query(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦æ˜¯ç®€å•é—®é¢˜"""
    simple_patterns = [
        "hello", "hi", "thanks", "bye",
        "what is", "who is", "when is"
    ]
    return any(pattern in text.lower() for pattern in simple_patterns)

def is_complex_query(text: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦æ˜¯å¤æ‚é—®é¢˜"""
    complex_patterns = [
        "analyze", "compare", "explain why",
        "debug", "design", "implement"
    ]
    return any(pattern in text.lower() for pattern in complex_patterns)

def adaptive_chat(state: MessagesState):
    """è‡ªé€‚åº”æ¨¡å‹é€‰æ‹©"""
    model = select_model(state)
    response = model.invoke(state["messages"])

    return {"messages": [response]}

# æˆæœ¬èŠ‚çœ
# å‡è®¾: 30% ç®€å•é—®é¢˜, 20% å¤æ‚é—®é¢˜, 50% ä¸­ç­‰é—®é¢˜
# å¹³å‡æˆæœ¬: 0.3 Ã— $0.002 + 0.5 Ã— $0.015 + 0.2 Ã— $0.03
#         = $0.0006 + $0.0075 + $0.006
#         = $0.0141/1K (æ¯”å…¨ç”¨ GPT-4 èŠ‚çœ 53%)
```

#### ä¼˜åŒ–ç­–ç•¥ 5ï¼šæ‰¹å¤„ç†å’Œå¼‚æ­¥

```python
import asyncio
from typing import List

async def batch_process_conversations(conversations: List[dict]):
    """æ‰¹å¤„ç†å¤šä¸ªå¯¹è¯"""
    # ä½¿ç”¨ asyncio.gather å¹¶è¡Œå¤„ç†
    tasks = [
        process_conversation_async(conv)
        for conv in conversations
    ]

    results = await asyncio.gather(*tasks)
    return results

async def process_conversation_async(conversation: dict):
    """å¼‚æ­¥å¤„ç†å•ä¸ªå¯¹è¯"""
    # ä½¿ç”¨ LangChain çš„å¼‚æ­¥æ¥å£
    response = await model.ainvoke(conversation["messages"])
    return response

# æ‰¹å¤„ç†çš„ä¼˜åŠ¿
# 1. å‡å°‘ç½‘ç»œå¼€é”€
# 2. å……åˆ†åˆ©ç”¨ API é™é€Ÿ
# 3. æé«˜ååé‡
```

#### ä¼˜åŒ–ç­–ç•¥ 6ï¼šToken é¢„ç®—æ§åˆ¶

```python
class BudgetControlledState(MessagesState):
    """å¸¦é¢„ç®—æ§åˆ¶çš„çŠ¶æ€"""
    token_budget: int = 10000      # æ¯ä¸ªä¼šè¯çš„ token é¢„ç®—
    tokens_used: int = 0           # å·²ä½¿ç”¨çš„ tokens
    budget_warning: bool = False   # é¢„ç®—è­¦å‘Š

def budget_aware_chat(state: BudgetControlledState):
    """é¢„ç®—æ„ŸçŸ¥çš„å¯¹è¯"""
    # 1. æ£€æŸ¥é¢„ç®—
    remaining_budget = state["token_budget"] - state["tokens_used"]

    if remaining_budget <= 0:
        return {
            "messages": [AIMessage("Token budget exhausted. Please start a new session.")]
        }

    # 2. æ ¹æ®å‰©ä½™é¢„ç®—è°ƒæ•´ç­–ç•¥
    if remaining_budget < 1000:
        # é¢„ç®—ä¸è¶³ï¼Œä½¿ç”¨æ¿€è¿›è£å‰ª
        max_tokens = 500
        state["budget_warning"] = True
    elif remaining_budget < 3000:
        # é¢„ç®—ç´§å¼ ï¼Œä½¿ç”¨ä¸­ç­‰è£å‰ª
        max_tokens = 1500
    else:
        # é¢„ç®—å……è¶³ï¼Œæ­£å¸¸ä½¿ç”¨
        max_tokens = 3000

    # 3. è£å‰ªæ¶ˆæ¯
    trimmed = trim_messages(
        state["messages"],
        max_tokens=max_tokens,
        token_counter=model
    )

    # 4. è°ƒç”¨ LLM
    response = model.invoke(trimmed)

    # 5. æ›´æ–°ä½¿ç”¨é‡
    input_tokens = sum(model.get_num_tokens(m.content) for m in trimmed)
    output_tokens = model.get_num_tokens(response.content)
    total_tokens = input_tokens + output_tokens

    return {
        "messages": [response],
        "tokens_used": state["tokens_used"] + total_tokens
    }
```

#### æˆæœ¬ç›‘æ§å’Œåˆ†æ

```python
class TokenAnalytics:
    """Token åˆ†æå·¥å…·"""

    def __init__(self):
        self.usage_log = []

    def log_usage(self, thread_id: str, input_tokens: int, output_tokens: int):
        """è®°å½•ä½¿ç”¨"""
        self.usage_log.append({
            "thread_id": thread_id,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "cost": self.calculate_cost(input_tokens, output_tokens),
            "timestamp": datetime.now()
        })

    def calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è®¡ç®—æˆæœ¬"""
        # GPT-4 å®šä»·
        input_cost = input_tokens / 1000 * 0.03
        output_cost = output_tokens / 1000 * 0.06
        return input_cost + output_cost

    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_tokens = sum(log["total_tokens"] for log in self.usage_log)
        total_cost = sum(log["cost"] for log in self.usage_log)
        avg_tokens_per_request = total_tokens / len(self.usage_log) if self.usage_log else 0

        return {
            "total_tokens": total_tokens,
            "total_cost": total_cost,
            "avg_tokens_per_request": avg_tokens_per_request,
            "total_requests": len(self.usage_log)
        }

    def get_top_consumers(self, n=10):
        """è·å– Token æ¶ˆè€—æœ€å¤šçš„ä¼šè¯"""
        from collections import defaultdict

        thread_usage = defaultdict(int)
        for log in self.usage_log:
            thread_usage[log["thread_id"]] += log["total_tokens"]

        return sorted(thread_usage.items(), key=lambda x: x[1], reverse=True)[:n]

# ä½¿ç”¨
analytics = TokenAnalytics()

def monitored_chat(state: MessagesState):
    """å¸¦ç›‘æ§çš„å¯¹è¯"""
    # è®¡ç®—è¾“å…¥ tokens
    input_tokens = sum(model.get_num_tokens(m.content) for m in state["messages"])

    # è°ƒç”¨ LLM
    response = model.invoke(state["messages"])

    # è®¡ç®—è¾“å‡º tokens
    output_tokens = model.get_num_tokens(response.content)

    # è®°å½•åˆ°åˆ†æç³»ç»Ÿ
    thread_id = state.get("thread_id", "unknown")
    analytics.log_usage(thread_id, input_tokens, output_tokens)

    return {"messages": [response]}

# å®šæœŸæŠ¥å‘Š
def generate_cost_report():
    """ç”Ÿæˆæˆæœ¬æŠ¥å‘Š"""
    stats = analytics.get_statistics()
    print(f"""
    Token Usage Report
    ==================
    Total Tokens: {stats['total_tokens']:,}
    Total Cost: ${stats['total_cost']:.2f}
    Avg Tokens/Request: {stats['avg_tokens_per_request']:.0f}
    Total Requests: {stats['total_requests']}

    Top 10 Consumers:
    """)

    for thread_id, tokens in analytics.get_top_consumers(10):
        cost = tokens / 1000 * 0.03
        print(f"  {thread_id}: {tokens:,} tokens (${cost:.2f})")
```

#### ç»¼åˆä¼˜åŒ–æ–¹æ¡ˆ

```python
class OptimizedChatbot:
    """å®Œå…¨ä¼˜åŒ–çš„èŠå¤©æœºå™¨äºº"""

    def __init__(self):
        # æ¨¡å‹
        self.gpt4 = ChatOpenAI(model="gpt-4")
        self.gpt35 = ChatOpenAI(model="gpt-3.5-turbo")

        # ç¼“å­˜
        self.cache = {}

        # åˆ†æ
        self.analytics = TokenAnalytics()

    def chat(self, state: MessagesState, budget: int = 5000):
        """ä¼˜åŒ–çš„å¯¹è¯æ–¹æ³•"""
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(state["messages"][-1])
        if cache_key in self.cache:
            return {"messages": [self.cache[cache_key]]}

        # 2. é€‰æ‹©æ¨¡å‹
        model = self._select_model(state)

        # 3. å‡†å¤‡ä¸Šä¸‹æ–‡
        # - ä½¿ç”¨åˆ†å±‚æ‘˜è¦
        # - Token è£å‰ª
        # - é¢„ç®—æ§åˆ¶
        context = self._prepare_optimized_context(state, budget)

        # 4. è°ƒç”¨ LLM
        response = model.invoke(context)

        # 5. ç¼“å­˜ç»“æœ
        self.cache[cache_key] = response

        # 6. è®°å½•ä½¿ç”¨
        self._log_usage(state, context, response)

        return {"messages": [response]}

    def _prepare_optimized_context(self, state, budget):
        """å‡†å¤‡ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡"""
        # å®ç°æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥...
        pass

# ä¼˜åŒ–æ•ˆæœæ€»ç»“
# åŸºçº¿: 10,000 tokens, $0.30
# ä¼˜åŒ–å: 3,000 tokens, $0.09
# èŠ‚çœ: 70%
```

#### å…³é”®æ´å¯Ÿ

> **Token ä¼˜åŒ–çš„é»„é‡‘æ³•åˆ™ï¼š**
>
> 1. **è£å‰ªä¼˜å…ˆ**ï¼šä½¿ç”¨ trim_messages ç²¾ç¡®æ§åˆ¶
> 2. **åˆ†å±‚æ‘˜è¦**ï¼šmicro/macro/full ä¸‰çº§æ‘˜è¦
> 3. **æ™ºèƒ½ç¼“å­˜**ï¼šç¼“å­˜å¸¸è§é—®é¢˜å’Œç­”æ¡ˆ
> 4. **åŠ¨æ€æ¨¡å‹**ï¼šæ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
> 5. **é¢„ç®—æ§åˆ¶**ï¼šè®¾ç½® token ä¸Šé™
> 6. **æŒç»­ç›‘æ§**ï¼šåˆ†æä½¿ç”¨æ¨¡å¼ï¼ŒæŒç»­ä¼˜åŒ–

</details>

---

### é—®é¢˜ 10ï¼šå¦‚ä½•å¤„ç† LangGraph çŠ¶æ€ç®¡ç†ä¸­çš„å¹¶å‘å’Œç«æ€æ¡ä»¶ï¼Ÿ

<details>
<summary>ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†è§£ç­”</summary>

#### å¹¶å‘é—®é¢˜çš„æœ¬è´¨

åœ¨å¤šç”¨æˆ·æˆ–é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼ŒçŠ¶æ€ç®¡ç†é¢ä¸´çš„æŒ‘æˆ˜ï¼š

```
æ—¶é—´ T1: ç”¨æˆ· A è¯»å–çŠ¶æ€ (count=0)
æ—¶é—´ T2: ç”¨æˆ· B è¯»å–çŠ¶æ€ (count=0)
æ—¶é—´ T3: ç”¨æˆ· A æ›´æ–°çŠ¶æ€ (count=1)
æ—¶é—´ T4: ç”¨æˆ· B æ›´æ–°çŠ¶æ€ (count=1)  â† åº”è¯¥æ˜¯ 2!

ç»“æœ: count=1 (ä¸¢å¤±äº†ä¸€æ¬¡æ›´æ–°)
```

#### è§£å†³æ–¹æ¡ˆ 1ï¼šThread éš”ç¦»

**åŸç†ï¼š** æ¯ä¸ªç”¨æˆ·ä½¿ç”¨ç‹¬ç«‹çš„ thread_idï¼ŒçŠ¶æ€å®Œå…¨éš”ç¦»

```python
from langgraph.checkpoint.postgres import PostgresSaver

# æ¯ä¸ªç”¨æˆ·ç‹¬ç«‹çš„ thread
def get_user_config(user_id: str):
    return {"configurable": {"thread_id": f"user_{user_id}"}}

# ç”¨æˆ· A
config_a = get_user_config("alice")
graph.invoke({"messages": [...]}, config_a)

# ç”¨æˆ· Bï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
config_b = get_user_config("bob")
graph.invoke({"messages": [...]}, config_b)

# ä¸¤ä¸ªç”¨æˆ·çš„çŠ¶æ€äº’ä¸å½±å“
```

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¤šç”¨æˆ·èŠå¤©æœºå™¨äºº
- âœ… ç‹¬ç«‹çš„å¯¹è¯ä¼šè¯
- âŒ éœ€è¦å…±äº«çŠ¶æ€çš„åœºæ™¯

#### è§£å†³æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ Reducer å¤„ç†å¹¶è¡ŒèŠ‚ç‚¹

**åŸç†ï¼š** Reducer å®šä¹‰äº†å¤šä¸ªæ›´æ–°çš„åˆå¹¶ç­–ç•¥

```python
from operator import add
from typing import Annotated

class ConcurrentState(TypedDict):
    # ä½¿ç”¨ Reducer é¿å…å†²çª
    results: Annotated[list, add]
    counter: Annotated[int, lambda x, y: x + y]  # ç´¯åŠ 

def parallel_node_1(state):
    return {"results": ["result_1"], "counter": 1}

def parallel_node_2(state):
    return {"results": ["result_2"], "counter": 1}

# å¹¶è¡Œæ‰§è¡Œ
# results = ["result_1", "result_2"]  âœ… åˆå¹¶
# counter = 2  âœ… ç´¯åŠ 
```

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å›¾å†…éƒ¨çš„å¹¶è¡ŒèŠ‚ç‚¹
- âœ… å¯ä»¥å®šä¹‰åˆå¹¶é€»è¾‘çš„åœºæ™¯
- âŒ è·¨è¿›ç¨‹çš„å¹¶å‘

#### è§£å†³æ–¹æ¡ˆ 3ï¼šæ•°æ®åº“äº‹åŠ¡å’Œé”

**åŸç†ï¼š** ä½¿ç”¨æ•°æ®åº“çš„äº‹åŠ¡æœºåˆ¶ä¿è¯åŸå­æ€§

```python
import asyncpg
from contextlib import asynccontextmanager

class TransactionalCheckpointer:
    """å¸¦äº‹åŠ¡çš„ Checkpointer"""

    def __init__(self, db_pool):
        self.pool = db_pool

    @asynccontextmanager
    async def transaction(self):
        """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        async with self.pool.acquire() as conn:
            async with conn.transaction():
                yield conn

    async def save_state(self, thread_id: str, state: dict):
        """åŸå­æ€§ä¿å­˜çŠ¶æ€"""
        async with self.transaction() as conn:
            # 1. é”å®šè¡Œï¼ˆé˜²æ­¢å¹¶å‘ä¿®æ”¹ï¼‰
            await conn.execute(
                "SELECT * FROM checkpoints WHERE thread_id = $1 FOR UPDATE",
                thread_id
            )

            # 2. æ›´æ–°çŠ¶æ€
            await conn.execute(
                "UPDATE checkpoints SET state = $1 WHERE thread_id = $2",
                state, thread_id
            )

    async def increment_counter(self, thread_id: str):
        """åŸå­æ€§é€’å¢è®¡æ•°å™¨"""
        async with self.transaction() as conn:
            result = await conn.fetchval(
                """
                UPDATE checkpoints
                SET counter = counter + 1
                WHERE thread_id = $1
                RETURNING counter
                """,
                thread_id
            )
            return result

# ä½¿ç”¨
checkpointer = TransactionalCheckpointer(pool)

# å³ä½¿å¹¶å‘è°ƒç”¨ï¼Œè®¡æ•°å™¨ä¹Ÿæ˜¯å‡†ç¡®çš„
await checkpointer.increment_counter("user_123")
await checkpointer.increment_counter("user_123")
# counter = 2 âœ…
```

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… éœ€è¦å¼ºä¸€è‡´æ€§çš„åœºæ™¯
- âœ… è·¨è¿›ç¨‹çš„å¹¶å‘
- âŒ å¯¹æ€§èƒ½è¦æ±‚æé«˜çš„åœºæ™¯

#### è§£å†³æ–¹æ¡ˆ 4ï¼šä¹è§‚é”

**åŸç†ï¼š** ä½¿ç”¨ç‰ˆæœ¬å·æ£€æµ‹å†²çª

```python
class VersionedState(MessagesState):
    """å¸¦ç‰ˆæœ¬å·çš„çŠ¶æ€"""
    version: int = 0

def optimistic_update(state: VersionedState, new_data: dict):
    """ä¹è§‚é”æ›´æ–°"""
    current_version = state["version"]

    # æ›´æ–°æ•°æ®
    updated_state = {**state, **new_data, "version": current_version + 1}

    # å°è¯•ä¿å­˜ï¼ˆæ£€æŸ¥ç‰ˆæœ¬ï¼‰
    try:
        save_with_version_check(updated_state, current_version)
        return updated_state
    except VersionConflictError:
        # ç‰ˆæœ¬å†²çªï¼Œé‡æ–°è¯»å–å¹¶é‡è¯•
        latest_state = reload_state()
        return optimistic_update(latest_state, new_data)

def save_with_version_check(state: dict, expected_version: int):
    """ä¿å­˜æ—¶æ£€æŸ¥ç‰ˆæœ¬"""
    result = conn.execute(
        """
        UPDATE checkpoints
        SET state = $1, version = $2
        WHERE thread_id = $3 AND version = $4
        """,
        state, state["version"], state["thread_id"], expected_version
    )

    if result.rowcount == 0:
        raise VersionConflictError("State was modified by another process")
```

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å†²çªè¾ƒå°‘çš„åœºæ™¯
- âœ… è¯»å¤šå†™å°‘
- âŒ å†²çªé¢‘ç¹çš„åœºæ™¯ï¼ˆä¼šé‡è¯•å¾ˆå¤šæ¬¡ï¼‰

#### è§£å†³æ–¹æ¡ˆ 5ï¼šæ¶ˆæ¯é˜Ÿåˆ—

**åŸç†ï¼š** å°†å¹¶å‘è¯·æ±‚ä¸²è¡ŒåŒ–

```python
import asyncio
from asyncio import Queue

class QueuedChatbot:
    """å¸¦é˜Ÿåˆ—çš„èŠå¤©æœºå™¨äºº"""

    def __init__(self, graph):
        self.graph = graph
        self.queues = {}  # æ¯ä¸ª thread ä¸€ä¸ªé˜Ÿåˆ—
        self.workers = {}  # æ¯ä¸ª thread ä¸€ä¸ª worker

    async def send_message(self, thread_id: str, message: str):
        """å‘é€æ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰"""
        # 1. è·å–æˆ–åˆ›å»ºé˜Ÿåˆ—
        if thread_id not in self.queues:
            self.queues[thread_id] = Queue()
            self.workers[thread_id] = asyncio.create_task(
                self._worker(thread_id)
            )

        # 2. å°†æ¶ˆæ¯æ”¾å…¥é˜Ÿåˆ—
        future = asyncio.Future()
        await self.queues[thread_id].put((message, future))

        # 3. ç­‰å¾…ç»“æœ
        return await future

    async def _worker(self, thread_id: str):
        """Worker åç¨‹ï¼šä¸²è¡Œå¤„ç†æ¶ˆæ¯"""
        queue = self.queues[thread_id]
        config = {"configurable": {"thread_id": thread_id}}

        while True:
            # ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯
            message, future = await queue.get()

            try:
                # ä¸²è¡Œæ‰§è¡Œï¼ˆé¿å…å¹¶å‘ï¼‰
                result = await self.graph.ainvoke(
                    {"messages": [HumanMessage(message)]},
                    config
                )
                future.set_result(result)
            except Exception as e:
                future.set_exception(e)
            finally:
                queue.task_done()

# ä½¿ç”¨
chatbot = QueuedChatbot(graph)

# å¹¶å‘è°ƒç”¨ä¼šè¢«ä¸²è¡ŒåŒ–
results = await asyncio.gather(
    chatbot.send_message("user_123", "Message 1"),
    chatbot.send_message("user_123", "Message 2"),
    chatbot.send_message("user_123", "Message 3")
)
# ä¿è¯æ‰§è¡Œé¡ºåºï¼šMessage 1 â†’ Message 2 â†’ Message 3
```

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… éœ€è¦ä¿è¯é¡ºåºçš„åœºæ™¯
- âœ… é«˜å¹¶å‘ä½†å•ä¸ªç”¨æˆ·è¯·æ±‚ä¸å¤š
- âŒ å¯¹å»¶è¿Ÿæ•æ„Ÿçš„åœºæ™¯

#### è§£å†³æ–¹æ¡ˆ 6ï¼šåˆ†å¸ƒå¼é”

**åŸç†ï¼š** ä½¿ç”¨ Redis ç­‰å®ç°åˆ†å¸ƒå¼é”

```python
import redis
from contextlib import contextmanager
import time
import uuid

class DistributedLock:
    """åˆ†å¸ƒå¼é”"""

    def __init__(self, redis_client, key: str, timeout: int = 10):
        self.redis = redis_client
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.identifier = str(uuid.uuid4())

    @contextmanager
    def acquire(self, retry_times: int = 10, retry_delay: float = 0.1):
        """è·å–é”"""
        acquired = False
        try:
            for _ in range(retry_times):
                # å°è¯•è·å–é”
                if self.redis.set(self.key, self.identifier, nx=True, ex=self.timeout):
                    acquired = True
                    break
                time.sleep(retry_delay)

            if not acquired:
                raise LockAcquisitionError("Failed to acquire lock")

            yield

        finally:
            # é‡Šæ”¾é”ï¼ˆæ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå·±çš„é”ï¼‰
            if acquired:
                lua_script = """
                if redis.call("get", KEYS[1]) == ARGV[1] then
                    return redis.call("del", KEYS[1])
                else
                    return 0
                end
                """
                self.redis.eval(lua_script, 1, self.key, self.identifier)

# ä½¿ç”¨
redis_client = redis.Redis(host='localhost', port=6379)

def concurrent_safe_update(thread_id: str):
    """å¹¶å‘å®‰å…¨çš„æ›´æ–°"""
    lock = DistributedLock(redis_client, thread_id)

    with lock.acquire():
        # ä¸´ç•ŒåŒºï¼šåªæœ‰ä¸€ä¸ªè¿›ç¨‹å¯ä»¥è¿›å…¥
        state = graph.get_state({"configurable": {"thread_id": thread_id}})
        # ... æ›´æ–°çŠ¶æ€ ...
        graph.update_state({"configurable": {"thread_id": thread_id}}, new_state)
```

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¤šæœåŠ¡å™¨éƒ¨ç½²
- âœ… éœ€è¦å¼ºä¸€è‡´æ€§
- âŒ å•æœºéƒ¨ç½²ï¼ˆPython GIL å·²è¶³å¤Ÿï¼‰

#### æœ€ä½³å®è·µ

```python
class ProductionChatbot:
    """ç”Ÿäº§çº§èŠå¤©æœºå™¨äººï¼ˆç»¼åˆæ–¹æ¡ˆï¼‰"""

    def __init__(self, checkpointer):
        self.graph = workflow.compile(checkpointer=checkpointer)
        self.queues = {}  # é˜Ÿåˆ—ï¼ˆä¸²è¡ŒåŒ–ï¼‰
        self.redis = redis.Redis()  # åˆ†å¸ƒå¼é”

    async def handle_message(self, user_id: str, message: str):
        """å¤„ç†æ¶ˆæ¯ï¼ˆç»¼åˆæ–¹æ¡ˆï¼‰"""
        thread_id = f"user_{user_id}"
        config = {"configurable": {"thread_id": thread_id}}

        # ç­–ç•¥ 1ï¼šç”¨æˆ·éš”ç¦»ï¼ˆThreadï¼‰
        # ä¸åŒç”¨æˆ·äº’ä¸å½±å“

        # ç­–ç•¥ 2ï¼šé˜Ÿåˆ—ä¸²è¡ŒåŒ–ï¼ˆåŒä¸€ç”¨æˆ·çš„å¹¶å‘è¯·æ±‚ï¼‰
        if thread_id not in self.queues:
            self.queues[thread_id] = asyncio.Queue()
            asyncio.create_task(self._worker(thread_id))

        future = asyncio.Future()
        await self.queues[thread_id].put((message, future))
        result = await future

        return result

    async def _worker(self, thread_id: str):
        """Workerï¼šä¸²è¡Œå¤„ç†"""
        queue = self.queues[thread_id]
        config = {"configurable": {"thread_id": thread_id}}

        while True:
            message, future = await queue.get()

            # ç­–ç•¥ 3ï¼šåˆ†å¸ƒå¼é”ï¼ˆå¤šæœåŠ¡å™¨éƒ¨ç½²ï¼‰
            lock = DistributedLock(self.redis, thread_id)

            try:
                with lock.acquire():
                    # ç­–ç•¥ 4ï¼šReducerï¼ˆå›¾å†…éƒ¨å¹¶è¡ŒèŠ‚ç‚¹ï¼‰
                    # å·²åœ¨çŠ¶æ€å®šä¹‰ä¸­ä½¿ç”¨ Annotated[list, add]

                    result = await self.graph.ainvoke(
                        {"messages": [HumanMessage(message)]},
                        config
                    )
                    future.set_result(result)
            except Exception as e:
                future.set_exception(e)
            finally:
                queue.task_done()
```

#### å…³é”®æ´å¯Ÿ

> **å¹¶å‘æ§åˆ¶çš„å±‚æ¬¡ï¼š**
>
> 1. **ç”¨æˆ·å±‚**ï¼šThread éš”ç¦»ï¼ˆä¸åŒç”¨æˆ·ï¼‰
> 2. **è¯·æ±‚å±‚**ï¼šé˜Ÿåˆ—ä¸²è¡ŒåŒ–ï¼ˆåŒä¸€ç”¨æˆ·çš„å¹¶å‘è¯·æ±‚ï¼‰
> 3. **å›¾å±‚**ï¼šReducerï¼ˆå›¾å†…éƒ¨å¹¶è¡ŒèŠ‚ç‚¹ï¼‰
> 4. **æ•°æ®å±‚**ï¼šäº‹åŠ¡/é”ï¼ˆæ•°æ®åº“çº§åˆ«ï¼‰
>
> **é€‰æ‹©æŒ‡å—ï¼š**
> - å•æœº + ä½å¹¶å‘ï¼šThread éš”ç¦» + Reducer
> - å•æœº + é«˜å¹¶å‘ï¼š+ é˜Ÿåˆ—ä¸²è¡ŒåŒ–
> - å¤šæœº + é«˜å¹¶å‘ï¼š+ åˆ†å¸ƒå¼é”
> - å¼ºä¸€è‡´æ€§éœ€æ±‚ï¼š+ æ•°æ®åº“äº‹åŠ¡

</details>

---

## ğŸ‰ å¤ä¹ å®Œæˆï¼

### çŸ¥è¯†æŒæ¡åº¦è‡ªæµ‹

å®Œæˆä»¥ä¸Š 10 é“é¢˜åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ç†Ÿç»ƒä½¿ç”¨ä¸‰ç§ State Schema å®šä¹‰æ–¹å¼
- [ ] ç†è§£å¹¶è§£å†³å¹¶è¡ŒèŠ‚ç‚¹çš„çŠ¶æ€å†²çª
- [ ] æŒæ¡ add_messages çš„ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½
- [ ] å®ç°ç§æœ‰çŠ¶æ€å’Œå¤šçŠ¶æ€æ¨¡å¼
- [ ] ä¼˜åŒ–æ¶ˆæ¯ç®¡ç†å’Œ Token ä½¿ç”¨
- [ ] æ„å»ºå¸¦æ‘˜è¦çš„æ™ºèƒ½èŠå¤©æœºå™¨äºº
- [ ] é…ç½®å¤–éƒ¨æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨
- [ ] è®¾è®¡ç”Ÿäº§çº§å®¢æœç³»ç»Ÿ
- [ ] ä¼˜åŒ– Token æˆæœ¬ï¼ˆèŠ‚çœ 50%+ï¼‰
- [ ] å¤„ç†å¹¶å‘å’Œç«æ€æ¡ä»¶

### ä¸‹ä¸€æ­¥å­¦ä¹ è·¯å¾„

å®Œæˆ Module-3 åï¼Œå»ºè®®ä½ ï¼š

1. **å®è·µé¡¹ç›®**
   - æ„å»ºä¸€ä¸ªå®Œæ•´çš„èŠå¤©æœºå™¨äºº
   - å®ç°æ¶ˆæ¯æ‘˜è¦å’ŒæŒä¹…åŒ–
   - ä¼˜åŒ– Token ä½¿ç”¨

2. **è¿›é˜¶å­¦ä¹ **
   - Module-4ï¼šå·¥å…·è°ƒç”¨å’Œå¤–éƒ¨é›†æˆ
   - Module-5ï¼šé«˜çº§å›¾ç»“æ„å’Œå­å›¾
   - Module-6ï¼šéƒ¨ç½²å’Œç”Ÿäº§ä¼˜åŒ–

3. **æ·±å…¥ç ”ç©¶**
   - ç ”ç©¶ LangGraph æºç 
   - è´¡çŒ®å¼€æºé¡¹ç›®
   - åˆ†äº«å­¦ä¹ ç»éªŒ

### å®è·µå»ºè®®

**é¡¹ç›® 1ï¼šæ™ºèƒ½å®¢æœæœºå™¨äºº** (1-2 å‘¨)
- å®ç°å¤šçŠ¶æ€æ¨¡å¼
- é›†æˆæ¶ˆæ¯æ‘˜è¦
- éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

**é¡¹ç›® 2ï¼šé•¿æœŸè®°å¿†åŠ©æ‰‹** (1-2 å‘¨)
- è®¾è®¡åˆ†å±‚æ‘˜è¦ç³»ç»Ÿ
- ä¼˜åŒ– Token æˆæœ¬
- å®ç°è·¨è®¾å¤‡åŒæ­¥

**é¡¹ç›® 3ï¼šä¼ä¸šçº§å¯¹è¯å¹³å°** (2-3 å‘¨)
- å¤šç§Ÿæˆ·æ¶æ„
- é«˜å¹¶å‘å¤„ç†
- ç›‘æ§å’Œåˆ†æç³»ç»Ÿ

---

## ğŸ“– å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [LangGraph State Management](https://langchain-ai.github.io/langgraph/concepts/low_level/#state)
- [LangGraph Reducers](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers)
- [LangGraph Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/)

### ç¤¾åŒºèµ„æº
- [LangChain Discord](https://discord.gg/langchain)
- [GitHub Discussions](https://github.com/langchain-ai/langgraph/discussions)
- [LangChain Blog](https://blog.langchain.dev/)

### å­¦ä¹ è·¯å¾„
- [LangGraph Academy](https://academy.langchain.com/)
- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/)

---

**æ­å–œä½ å®Œæˆ Module-3 çš„å­¦ä¹ ï¼**

ä½ å·²ç»æŒæ¡äº† LangGraph çŠ¶æ€ç®¡ç†çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œè¿™æ˜¯æ„å»ºå¤æ‚ AI ç³»ç»Ÿçš„å…³é”®æŠ€èƒ½ã€‚

ç»§ç»­åŠ æ²¹ï¼ŒæœŸå¾…ä½ åœ¨ Module-4 ä¸­çš„ç²¾å½©è¡¨ç°ï¼ğŸš€
