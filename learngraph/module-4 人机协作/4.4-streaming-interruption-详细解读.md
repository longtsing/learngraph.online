# LangGraph Streaming è¯¦ç»†è§£è¯»

## ğŸ“š æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è§£è¯» LangGraph ä¸­çš„ **Streamingï¼ˆæµå¼è¾“å‡ºï¼‰** æŠ€æœ¯ã€‚Streaming æ˜¯æ„å»ºäº¤äº’å¼ AI åº”ç”¨çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œå®ƒå…è®¸æˆ‘ä»¬åœ¨ Graph æ‰§è¡Œè¿‡ç¨‹ä¸­å®æ—¶è·å–è¾“å‡ºï¼Œè€Œä¸æ˜¯ç­‰å¾…æ•´ä¸ªæµç¨‹å®Œæˆã€‚è¿™å¯¹äºæå‡ç”¨æˆ·ä½“éªŒã€å®ç°äººæœºäº¤äº’è‡³å…³é‡è¦ã€‚

---

## ğŸ“š æœ¯è¯­è¡¨

| æœ¯è¯­åç§° | LangGraph å®šä¹‰å’Œè§£è¯» | Python å®šä¹‰å’Œè¯´æ˜ | é‡è¦ç¨‹åº¦ |
|---------|---------------------|------------------|---------|
| **Streaming** | å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­é€æ­¥å®æ—¶è¾“å‡ºç»“æœçš„æŠ€æœ¯ï¼Œæ”¯æŒèŠ‚ç‚¹çº§ã€æ¶ˆæ¯çº§å’Œ Token çº§æµå¼è¾“å‡º | é€šè¿‡ç”Ÿæˆå™¨(generator)æˆ–å¼‚æ­¥ç”Ÿæˆå™¨å®ç° | â­â­â­â­â­ |
| **stream_mode** | æ§åˆ¶æµå¼è¾“å‡ºç²’åº¦çš„å‚æ•°ï¼ŒåŒ…æ‹¬ updatesã€valuesã€messages ç­‰æ¨¡å¼ | `graph.stream(..., stream_mode="values")` | â­â­â­â­â­ |
| **updates æ¨¡å¼** | åªè¿”å›æ¯ä¸ªèŠ‚ç‚¹å¯¹çŠ¶æ€çš„å¢é‡æ›´æ–°ï¼Œä¸åŒ…å«å®Œæ•´çŠ¶æ€ï¼Œé€‚åˆè¿½è¸ªå˜åŒ– | `stream_mode="updates"` | â­â­â­â­ |
| **values æ¨¡å¼** | è¿”å›æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„å®Œæ•´çŠ¶æ€ï¼Œé€‚åˆè°ƒè¯•å’ŒçŠ¶æ€æ£€æŸ¥ | `stream_mode="values"` | â­â­â­â­ |
| **astream_events** | å¼‚æ­¥æµå¼äº‹ä»¶ APIï¼Œè¾“å‡ºå›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰äº‹ä»¶ï¼Œæ”¯æŒ Token çº§æµå¼è¾“å‡º | `async for event in graph.astream_events(..., version="v2")` | â­â­â­â­â­ |
| **on_chat_model_stream** | astream_events ä¸­çš„ Token æµäº‹ä»¶ç±»å‹ï¼ŒåŒ…å« LLM ç”Ÿæˆçš„æ¯ä¸ª Token | äº‹ä»¶å¯¹è±¡çš„ event å­—æ®µå€¼ | â­â­â­â­â­ |
| **messages æ¨¡å¼** | LangGraph API ç‹¬æœ‰çš„æµå¼æ¨¡å¼ï¼Œä¸“ä¸ºèŠå¤©åº”ç”¨ä¼˜åŒ–ï¼Œè‡ªåŠ¨å¤„ç†æ¶ˆæ¯å¢é‡æ›´æ–° | `client.runs.stream(..., stream_mode="messages")` | â­â­â­â­â­ |
| **async/await** | Python å¼‚æ­¥ç¼–ç¨‹è¯­æ³•ï¼Œç”¨äºå¤„ç† I/O å¯†é›†å‹æ“ä½œ(å¦‚ç½‘ç»œè¯·æ±‚ã€æµå¼è¾“å‡º) | `async def` å®šä¹‰å¼‚æ­¥å‡½æ•°ï¼Œ`await` ç­‰å¾…å¼‚æ­¥æ“ä½œ | â­â­â­â­â­ |
| **async for** | å¼‚æ­¥è¿­ä»£è¯­æ³•ï¼Œç”¨äºéå†å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œå¸¸ç”¨äºæµå¼è¾“å‡ºå¤„ç† | `async for chunk in async_generator()` | â­â­â­â­â­ |
| **Token Streaming** | é€ Token è¾“å‡º LLM ç”Ÿæˆå†…å®¹ï¼Œå®ç°ç±» ChatGPT çš„æ‰“å­—æœºæ•ˆæœ | é€šè¿‡ astream_events è¿‡æ»¤ on_chat_model_stream äº‹ä»¶ | â­â­â­â­â­ |
| **StreamPart** | LangGraph API è¿”å›çš„æµå¼æ•°æ®å—ï¼ŒåŒ…å« event(äº‹ä»¶ç±»å‹)å’Œ data(æ•°æ®)å±æ€§ | API å“åº”å¯¹è±¡ç»“æ„ | â­â­â­â­ |
| **RunnableConfig** | ä¼ é€’ç»™èŠ‚ç‚¹çš„é…ç½®å¯¹è±¡ï¼Œå¯ç”¨ streamingã€callbacks ç­‰è¿è¡Œæ—¶å‚æ•° | å‡½æ•°å‚æ•°ç±»å‹: `def node(state, config: RunnableConfig)` | â­â­â­â­ |

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Streamingï¼Ÿ

Streaming æ˜¯æŒ‡åœ¨ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œé€æ­¥è¾“å‡ºç»“æœçš„æŠ€æœ¯ï¼š

**ä¼ ç»Ÿæ–¹å¼ï¼ˆéæµå¼ï¼‰ï¼š**
```
ç”¨æˆ·æé—® â†’ [ç­‰å¾…...] â†’ å®Œæ•´ç­”æ¡ˆä¸€æ¬¡æ€§è¿”å›
```

**æµå¼æ–¹å¼ï¼š**
```
ç”¨æˆ·æé—® â†’ ç¬¬ä¸€ä¸ªè¯ â†’ ç¬¬äºŒä¸ªè¯ â†’ ... â†’ å®Œæˆ
               â†“         â†“           â†“
            å®æ—¶æ˜¾ç¤º  å®æ—¶æ˜¾ç¤º    å®æ—¶æ˜¾ç¤º
```

### ä¸ºä»€ä¹ˆéœ€è¦ Streamingï¼Ÿ

1. **æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ**
   - ç±»ä¼¼ ChatGPT çš„æ‰“å­—æœºæ•ˆæœ
   - è®©ç”¨æˆ·çŸ¥é“ç³»ç»Ÿæ­£åœ¨å·¥ä½œ
   - å‡å°‘ç­‰å¾…ç„¦è™‘

2. **å®æ—¶ç›‘æ§**
   - è§‚å¯Ÿ Graph çš„æ‰§è¡Œæµç¨‹
   - è°ƒè¯•æ¯ä¸ªèŠ‚ç‚¹çš„è¾“å‡º
   - è·Ÿè¸ªçŠ¶æ€å˜åŒ–

3. **äººæœºäº¤äº’åŸºç¡€**
   - åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­æš‚åœ
   - è®©ç”¨æˆ·ç¡®è®¤æˆ–ä¿®æ”¹
   - å®ç° Human-in-the-loop

---

## ğŸ­ å®æˆ˜æ¡ˆä¾‹ï¼šå¸¦è®°å¿†çš„èŠå¤©æœºå™¨äºº

æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªå®Œæ•´çš„èŠå¤©æœºå™¨äººï¼Œæ¼”ç¤º LangGraph çš„æ‰€æœ‰ Streaming æ¨¡å¼ã€‚

### ç³»ç»Ÿæ¶æ„

```
START
  â†“
[conversation] è°ƒç”¨ LLM
  â†“
åˆ¤æ–­æ¶ˆæ¯æ•°é‡
  â†“
â”œâ”€ â‰¤ 6 æ¡ â†’ END
â””â”€ > 6 æ¡ â†’ [summarize_conversation] â†’ END
              (æ€»ç»“å¹¶åˆ é™¤æ—§æ¶ˆæ¯)
```

**æ ¸å¿ƒåŠŸèƒ½ï¼š**
1. æ”¯æŒå¤šè½®å¯¹è¯
2. è‡ªåŠ¨æ€»ç»“é•¿å¯¹è¯
3. æ”¯æŒå¤šç§æµå¼è¾“å‡ºæ¨¡å¼

---

## ğŸ”§ ä»£ç å®ç°è¯¦è§£

### 1. ç¯å¢ƒé…ç½®

```python
import os, getpass

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")
```

**è¯´æ˜ï¼š** å®‰å…¨åœ°è®¾ç½® API å¯†é’¥ï¼Œé¿å…ç¡¬ç¼–ç åœ¨ä»£ç ä¸­ã€‚

---

### 2. å®šä¹‰çŠ¶æ€

```python
from langgraph.graph import MessagesState

class State(MessagesState):
    summary: str
```

**LangGraph çŸ¥è¯†ç‚¹ï¼šMessagesState**

`MessagesState` æ˜¯ LangGraph å†…ç½®çš„çŠ¶æ€ç±»ï¼Œä¸“ä¸ºèŠå¤©åº”ç”¨è®¾è®¡ï¼š

```python
class MessagesState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
```

**å…³é”®ç‰¹æ€§ï¼š**
- `messages` å­—æ®µè‡ªåŠ¨ä½¿ç”¨ `add_messages` reducer
- æ”¯æŒæ™ºèƒ½æ¶ˆæ¯åˆå¹¶å’Œå»é‡
- å¤„ç† `RemoveMessage` ç­‰ç‰¹æ®Šæ“ä½œ

**æˆ‘ä»¬çš„æ‰©å±•ï¼š**
```python
class State(MessagesState):
    summary: str  # æ·»åŠ å¯¹è¯æ‘˜è¦å­—æ®µ
```

---

### 3. æ ¸å¿ƒèŠ‚ç‚¹ï¼šcall_model

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.runnables import RunnableConfig

model = ChatOpenAI(model="gpt-5-nano", temperature=0)

def call_model(state: State, config: RunnableConfig):
    # è·å–æ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    summary = state.get("summary", "")

    # å¦‚æœæœ‰æ‘˜è¦ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯
    if summary:
        system_message = f"Summary of conversation earlier: {summary}"
        messages = [SystemMessage(content=system_message)] + state["messages"]
    else:
        messages = state["messages"]

    # è°ƒç”¨æ¨¡å‹
    response = model.invoke(messages, config)
    return {"messages": response}
```

**Python çŸ¥è¯†ç‚¹ï¼šRunnableConfig**

`RunnableConfig` æ˜¯ LangChain çš„é…ç½®å¯¹è±¡ï¼Œç”¨äºï¼š
- ä¼ é€’è¿è¡Œæ—¶é…ç½®ï¼ˆå¦‚ callbacksï¼‰
- å¯ç”¨ Token æµå¼è¾“å‡ºï¼ˆPython < 3.11ï¼‰
- ä¼ é€’å…ƒæ•°æ®

```python
# ä¸ºä»€ä¹ˆéœ€è¦ config å‚æ•°ï¼Ÿ
# 1. å¯ç”¨ token-level streaming
# 2. ä¼ é€’å›è°ƒå‡½æ•°
# 3. é…ç½®è¶…æ—¶ã€é‡è¯•ç­‰
```

**æ‘˜è¦æœºåˆ¶ï¼š**
1. å¦‚æœå­˜åœ¨æ‘˜è¦ï¼Œå°†å…¶ä½œä¸ºç³»ç»Ÿæ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯å¼€å¤´
2. è¿™æ · LLM å¯ä»¥äº†è§£ä¹‹å‰çš„å¯¹è¯å†…å®¹
3. ä½†ä¸éœ€è¦å¤„ç†å®Œæ•´çš„å†å²æ¶ˆæ¯ï¼ˆèŠ‚çœ tokenï¼‰

---

### 4. æ‘˜è¦èŠ‚ç‚¹ï¼šsummarize_conversation

```python
from langchain_core.messages import RemoveMessage

def summarize_conversation(state: State):
    # è·å–ç°æœ‰æ‘˜è¦
    summary = state.get("summary", "")

    # åˆ›å»ºæ‘˜è¦æç¤º
    if summary:
        summary_message = (
            f"This is summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )
    else:
        summary_message = "Create a summary of the conversation above:"

    # æ·»åŠ æç¤ºåˆ°æ¶ˆæ¯åˆ—è¡¨
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

    # åˆ é™¤é™¤æœ€å 2 æ¡ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]

    return {"summary": response.content, "messages": delete_messages}
```

**LangGraph çŸ¥è¯†ç‚¹ï¼šRemoveMessage**

`RemoveMessage` æ˜¯ç‰¹æ®Šçš„æ¶ˆæ¯ç±»å‹ï¼Œç”¨äºä»çŠ¶æ€ä¸­åˆ é™¤æ¶ˆæ¯ï¼š

```python
# åˆ›å»ºåˆ é™¤æ¶ˆæ¯
RemoveMessage(id=message_id)

# add_messages reducer ä¼šè‡ªåŠ¨å¤„ç†
# å½“é‡åˆ° RemoveMessage æ—¶ï¼Œä¼šä»çŠ¶æ€ä¸­åˆ é™¤å¯¹åº”çš„æ¶ˆæ¯
```

**ä¸ºä»€ä¹ˆéœ€è¦åˆ é™¤æ¶ˆæ¯ï¼Ÿ**
- é¿å…æ¶ˆæ¯åˆ—è¡¨æ— é™å¢é•¿
- èŠ‚çœå†…å­˜å’Œ token æˆæœ¬
- ä¿æŒå¯¹è¯çª—å£å¯æ§

**æ‘˜è¦ç­–ç•¥ï¼š**
1. ä¿ç•™æœ€å 2 æ¡æ¶ˆæ¯ï¼ˆæœ€æ–°çš„äº¤äº’ï¼‰
2. åˆ é™¤å…¶ä»–æ—§æ¶ˆæ¯
3. ç”¨æ‘˜è¦ä»£æ›¿åˆ é™¤çš„å†…å®¹

---

### 5. æ¡ä»¶åˆ¤æ–­ï¼šshould_continue

```python
from langgraph.graph import END

def should_continue(state: State):
    """å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
    messages = state["messages"]

    # å¦‚æœæ¶ˆæ¯è¶…è¿‡ 6 æ¡ï¼Œè¿›è¡Œæ€»ç»“
    if len(messages) > 6:
        return "summarize_conversation"

    # å¦åˆ™ç»“æŸ
    return END
```

**LangGraph çŸ¥è¯†ç‚¹ï¼šæ¡ä»¶è¾¹è¿”å›å€¼**

æ¡ä»¶å‡½æ•°å¯ä»¥è¿”å›ï¼š
1. **èŠ‚ç‚¹åç§°**ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼š`"summarize_conversation"`
2. **ç‰¹æ®Šå¸¸é‡**ï¼š`END`ã€`START`
3. **Send å¯¹è±¡**ï¼šç”¨äºåŠ¨æ€å¹¶è¡Œï¼ˆä¸‹ä¸€èŠ‚è¯¾å­¦ä¹ ï¼‰

```python
# ç¤ºä¾‹
graph.add_conditional_edges(
    "conversation",      # æºèŠ‚ç‚¹
    should_continue,     # æ¡ä»¶å‡½æ•°
    {                    # è·¯ç”±æ˜ å°„
        "summarize_conversation": "summarize_conversation",
        END: END
    }
)

# ç®€åŒ–å†™æ³•ï¼ˆå¦‚æœè¿”å›å€¼å°±æ˜¯èŠ‚ç‚¹åï¼‰
graph.add_conditional_edges("conversation", should_continue)
```

---

### 6. æ„å»º Graph

```python
from langgraph.graph import StateGraph, START
from langgraph.checkpoint.memory import MemorySaver

# åˆ›å»ºå›¾
workflow = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("conversation", call_model)
workflow.add_node(summarize_conversation)

# æ·»åŠ è¾¹
workflow.add_edge(START, "conversation")
workflow.add_conditional_edges("conversation", should_continue)
workflow.add_edge("summarize_conversation", END)

# ç¼–è¯‘ï¼ˆå¸¦ checkpointerï¼‰
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)
```

**LangGraph çŸ¥è¯†ç‚¹ï¼šCheckpointer**

`MemorySaver` æ˜¯ä¸€ä¸ªå†…å­˜ä¸­çš„ checkpointerï¼Œç”¨äºï¼š
- ä¿å­˜æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€
- æ”¯æŒå¤šè½®å¯¹è¯ï¼ˆé€šè¿‡ thread_idï¼‰
- å¯ç”¨ Streaming å’Œ Interruption åŠŸèƒ½

```python
# Checkpointer çš„ä½œç”¨
graph = workflow.compile(checkpointer=memory)
#                        ^^^^^^^^^^^^^^^^^^^^
#                        æ²¡æœ‰è¿™ä¸ªï¼Œæ— æ³•ï¼š
#                        1. ä¿æŒå¯¹è¯çŠ¶æ€
#                        2. ä½¿ç”¨ thread_id
#                        3. å®ç° human-in-the-loop
```

---

## ğŸŒŠ Streaming æ¨¡å¼è¯¦è§£

![Streaming Modes: Values vs Updates](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)

LangGraph æ”¯æŒå¤šç§ streaming æ¨¡å¼ï¼Œæ¯ç§æ¨¡å¼æä¾›ä¸åŒç²’åº¦çš„è¾“å‡ºã€‚

### æ¨¡å¼ 1ï¼šstream_mode="updates"

**ç‰¹ç‚¹ï¼š** åªè¿”å›èŠ‚ç‚¹æ‰§è¡Œåçš„**çŠ¶æ€æ›´æ–°**

```python
config = {"configurable": {"thread_id": "1"}}

for chunk in graph.stream(
    {"messages": [HumanMessage(content="hi! I'm Lance")]},
    config,
    stream_mode="updates"
):
    print(chunk)
```

**è¾“å‡ºï¼š**
```python
{
    'conversation': {
        'messages': AIMessage(
            content='Hi Lance! How can I assist you today?',
            id='run-6d58e31e-...'
        )
    }
}
```

**è¾“å‡ºç»“æ„ï¼š**
```python
{
    'èŠ‚ç‚¹åç§°': {
        'æ›´æ–°çš„çŠ¶æ€å­—æ®µ': æ–°å€¼
    }
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- åªå…³å¿ƒçŠ¶æ€å˜åŒ–
- ä¸éœ€è¦å®Œæ•´çŠ¶æ€
- å‡å°‘è¾“å‡ºæ•°æ®é‡

**æ›´ä¼˜é›…çš„ä½¿ç”¨æ–¹å¼ï¼š**
```python
for chunk in graph.stream(..., stream_mode="updates"):
    chunk['conversation']["messages"].pretty_print()
```

è¾“å‡ºï¼š
```
================================== Ai Message ==================================
Hi Lance! How are you doing today?
```

---

### æ¨¡å¼ 2ï¼šstream_mode="values"

**ç‰¹ç‚¹ï¼š** è¿”å›èŠ‚ç‚¹æ‰§è¡Œåçš„**å®Œæ•´çŠ¶æ€**

```python
config = {"configurable": {"thread_id": "2"}}
input_message = HumanMessage(content="hi! I'm Lance")

for event in graph.stream(
    {"messages": [input_message]},
    config,
    stream_mode="values"
):
    for m in event['messages']:
        m.pretty_print()
    print("---" * 25)
```

**è¾“å‡ºï¼š**
```
================================ Human Message =================================
hi! I'm Lance
---------------------------------------------------------------------------
================================ Human Message =================================
hi! I'm Lance
================================== Ai Message ==================================
Hi Lance! How can I assist you today?
---------------------------------------------------------------------------
```

**å…³é”®åŒºåˆ«ï¼š**

| ç‰¹æ€§ | updates | values |
|------|---------|--------|
| è¾“å‡ºå†…å®¹ | åªæœ‰æ›´æ–°çš„éƒ¨åˆ† | å®Œæ•´çŠ¶æ€ |
| è¾“å‡ºæ¬¡æ•° | æ¯ä¸ªèŠ‚ç‚¹ 1 æ¬¡ | æ¯ä¸ªèŠ‚ç‚¹ 1 æ¬¡ï¼ˆåŒ…å«ç´¯ç§¯çŠ¶æ€ï¼‰ |
| æ•°æ®é‡ | å° | å¤§ |
| ç”¨é€” | å¢é‡æ›´æ–° | çŠ¶æ€æ£€æŸ¥ã€è°ƒè¯• |

**ä¸ºä»€ä¹ˆ values è¾“å‡ºä¸¤æ¬¡ï¼Ÿ**

1. **ç¬¬ä¸€æ¬¡**ï¼šåˆå§‹çŠ¶æ€ï¼ˆåªæœ‰ç”¨æˆ·æ¶ˆæ¯ï¼‰
2. **ç¬¬äºŒæ¬¡**ï¼šconversation èŠ‚ç‚¹æ‰§è¡Œåï¼ˆç”¨æˆ·æ¶ˆæ¯ + AI å›å¤ï¼‰

è¿™æ˜¯ `values` æ¨¡å¼çš„ç‰¹ç‚¹ï¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹å‰åéƒ½è¾“å‡ºå®Œæ•´çŠ¶æ€ã€‚

---

### æ¨¡å¼ 3ï¼šastream_eventsï¼ˆToken Streamingï¼‰â­

**ç‰¹ç‚¹ï¼š** æµå¼è¾“å‡º**èŠå¤©æ¨¡å‹çš„ token**ï¼ˆæœ€æ¥è¿‘ ChatGPT çš„æ•ˆæœï¼‰

```python
config = {"configurable": {"thread_id": "3"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")

async for event in graph.astream_events(
    {"messages": [input_message]},
    config,
    version="v2"
):
    print(f"Node: {event['metadata'].get('langgraph_node', '')}")
    print(f"Type: {event['event']}")
    print(f"Name: {event['name']}")
```

**Python çŸ¥è¯†ç‚¹ï¼šasync/await**

```python
async for event in graph.astream_events(...):
    #^^^^                  ^^^^^^^^^^^^
    # å¼‚æ­¥å¾ªç¯              å¼‚æ­¥æ–¹æ³•
```

**ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥ï¼Ÿ**
- Streaming æ˜¯ I/O å¯†é›†å‹æ“ä½œ
- å¼‚æ­¥å¯ä»¥åœ¨ç­‰å¾…æ—¶å¤„ç†å…¶ä»–ä»»åŠ¡
- LangChain/LangGraph çš„ streaming åŸºäºå¼‚æ­¥

**åŸºæœ¬æ¦‚å¿µï¼š**
```python
# åŒæ­¥ï¼ˆé˜»å¡ï¼‰
for item in sync_generator():
    process(item)  # ç­‰å¾…æ¯ä¸€é¡¹

# å¼‚æ­¥ï¼ˆéé˜»å¡ï¼‰
async for item in async_generator():
    await process(item)  # ç­‰å¾…æ—¶å¯ä»¥åˆ‡æ¢ä»»åŠ¡
```

**äº‹ä»¶ç±»å‹ï¼š**

astream_events ä¼šè¾“å‡ºå›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„**æ‰€æœ‰**äº‹ä»¶ï¼š

```
Node: . Type: on_chain_start. Name: LangGraph
Node: conversation. Type: on_chain_start. Name: RunnableSequence
Node: conversation. Type: on_prompt_start. Name: ChatPromptTemplate
Node: conversation. Type: on_prompt_end. Name: ChatPromptTemplate
Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI  â† è¿™ä¸ªï¼
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI  â† è¿™ä¸ªï¼
Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI  â† è¿™ä¸ªï¼
...
```

**å…³é”®äº‹ä»¶ï¼š`on_chat_model_stream`**

è¿™æ˜¯æˆ‘ä»¬è¦æ‰¾çš„ï¼å®ƒåŒ…å« LLM ç”Ÿæˆçš„æ¯ä¸ª tokenã€‚

---

### è¿‡æ»¤ Token äº‹ä»¶

```python
node_to_stream = 'conversation'
config = {"configurable": {"thread_id": "4"}}
input_message = HumanMessage(content="Tell me about the 49ers NFL team")

async for event in graph.astream_events(
    {"messages": [input_message]},
    config,
    version="v2"
):
    # è¿‡æ»¤æ¡ä»¶ï¼š
    # 1. äº‹ä»¶ç±»å‹æ˜¯ on_chat_model_stream
    # 2. æ¥è‡ªæˆ‘ä»¬å…³å¿ƒçš„èŠ‚ç‚¹
    if (event["event"] == "on_chat_model_stream" and
        event['metadata'].get('langgraph_node', '') == node_to_stream):
        print(event["data"])
```

**è¾“å‡ºç»“æ„ï¼š**
```python
{
    'chunk': AIMessageChunk(content='The', id='run-...')
}
```

**æå– Tokenï¼š**
```python
async for event in graph.astream_events(...):
    if event["event"] == "on_chat_model_stream" and ....:
        data = event["data"]
        print(data["chunk"].content, end="|")
```

**è¾“å‡ºæ•ˆæœï¼š**
```
The| San| Francisco| |49|ers| are| a| professional| American| football| team|...
```

**å®ç°æ‰“å­—æœºæ•ˆæœï¼š**
```python
import sys

async for event in graph.astream_events(...):
    if event["event"] == "on_chat_model_stream":
        token = event["data"]["chunk"].content
        print(token, end="", flush=True)
        #                      ^^^^^^^^^^
        #                      ç«‹å³è¾“å‡ºï¼Œä¸ç¼“å†²
```

---

## ğŸŒ ä½¿ç”¨ LangGraph API çš„ Streaming

LangGraph æä¾›äº†éƒ¨ç½²å’Œ API æœåŠ¡èƒ½åŠ›ï¼Œæ”¯æŒé€šè¿‡ HTTP è¿›è¡Œ streamingã€‚

### å¯åŠ¨æœ¬åœ°å¼€å‘æœåŠ¡å™¨

```bash
cd /path/to/studio
langgraph dev
```

**è¾“å‡ºï¼š**
```
- ğŸš€ API: http://127.0.0.1:2024
- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- ğŸ“š API Docs: http://127.0.0.1:2024/docs
```

---

### è¿æ¥åˆ° API

```python
from langgraph_sdk import get_client

URL = "http://127.0.0.1:2024"
client = get_client(url=URL)

# æŸ¥çœ‹æ‰€æœ‰ assistantsï¼ˆå›¾ï¼‰
assistants = await client.assistants.search()
```

**LangGraph çŸ¥è¯†ç‚¹ï¼šAssistants**

åœ¨ LangGraph API ä¸­ï¼š
- **Assistant** = ä¸€ä¸ªç¼–è¯‘åçš„ Graph
- å¯ä»¥éƒ¨ç½²å¤šä¸ª assistants
- æ¯ä¸ª assistant æœ‰ç‹¬ç«‹çš„é…ç½®

---

### API Streamingï¼šValues æ¨¡å¼

```python
# åˆ›å»ºçº¿ç¨‹
thread = await client.threads.create()

# æµå¼æ‰§è¡Œ
input_message = HumanMessage(content="Multiply 2 and 3")
async for event in client.runs.stream(
    thread["thread_id"],
    assistant_id="agent",
    input={"messages": [input_message]},
    stream_mode="values"
):
    print(event)
```

**è¾“å‡ºï¼š**
```python
StreamPart(event='metadata', data={'run_id': '1ef6a3d0-...'})
StreamPart(event='values', data={'messages': [...]})
StreamPart(event='values', data={'messages': [..., tool_call_message]})
StreamPart(event='values', data={'messages': [..., tool_result]})
StreamPart(event='values', data={'messages': [..., final_answer]})
```

**StreamPart ç»“æ„ï¼š**
```python
StreamPart(
    event='...',  # äº‹ä»¶ç±»å‹
    data={...}    # çŠ¶æ€æ•°æ®
)
```

**æå–æ¶ˆæ¯ï¼š**
```python
from langchain_core.messages import convert_to_messages

async for event in client.runs.stream(...):
    messages = event.data.get('messages', None)
    if messages:
        # è½¬æ¢ä¸º LangChain æ¶ˆæ¯å¯¹è±¡
        converted = convert_to_messages(messages)
        print(converted[-1])  # æœ€æ–°æ¶ˆæ¯
    print('=' * 25)
```

---

### API Streamingï¼šMessages æ¨¡å¼ â­

**è¿™æ˜¯ API ç‹¬æœ‰çš„æ¨¡å¼ï¼** æœ¬åœ° graph.stream() ä¸æ”¯æŒã€‚

**ç‰¹ç‚¹ï¼š**
- ä¸“ä¸ºèŠå¤©åº”ç”¨ä¼˜åŒ–
- è‡ªåŠ¨å¤„ç†æ¶ˆæ¯å¢é‡æ›´æ–°
- æ›´ç»†ç²’åº¦çš„ token streaming

```python
thread = await client.threads.create()
input_message = HumanMessage(content="Multiply 2 and 3")

async for event in client.runs.stream(
    thread["thread_id"],
    assistant_id="agent",
    input={"messages": [input_message]},
    stream_mode="messages"
):
    print(event.event)
```

**è¾“å‡ºï¼š**
```
metadata
messages/complete
messages/metadata
messages/partial  â† Token æµ
messages/partial
messages/partial
...
messages/complete
```

**äº‹ä»¶ç±»å‹ï¼š**

| äº‹ä»¶ | å«ä¹‰ | æ•°æ® |
|------|------|------|
| `metadata` | è¿è¡Œå…ƒæ•°æ® | run_id ç­‰ |
| `messages/complete` | å®Œæ•´æ¶ˆæ¯ | å®Œæ•´çš„æ¶ˆæ¯å¯¹è±¡ |
| `messages/partial` | éƒ¨åˆ†æ¶ˆæ¯ | Token æˆ–éƒ¨åˆ†å†…å®¹ |
| `messages/metadata` | æ¶ˆæ¯å…ƒæ•°æ® | finish_reason ç­‰ |

---

### å¤„ç† Messages äº‹ä»¶

```python
def format_tool_calls(tool_calls):
    """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨"""
    if tool_calls:
        formatted_calls = []
        for call in tool_calls:
            formatted_calls.append(
                f"Tool Call ID: {call['id']}, "
                f"Function: {call['name']}, "
                f"Arguments: {call['args']}"
            )
        return "\n".join(formatted_calls)
    return "No tool calls"

async for event in client.runs.stream(..., stream_mode="messages"):
    # å¤„ç†å…ƒæ•°æ®
    if event.event == "metadata":
        print(f"Metadata: Run ID - {event.data['run_id']}")
        print("-" * 50)

    # å¤„ç†éƒ¨åˆ†æ¶ˆæ¯ï¼ˆToken æµï¼‰
    elif event.event == "messages/partial":
        for data_item in event.data:
            # ç”¨æˆ·æ¶ˆæ¯
            if "role" in data_item and data_item["role"] == "user":
                print(f"Human: {data_item['content']}")

            # AI å“åº”
            else:
                tool_calls = data_item.get("tool_calls", [])
                content = data_item.get("content", "")
                response_metadata = data_item.get("response_metadata", {})

                if content:
                    print(f"AI: {content}")

                if tool_calls:
                    print("Tool Calls:")
                    print(format_tool_calls(tool_calls))

                if response_metadata:
                    finish_reason = response_metadata.get("finish_reason", "N/A")
                    print(f"Response Metadata: Finish Reason - {finish_reason}")

        print("-" * 50)
```

**è¾“å‡ºæ•ˆæœï¼š**
```
Metadata: Run ID - 1ef6a3da-687f-6253-915a-701de5327165
--------------------------------------------------
Tool Calls:
Tool Call ID: call_IL4M..., Function: multiply, Arguments: {}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_IL4M..., Function: multiply, Arguments: {'a': 2}
--------------------------------------------------
Tool Calls:
Tool Call ID: call_IL4M..., Function: multiply, Arguments: {'a': 2, 'b': 3}
Response Metadata: Finish Reason - tool_calls
--------------------------------------------------
AI: The
--------------------------------------------------
AI: The result
--------------------------------------------------
AI: The result of
--------------------------------------------------
AI: The result of multiplying
--------------------------------------------------
...
AI: The result of multiplying 2 and 3 is 6.
Response Metadata: Finish Reason - stop
--------------------------------------------------
```

**è§‚å¯Ÿè¦ç‚¹ï¼š**
1. å·¥å…·è°ƒç”¨çš„å‚æ•°æ˜¯é€æ­¥ç”Ÿæˆçš„ï¼š`{}` â†’ `{'a': 2}` â†’ `{'a': 2, 'b': 3}`
2. AI å›å¤æ˜¯é€ token ç”Ÿæˆçš„ï¼š`The` â†’ `The result` â†’ ...
3. å¯ä»¥è·å– finish_reason ç­‰å…ƒæ•°æ®

---

## ğŸ“ æ ¸å¿ƒçŸ¥è¯†ç‚¹æ€»ç»“

### LangGraph ç‰¹æœ‰æ¦‚å¿µ

#### 1. Streaming æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | è¾“å‡ºå†…å®¹ | ç²’åº¦ | ç”¨é€” |
|------|---------|------|------|
| `updates` | çŠ¶æ€æ›´æ–° | èŠ‚ç‚¹çº§ | è¿½è¸ªçŠ¶æ€å˜åŒ– |
| `values` | å®Œæ•´çŠ¶æ€ | èŠ‚ç‚¹çº§ | è°ƒè¯•ã€çŠ¶æ€æ£€æŸ¥ |
| `astream_events` | æ‰€æœ‰äº‹ä»¶ | äº‹ä»¶çº§ | Token streaming |
| `messages` (API) | æ¶ˆæ¯æµ | Token çº§ | èŠå¤©åº”ç”¨ |

#### 2. æœ¬åœ° vs API Streaming

| ç‰¹æ€§ | æœ¬åœ° (graph.stream) | API (client.runs.stream) |
|------|-------------------|------------------------|
| `updates` | âœ… | âœ… |
| `values` | âœ… | âœ… |
| `astream_events` | âœ… | âŒ |
| `messages` | âŒ | âœ… (æ¨è) |
| éƒ¨ç½²éœ€æ±‚ | æ—  | éœ€è¦ LangGraph Server |

#### 3. MessagesState

```python
class MessagesState(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
```

**ç‰¹æ€§ï¼š**
- è‡ªåŠ¨æ¶ˆæ¯åˆå¹¶
- æ”¯æŒ RemoveMessage
- å»é‡å’Œæ’åº

#### 4. RemoveMessage

```python
# åˆ›å»ºåˆ é™¤æŒ‡ä»¤
delete_messages = [RemoveMessage(id=m.id) for m in old_messages]

# è¿”å›æ›´æ–°
return {"messages": delete_messages}

# add_messages reducer ä¼šè‡ªåŠ¨å¤„ç†åˆ é™¤
```

#### 5. Checkpointer

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)

# ä½¿ç”¨ thread_id éš”ç¦»å¯¹è¯
config = {"configurable": {"thread_id": "user123"}}
```

**ä½œç”¨ï¼š**
- ä¿å­˜çŠ¶æ€å¿«ç…§
- æ”¯æŒå¤šè½®å¯¹è¯
- å¯ç”¨ interruptionï¼ˆä¸‹ä¸€èŠ‚è¯¾ï¼‰

---

### Python ç‰¹æœ‰çŸ¥è¯†ç‚¹

#### 1. async/await åŸºç¡€

```python
# å®šä¹‰å¼‚æ­¥å‡½æ•°
async def fetch_data():
    return await some_async_operation()

# è°ƒç”¨å¼‚æ­¥å‡½æ•°
result = await fetch_data()

# å¼‚æ­¥å¾ªç¯
async for item in async_generator():
    process(item)
```

**å…³é”®æ¦‚å¿µï¼š**
- `async def`ï¼šå®šä¹‰å¼‚æ­¥å‡½æ•°
- `await`ï¼šç­‰å¾…å¼‚æ­¥æ“ä½œå®Œæˆ
- `async for`ï¼šå¼‚æ­¥è¿­ä»£

**ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥ï¼Ÿ**
```python
# åŒæ­¥ï¼ˆé˜»å¡ï¼‰- æ€»æ—¶é—´ï¼š3ç§’
result1 = fetch_url1()  # 1ç§’
result2 = fetch_url2()  # 1ç§’
result3 = fetch_url3()  # 1ç§’

# å¼‚æ­¥ï¼ˆå¹¶å‘ï¼‰- æ€»æ—¶é—´ï¼š1ç§’
results = await asyncio.gather(
    fetch_url1(),
    fetch_url2(),
    fetch_url3()
)
```

#### 2. RunnableConfig

```python
from langchain_core.runnables import RunnableConfig

def my_node(state: State, config: RunnableConfig):
    # è®¿é—®é…ç½®
    thread_id = config["configurable"]["thread_id"]

    # ä¼ é€’ç»™ LLMï¼ˆå¯ç”¨ streamingï¼‰
    response = model.invoke(messages, config)

    return {"messages": response}
```

**ç”¨é€”ï¼š**
- ä¼ é€’ callbacks
- é…ç½® streaming
- è¶…æ—¶å’Œé‡è¯•è®¾ç½®

#### 3. dict.get() é»˜è®¤å€¼

```python
summary = state.get("summary", "")
#                              ^^^
#                              é»˜è®¤å€¼ï¼ˆå¦‚æœ key ä¸å­˜åœ¨ï¼‰

# ç­‰ä»·äºï¼š
if "summary" in state:
    summary = state["summary"]
else:
    summary = ""
```

#### 4. åˆ—è¡¨æ¨å¯¼å¼

```python
# åˆ›å»ºåˆ é™¤æ¶ˆæ¯åˆ—è¡¨
delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
#                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
#                  å¯¹æ¯ä¸ªæ¶ˆæ¯åˆ›å»º RemoveMessage å¯¹è±¡

# ç­‰ä»·äºï¼š
delete_messages = []
for m in state["messages"][:-2]:  # é™¤æœ€å2æ¡å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    delete_messages.append(RemoveMessage(id=m.id))
```

#### 5. åˆ‡ç‰‡æ“ä½œ

```python
messages = [m1, m2, m3, m4, m5]

messages[-2:]   # æœ€å2ä¸ªï¼š[m4, m5]
messages[:-2]   # é™¤æœ€å2ä¸ªï¼š[m1, m2, m3]
messages[:3]    # å‰3ä¸ªï¼š[m1, m2, m3]
messages[1:4]   # ç´¢å¼•1-3ï¼š[m2, m3, m4]
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„ Streaming æ¨¡å¼

**åœºæ™¯ 1ï¼šèŠå¤©åº”ç”¨**
```python
# âœ… æ¨èï¼šmessages æ¨¡å¼ï¼ˆå¦‚æœä½¿ç”¨ APIï¼‰
async for event in client.runs.stream(..., stream_mode="messages")

# âœ… æˆ–ï¼šastream_eventsï¼ˆæœ¬åœ°ï¼‰
async for event in graph.astream_events(..., version="v2"):
    if event["event"] == "on_chat_model_stream":
        print(event["data"]["chunk"].content, end="")
```

**åœºæ™¯ 2ï¼šè°ƒè¯•å’Œå¼€å‘**
```python
# âœ… æ¨èï¼švalues æ¨¡å¼
for event in graph.stream(..., stream_mode="values"):
    print(event)  # æŸ¥çœ‹å®Œæ•´çŠ¶æ€
```

**åœºæ™¯ 3ï¼šçŠ¶æ€ç›‘æ§**
```python
# âœ… æ¨èï¼šupdates æ¨¡å¼
for chunk in graph.stream(..., stream_mode="updates"):
    for node, update in chunk.items():
        print(f"{node} updated: {update}")
```

---

### 2. Token Streaming æœ€ä½³å®è·µ

```python
async def stream_response(graph, input_message, node_name="conversation"):
    """ä¼˜é›…çš„ token streaming å°è£…"""

    config = {"configurable": {"thread_id": generate_thread_id()}}

    async for event in graph.astream_events(
        {"messages": [input_message]},
        config,
        version="v2"
    ):
        # è¿‡æ»¤ç›®æ ‡èŠ‚ç‚¹çš„ token äº‹ä»¶
        if (event["event"] == "on_chat_model_stream" and
            event["metadata"].get("langgraph_node") == node_name):

            token = event["data"]["chunk"].content
            if token:  # é¿å…ç©º token
                yield token

# ä½¿ç”¨
async for token in stream_response(graph, user_message):
    print(token, end="", flush=True)
```

---

### 3. é”™è¯¯å¤„ç†

```python
async def safe_stream(graph, input_data, config):
    """å¸¦é”™è¯¯å¤„ç†çš„ streaming"""

    try:
        async for event in graph.astream_events(input_data, config, version="v2"):
            if event["event"] == "on_chat_model_stream":
                yield event["data"]["chunk"].content

    except Exception as e:
        print(f"\n[é”™è¯¯] Streaming ä¸­æ–­: {e}")
        # å¯ä»¥é€‰æ‹©è¿”å›éƒ¨åˆ†ç»“æœæˆ–é‡è¯•
        raise

# ä½¿ç”¨
try:
    async for token in safe_stream(graph, input_data, config):
        print(token, end="")
except Exception:
    print("\n[ç³»ç»Ÿ] è¯·ç¨åé‡è¯•")
```

---

### 4. å¯¹è¯å†å²ç®¡ç†

```python
class ConversationManager:
    """ç®¡ç†å¯¹è¯å†å²å’Œæ‘˜è¦"""

    def __init__(self, max_messages=6):
        self.max_messages = max_messages

    def should_summarize(self, state: State) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ€»ç»“"""
        return len(state["messages"]) > self.max_messages

    def get_messages_to_keep(self, messages, keep_last=2):
        """è·å–è¦ä¿ç•™çš„æ¶ˆæ¯"""
        return messages[-keep_last:]

    def get_messages_to_delete(self, messages, keep_last=2):
        """è·å–è¦åˆ é™¤çš„æ¶ˆæ¯"""
        return [RemoveMessage(id=m.id) for m in messages[:-keep_last]]

# ä½¿ç”¨
manager = ConversationManager(max_messages=10)

def should_continue(state: State):
    if manager.should_summarize(state):
        return "summarize_conversation"
    return END
```

---

### 5. çº¿ç¨‹ç®¡ç†

```python
import uuid

def create_thread_config(user_id: str, conversation_id: str = None):
    """åˆ›å»ºçº¿ç¨‹é…ç½®"""

    # ä½¿ç”¨ç”¨æˆ· ID + å¯¹è¯ ID ä½œä¸º thread_id
    if conversation_id is None:
        conversation_id = str(uuid.uuid4())

    thread_id = f"{user_id}:{conversation_id}"

    return {
        "configurable": {
            "thread_id": thread_id
        }
    }

# ä½¿ç”¨
config = create_thread_config(user_id="alice", conversation_id="chat-001")
for event in graph.stream(input_data, config, stream_mode="updates"):
    ...
```

---

## ğŸš€ è¿›é˜¶æŠ€å·§

### 1. å¤šèŠ‚ç‚¹ Token Streaming

å¦‚æœ Graph ä¸­æœ‰å¤šä¸ªèŠå¤©æ¨¡å‹èŠ‚ç‚¹ï¼Œå¯ä»¥åˆ†åˆ« streamï¼š

```python
async def stream_all_nodes(graph, input_data, config):
    """Stream æ‰€æœ‰èŠ‚ç‚¹çš„ tokens"""

    async for event in graph.astream_events(input_data, config, version="v2"):
        if event["event"] == "on_chat_model_stream":
            node_name = event["metadata"].get("langgraph_node", "unknown")
            token = event["data"]["chunk"].content

            # æ ¹æ®èŠ‚ç‚¹åŒºåˆ†è¾“å‡º
            print(f"[{node_name}] {token}", end="", flush=True)
```

---

### 2. å®æ—¶è¿›åº¦æ˜¾ç¤º

```python
import sys

async def stream_with_progress(graph, input_data, config):
    """å¸¦è¿›åº¦æç¤ºçš„ streaming"""

    current_node = None

    async for event in graph.astream_events(input_data, config, version="v2"):
        # èŠ‚ç‚¹å¼€å§‹
        if event["event"] == "on_chain_start":
            new_node = event["metadata"].get("langgraph_node")
            if new_node and new_node != current_node:
                current_node = new_node
                print(f"\n\n[{current_node}] ", end="", flush=True)

        # Token è¾“å‡º
        elif event["event"] == "on_chat_model_stream":
            token = event["data"]["chunk"].content
            print(token, end="", flush=True)
```

---

### 3. æ¡ä»¶ Streaming

```python
async def conditional_stream(graph, input_data, config, stream_tokens=True):
    """æ ¹æ®æ¡ä»¶å†³å®šæ˜¯å¦ stream tokens"""

    if stream_tokens:
        # Token-level streaming
        async for event in graph.astream_events(input_data, config, version="v2"):
            if event["event"] == "on_chat_model_stream":
                yield event["data"]["chunk"].content
    else:
        # Node-level streaming
        async for chunk in graph.astream(input_data, config):
            yield chunk
```

---

### 4. ç¼“å­˜å’Œé‡æ”¾

```python
class StreamCache:
    """ç¼“å­˜ streaming è¾“å‡ºï¼Œæ”¯æŒé‡æ”¾"""

    def __init__(self):
        self.cache = []

    async def stream_and_cache(self, graph, input_data, config):
        """Stream å¹¶ç¼“å­˜"""
        self.cache.clear()

        async for event in graph.astream_events(input_data, config, version="v2"):
            if event["event"] == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                self.cache.append(token)
                yield token

    def replay(self, delay=0.05):
        """é‡æ”¾ç¼“å­˜çš„è¾“å‡º"""
        import time
        for token in self.cache:
            print(token, end="", flush=True)
            time.sleep(delay)

# ä½¿ç”¨
cache = StreamCache()
async for token in cache.stream_and_cache(graph, input_data, config):
    print(token, end="")

# ç¨åé‡æ”¾
cache.replay(delay=0.1)
```

---

## ğŸ“Š Streaming æ€§èƒ½ä¼˜åŒ–

### 1. å‡å°‘äº‹ä»¶è¿‡æ»¤å¼€é”€

```python
# âŒ ä¸é«˜æ•ˆ - æ¯æ¬¡éƒ½æ£€æŸ¥å¤šä¸ªæ¡ä»¶
async for event in graph.astream_events(...):
    if (event["event"] == "on_chat_model_stream" and
        event["metadata"].get("langgraph_node") == "conversation" and
        event["data"] is not None and
        "chunk" in event["data"]):
        ...

# âœ… é«˜æ•ˆ - æå‰å‡†å¤‡æ¡ä»¶
target_event = "on_chat_model_stream"
target_node = "conversation"

async for event in graph.astream_events(...):
    if event["event"] == target_event:
        if event["metadata"].get("langgraph_node") == target_node:
            token = event["data"]["chunk"].content
            if token:
                yield token
```

---

### 2. æ‰¹é‡å¤„ç† Tokens

```python
async def batch_stream(graph, input_data, config, batch_size=5):
    """æ‰¹é‡è¾“å‡º tokensï¼Œå‡å°‘ I/O"""

    buffer = []

    async for event in graph.astream_events(input_data, config, version="v2"):
        if event["event"] == "on_chat_model_stream":
            token = event["data"]["chunk"].content
            buffer.append(token)

            if len(buffer) >= batch_size:
                yield "".join(buffer)
                buffer.clear()

    # è¾“å‡ºå‰©ä½™ tokens
    if buffer:
        yield "".join(buffer)

# ä½¿ç”¨
async for batch in batch_stream(graph, input_data, config):
    print(batch, end="", flush=True)
```

---

## ğŸ¯ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šèŠå¤©æœºå™¨äºº Web åº”ç”¨

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse

app = FastAPI()

@app.post("/chat/stream")
async def chat_stream(message: str, user_id: str):
    """SSE (Server-Sent Events) æµå¼èŠå¤©"""

    async def event_generator():
        config = {"configurable": {"thread_id": user_id}}
        input_msg = HumanMessage(content=message)

        async for event in graph.astream_events(
            {"messages": [input_msg]},
            config,
            version="v2"
        ):
            if event["event"] == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                if token:
                    # SSE æ ¼å¼
                    yield f"data: {token}\n\n"

        yield "data: [DONE]\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

---

### æ¡ˆä¾‹ 2ï¼šå‘½ä»¤è¡ŒèŠå¤©å·¥å…·

```python
import asyncio
from rich.console import Console
from rich.markdown import Markdown

console = Console()

async def cli_chat():
    """å‘½ä»¤è¡ŒèŠå¤©ç•Œé¢"""

    config = {"configurable": {"thread_id": "cli-session"}}

    while True:
        # ç”¨æˆ·è¾“å…¥
        user_input = console.input("\n[bold green]You:[/bold green] ")
        if user_input.lower() in ["exit", "quit"]:
            break

        # AI å“åº”
        console.print("[bold blue]AI:[/bold blue] ", end="")

        response_text = ""
        async for event in graph.astream_events(
            {"messages": [HumanMessage(content=user_input)]},
            config,
            version="v2"
        ):
            if event["event"] == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                if token:
                    console.print(token, end="", style="blue")
                    response_text += token

        print()  # æ¢è¡Œ

# è¿è¡Œ
asyncio.run(cli_chat())
```

---

### æ¡ˆä¾‹ 3ï¼šå¤šç”¨æˆ·å¹¶å‘èŠå¤©

```python
import asyncio
from collections import defaultdict

class MultiUserChatManager:
    """ç®¡ç†å¤šç”¨æˆ·å¹¶å‘èŠå¤©"""

    def __init__(self, graph):
        self.graph = graph
        self.active_streams = defaultdict(int)

    async def stream_for_user(self, user_id: str, message: str):
        """ä¸ºç‰¹å®šç”¨æˆ· stream å“åº”"""

        config = {"configurable": {"thread_id": user_id}}
        self.active_streams[user_id] += 1

        try:
            async for event in self.graph.astream_events(
                {"messages": [HumanMessage(content=message)]},
                config,
                version="v2"
            ):
                if event["event"] == "on_chat_model_stream":
                    token = event["data"]["chunk"].content
                    if token:
                        yield {
                            "user_id": user_id,
                            "token": token,
                            "timestamp": event["data"].get("timestamp")
                        }
        finally:
            self.active_streams[user_id] -= 1

    def get_active_users(self):
        """è·å–æ´»è·ƒç”¨æˆ·åˆ—è¡¨"""
        return [uid for uid, count in self.active_streams.items() if count > 0]

# ä½¿ç”¨
manager = MultiUserChatManager(graph)

# å¹¶å‘å¤„ç†å¤šä¸ªç”¨æˆ·
async def handle_multiple_users():
    async with asyncio.TaskGroup() as tg:
        tg.create_task(process_user("alice", "Hello!"))
        tg.create_task(process_user("bob", "Hi there!"))
        tg.create_task(process_user("charlie", "Good morning!"))

async def process_user(user_id, message):
    async for data in manager.stream_for_user(user_id, message):
        print(f"[{data['user_id']}] {data['token']}", end="")
```

---

## ğŸ“– æ‰©å±•é˜…è¯»

- [LangGraph Streaming å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming)
- [LangGraph API Streaming æ–‡æ¡£](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream-values/)
- [Python Async/Await æ•™ç¨‹](https://realpython.com/async-io-python/)
- [Server-Sent Events (SSE) è§„èŒƒ](https://html.spec.whatwg.org/multipage/server-sent-events.html)

---

## å®Œæ•´æ¡ˆä¾‹ä»£ç ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ã€å¯ä»¥ç›´æ¥åœ¨ Jupyter Notebook ä¸­è¿è¡Œçš„ä»£ç ç¤ºä¾‹ï¼Œæ¼”ç¤º LangGraph çš„æ‰€æœ‰ Streaming æ¨¡å¼ï¼š

```python
# ============================================================
# LangGraph Streaming å®Œæ•´ç¤ºä¾‹
# æ¼”ç¤ºï¼šupdatesã€valuesã€astream_events (token streaming)
# ============================================================

# --------------------------
# 1. å¯¼å…¥å¿…è¦çš„åº“
# --------------------------
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import MessagesState, StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from IPython.display import Image, display
import asyncio

# --------------------------
# 2. å®šä¹‰çŠ¶æ€ï¼ˆå¸¦æ‘˜è¦åŠŸèƒ½ï¼‰
# --------------------------
class State(MessagesState):
    summary: str  # å¯¹è¯æ‘˜è¦

# --------------------------
# 3. åˆå§‹åŒ–æ¨¡å‹
# --------------------------
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# --------------------------
# 4. å®šä¹‰èŠ‚ç‚¹
# --------------------------
def conversation(state: State, config: RunnableConfig):
    """å¯¹è¯èŠ‚ç‚¹ï¼šè°ƒç”¨ LLM ç”Ÿæˆå“åº”"""
    summary = state.get("summary", "")

    # å¦‚æœæœ‰æ‘˜è¦ï¼Œæ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯
    if summary:
        system_message = f"Summary of conversation earlier: {summary}"
        messages = [SystemMessage(content=system_message)] + state["messages"]
    else:
        messages = state["messages"]

    response = model.invoke(messages, config)
    return {"messages": response}

def summarize_conversation(state: State):
    """æ‘˜è¦èŠ‚ç‚¹ï¼šæ€»ç»“å¯¹è¯å¹¶åˆ é™¤æ—§æ¶ˆæ¯"""
    summary = state.get("summary", "")

    if summary:
        summary_message = (
            f"This is summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )
    else:
        summary_message = "Create a summary of the conversation above:"

    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

    # åˆ é™¤é™¤æœ€å 2 æ¡ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]

    return {"summary": response.content, "messages": delete_messages}

def should_continue(state: State):
    """å†³å®šæ˜¯å¦éœ€è¦æ€»ç»“"""
    if len(state["messages"]) > 6:
        return "summarize_conversation"
    return END

# --------------------------
# 5. æ„å»ºå›¾
# --------------------------
builder = StateGraph(State)

builder.add_node("conversation", conversation)
builder.add_node("summarize_conversation", summarize_conversation)

builder.add_edge(START, "conversation")
builder.add_conditional_edges("conversation", should_continue)
builder.add_edge("summarize_conversation", END)

memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# --------------------------
# 6. å¯è§†åŒ–å›¾ç»“æ„
# --------------------------
print("ğŸ“Š å›¾ç»“æ„å¯è§†åŒ–ï¼š")
display(Image(graph.get_graph().draw_mermaid_png()))

# --------------------------
# 7. æ¼”ç¤ºæ¨¡å¼ 1ï¼šstream_mode="updates"
# --------------------------
print("\n" + "=" * 60)
print("ğŸ“˜ æ¨¡å¼ 1ï¼šstream_mode='updates'ï¼ˆåªè¿”å›çŠ¶æ€æ›´æ–°ï¼‰")
print("=" * 60)

config1 = {"configurable": {"thread_id": "streaming-demo-1"}}
input1 = {"messages": [HumanMessage(content="Hi! I'm Alice. What's 2+2?")]}

print("\nâ–¶ï¸ æ‰§è¡Œä¸­...")
for chunk in graph.stream(input1, config1, stream_mode="updates"):
    print(f"\nğŸ”„ æ›´æ–°æ¥è‡ªèŠ‚ç‚¹: {list(chunk.keys())}")
    for node_name, update in chunk.items():
        if "messages" in update:
            msg = update["messages"]
            if hasattr(msg, 'content'):
                print(f"   å†…å®¹: {msg.content[:100]}...")

# --------------------------
# 8. æ¼”ç¤ºæ¨¡å¼ 2ï¼šstream_mode="values"
# --------------------------
print("\n" + "=" * 60)
print("ğŸ“— æ¨¡å¼ 2ï¼šstream_mode='values'ï¼ˆè¿”å›å®Œæ•´çŠ¶æ€ï¼‰")
print("=" * 60)

config2 = {"configurable": {"thread_id": "streaming-demo-2"}}
input2 = {"messages": [HumanMessage(content="Hello! Tell me a short joke.")]}

print("\nâ–¶ï¸ æ‰§è¡Œä¸­...")
step = 0
for event in graph.stream(input2, config2, stream_mode="values"):
    step += 1
    print(f"\n--- æ­¥éª¤ {step} ---")
    print(f"æ¶ˆæ¯æ•°é‡: {len(event['messages'])}")
    for i, msg in enumerate(event['messages']):
        role = msg.__class__.__name__.replace("Message", "")
        content = msg.content[:50] if msg.content else "[æ— å†…å®¹]"
        print(f"  {i+1}. [{role}] {content}...")

# --------------------------
# 9. æ¼”ç¤ºæ¨¡å¼ 3ï¼šastream_eventsï¼ˆToken Streamingï¼‰
# --------------------------
print("\n" + "=" * 60)
print("ğŸ“• æ¨¡å¼ 3ï¼šastream_eventsï¼ˆToken çº§æµå¼è¾“å‡ºï¼‰")
print("=" * 60)

config3 = {"configurable": {"thread_id": "streaming-demo-3"}}
input3 = {"messages": [HumanMessage(content="Count from 1 to 5 slowly, one number per line.")]}

async def demo_token_streaming():
    """æ¼”ç¤º Token çº§åˆ«çš„æµå¼è¾“å‡º"""
    print("\nâ–¶ï¸ Token æµå¼è¾“å‡ºï¼ˆç±»ä¼¼ ChatGPT æ•ˆæœï¼‰:")
    print("-" * 40)

    token_count = 0
    async for event in graph.astream_events(input3, config3, version="v2"):
        # åªå¤„ç† on_chat_model_stream äº‹ä»¶ï¼ˆToken è¾“å‡ºï¼‰
        if event["event"] == "on_chat_model_stream":
            node = event["metadata"].get("langgraph_node", "")
            if node == "conversation":  # åªå…³æ³¨ conversation èŠ‚ç‚¹
                token = event["data"]["chunk"].content
                if token:  # è¿‡æ»¤ç©º token
                    print(token, end="", flush=True)
                    token_count += 1

    print(f"\n-" * 40)
    print(f"âœ… å…±è¾“å‡º {token_count} ä¸ª token")

# è¿è¡Œå¼‚æ­¥æ¼”ç¤º
await demo_token_streaming()

# --------------------------
# 10. æ¼”ç¤ºï¼šäº‹ä»¶ç±»å‹æ€»è§ˆ
# --------------------------
print("\n" + "=" * 60)
print("ğŸ” astream_events äº‹ä»¶ç±»å‹æ€»è§ˆ")
print("=" * 60)

config4 = {"configurable": {"thread_id": "streaming-demo-4"}}
input4 = {"messages": [HumanMessage(content="Say 'Hello' only.")]}

async def demo_event_types():
    """å±•ç¤ºæ‰€æœ‰äº‹ä»¶ç±»å‹"""
    event_types = set()

    async for event in graph.astream_events(input4, config4, version="v2"):
        event_type = event["event"]
        node = event["metadata"].get("langgraph_node", "N/A")
        event_types.add((event_type, node))

    print("\nğŸ“‹ æ•è·åˆ°çš„äº‹ä»¶ç±»å‹:")
    for event_type, node in sorted(event_types):
        print(f"   [{node}] {event_type}")

await demo_event_types()

# --------------------------
# 11. æ¼”ç¤ºï¼šå¤šè½®å¯¹è¯ + æ‘˜è¦è§¦å‘
# --------------------------
print("\n" + "=" * 60)
print("ğŸ”„ å¤šè½®å¯¹è¯æ¼”ç¤ºï¼ˆè§¦å‘è‡ªåŠ¨æ‘˜è¦ï¼‰")
print("=" * 60)

config5 = {"configurable": {"thread_id": "streaming-demo-5"}}

# æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
messages_to_send = [
    "Hi, I'm Bob!",
    "What's the capital of France?",
    "And what about Germany?",
    "What's 10 * 10?",
]

for i, msg_content in enumerate(messages_to_send, 1):
    print(f"\n--- è½®æ¬¡ {i} ---")
    print(f"ğŸ‘¤ ç”¨æˆ·: {msg_content}")

    input_msg = {"messages": [HumanMessage(content=msg_content)]}

    for chunk in graph.stream(input_msg, config5, stream_mode="updates"):
        for node_name, update in chunk.items():
            print(f"ğŸ“ èŠ‚ç‚¹: {node_name}")
            if "messages" in update and hasattr(update["messages"], 'content'):
                content = update["messages"].content
                print(f"ğŸ¤– AI: {content[:100]}...")
            if "summary" in update:
                print(f"ğŸ“ æ‘˜è¦: {update['summary'][:100]}...")

# æ£€æŸ¥æœ€ç»ˆçŠ¶æ€
final_state = graph.get_state(config5)
print(f"\nğŸ“Š æœ€ç»ˆçŠ¶æ€:")
print(f"   æ¶ˆæ¯æ•°é‡: {len(final_state.values['messages'])}")
print(f"   æœ‰æ‘˜è¦: {'summary' in final_state.values and bool(final_state.values.get('summary'))}")

print("\nâœ¨ Streaming æ¼”ç¤ºå®Œæˆï¼")
```

**è¿è¡Œç»“æœç¤ºä¾‹ï¼š**

```
ğŸ“Š å›¾ç»“æ„å¯è§†åŒ–ï¼š
[æ˜¾ç¤ºå›¾ï¼šSTART â†’ conversation â†’ summarize_conversation â†’ END]

============================================================
ğŸ“˜ æ¨¡å¼ 1ï¼šstream_mode='updates'ï¼ˆåªè¿”å›çŠ¶æ€æ›´æ–°ï¼‰
============================================================

â–¶ï¸ æ‰§è¡Œä¸­...

ğŸ”„ æ›´æ–°æ¥è‡ªèŠ‚ç‚¹: ['conversation']
   å†…å®¹: Hi Alice! Nice to meet you. 2 + 2 = 4. Is there anything else...

============================================================
ğŸ“— æ¨¡å¼ 2ï¼šstream_mode='values'ï¼ˆè¿”å›å®Œæ•´çŠ¶æ€ï¼‰
============================================================

â–¶ï¸ æ‰§è¡Œä¸­...

--- æ­¥éª¤ 1 ---
æ¶ˆæ¯æ•°é‡: 1
  1. [Human] Hello! Tell me a short joke....

--- æ­¥éª¤ 2 ---
æ¶ˆæ¯æ•°é‡: 2
  1. [Human] Hello! Tell me a short joke....
  2. [AI] Why don't scientists trust atoms? Because they make...

============================================================
ğŸ“• æ¨¡å¼ 3ï¼šastream_eventsï¼ˆToken çº§æµå¼è¾“å‡ºï¼‰
============================================================

â–¶ï¸ Token æµå¼è¾“å‡ºï¼ˆç±»ä¼¼ ChatGPT æ•ˆæœï¼‰:
----------------------------------------
1
2
3
4
5
----------------------------------------
âœ… å…±è¾“å‡º 15 ä¸ª token
```

**ä»£ç è¦ç‚¹è¯´æ˜ï¼š**

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| `stream_mode="updates"` | åªè¿”å›æ¯ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€æ›´æ–° | è¿½è¸ªçŠ¶æ€å˜åŒ–ã€èŠ‚çœå¸¦å®½ |
| `stream_mode="values"` | è¿”å›å®Œæ•´çŠ¶æ€ï¼ˆç´¯ç§¯ï¼‰ | è°ƒè¯•ã€çŠ¶æ€æ£€æŸ¥ |
| `astream_events` + `version="v2"` | è¿”å›æ‰€æœ‰äº‹ä»¶ï¼ˆå« Tokenï¼‰ | å®ç°æ‰“å­—æœºæ•ˆæœ |
| è¿‡æ»¤ `on_chat_model_stream` | æå– LLM ç”Ÿæˆçš„æ¯ä¸ª Token | Token çº§æµå¼è¾“å‡º |

**å…³é”®è¿‡æ»¤æ¡ä»¶ï¼š**
```python
if event["event"] == "on_chat_model_stream":
    if event["metadata"].get("langgraph_node") == "conversation":
        token = event["data"]["chunk"].content
        if token:  # è¿‡æ»¤ç©º token
            print(token, end="", flush=True)
```

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆ astream_events éœ€è¦ version="v2"ï¼Ÿ

**ç­”ï¼š** `astream_events` æœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼š
- **v1**ï¼ˆæ—§ï¼‰ï¼šäº‹ä»¶ç»“æ„ä¸åŒï¼Œå·²å¼ƒç”¨
- **v2**ï¼ˆæ–°ï¼‰ï¼šæ ‡å‡†åŒ–çš„äº‹ä»¶ç»“æ„ï¼Œæ¨èä½¿ç”¨

```python
# âœ… æ­£ç¡®
async for event in graph.astream_events(..., version="v2")

# âŒ ä¸æ¨è
async for event in graph.astream_events(...)  # é»˜è®¤ v1
```

---

### Q2: stream å’Œ astream æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

| æ–¹æ³• | ç±»å‹ | è¿”å› | ç”¨æ³• |
|------|------|------|------|
| `stream()` | åŒæ­¥ | ç”Ÿæˆå™¨ | `for chunk in graph.stream(...)` |
| `astream()` | å¼‚æ­¥ | å¼‚æ­¥ç”Ÿæˆå™¨ | `async for chunk in graph.astream(...)` |

**æ€§èƒ½å·®å¼‚ï¼š**
- å¯¹äºå•ä¸ªè¯·æ±‚ï¼Œæ€§èƒ½ç›¸è¿‘
- å¯¹äºå¹¶å‘åœºæ™¯ï¼Œ`astream` æ›´é«˜æ•ˆ

---

### Q3: å¦‚ä½•åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨å¼‚æ­¥ï¼Ÿ

Jupyter å†…ç½®äº†äº‹ä»¶å¾ªç¯ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ `await`ï¼š

```python
# Jupyter ä¸­å¯ä»¥ç›´æ¥è¿™æ ·å†™
async for event in graph.astream_events(...):
    print(event)

# æˆ–
result = await some_async_function()
```

åœ¨æ™®é€š Python è„šæœ¬ä¸­ï¼Œéœ€è¦ï¼š
```python
import asyncio

asyncio.run(main())
```

---

### Q4: ä¸ºä»€ä¹ˆ token streaming æœ‰æ—¶ä¼šè¾“å‡ºç©ºå­—ç¬¦ä¸²ï¼Ÿ

LLM åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯èƒ½äº§ç”Ÿç©º tokenï¼ˆå¦‚å†…éƒ¨æ¨ç†æ­¥éª¤ï¼‰ï¼Œéœ€è¦è¿‡æ»¤ï¼š

```python
# âœ… è¿‡æ»¤ç©º token
async for event in graph.astream_events(...):
    if event["event"] == "on_chat_model_stream":
        token = event["data"]["chunk"].content
        if token:  # æ£€æŸ¥éç©º
            print(token, end="")
```

---

### Q5: å¦‚ä½•åœ¨ stream è¿‡ç¨‹ä¸­è·å–å®Œæ•´æ¶ˆæ¯ï¼Ÿ

**æ–¹æ³• 1ï¼šç´¯ç§¯ tokens**
```python
full_response = ""
async for event in graph.astream_events(...):
    if event["event"] == "on_chat_model_stream":
        token = event["data"]["chunk"].content
        full_response += token
        print(token, end="")

print(f"\n\nå®Œæ•´å“åº”: {full_response}")
```

**æ–¹æ³• 2ï¼šç›‘å¬ on_chat_model_end**
```python
async for event in graph.astream_events(...):
    if event["event"] == "on_chat_model_end":
        full_message = event["data"]["output"]
        print(f"å®Œæ•´æ¶ˆæ¯: {full_message.content}")
```

---

### Q6: messages æ¨¡å¼ä¸ºä»€ä¹ˆåªåœ¨ API ä¸­å¯ç”¨ï¼Ÿ

`messages` æ¨¡å¼æ˜¯ LangGraph API Server çš„ç‰¹æ®Šä¼˜åŒ–ï¼Œä¸“ä¸ºèŠå¤©åº”ç”¨è®¾è®¡ï¼š
- è‡ªåŠ¨å¤„ç†æ¶ˆæ¯å·®å¼‚
- æ›´é«˜æ•ˆçš„ç½‘ç»œä¼ è¾“
- ä¸å‰ç«¯æ¡†æ¶é›†æˆæ›´ç®€å•

æœ¬åœ° `graph.stream()` ä½¿ç”¨ `astream_events` å¯ä»¥è¾¾åˆ°ç±»ä¼¼æ•ˆæœã€‚

---

## ğŸ‰ æ€»ç»“

Streaming æ˜¯ LangGraph çš„æ ¸å¿ƒèƒ½åŠ›ï¼ŒæŒæ¡å®ƒçš„å…³é”®ï¼š

1. **ç†è§£ä¸åŒæ¨¡å¼**
   - `updates`ï¼šçŠ¶æ€æ›´æ–°
   - `values`ï¼šå®Œæ•´çŠ¶æ€
   - `astream_events`ï¼šäº‹ä»¶æµï¼ˆtoken streamingï¼‰
   - `messages`ï¼šAPI ä¸“å±ï¼ŒèŠå¤©ä¼˜åŒ–

2. **é€‰æ‹©åˆé€‚çš„å·¥å…·**
   - æœ¬åœ°å¼€å‘ï¼š`astream_events`
   - ç”Ÿäº§éƒ¨ç½²ï¼šLangGraph API + `messages` æ¨¡å¼

3. **æŒæ¡å¼‚æ­¥ç¼–ç¨‹**
   - `async`/`await`
   - `async for`
   - å¼‚æ­¥ç”Ÿæˆå™¨

4. **æœ€ä½³å®è·µ**
   - é”™è¯¯å¤„ç†
   - æ€§èƒ½ä¼˜åŒ–
   - ç”¨æˆ·ä½“éªŒï¼ˆè¿›åº¦æç¤ºã€æ‰¹é‡è¾“å‡ºï¼‰

é€šè¿‡ Streamingï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºç±»ä¼¼ ChatGPT çš„å®æ—¶äº¤äº’ä½“éªŒï¼Œè¿™æ˜¯ç°ä»£ AI åº”ç”¨çš„æ ‡é…ï¼

ä¸‹ä¸€èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†å­¦ä¹  **Interruptionï¼ˆäººæœºäº¤äº’ï¼‰**ï¼Œå®ç°åœ¨ Graph æ‰§è¡Œè¿‡ç¨‹ä¸­æš‚åœã€è®©ç”¨æˆ·ç¡®è®¤æˆ–ä¿®æ”¹çš„é«˜çº§åŠŸèƒ½ã€‚
