# Module-4 å°ç»“å’Œå¤ä¹ ï¼šHuman-in-the-Loop ç²¾é€šæŒ‡å—

> **æ¥è‡ªå›¾çµå¥–è·å¾—è€…çš„æ€»ç»“å¯„è¯­**
>
> "å½“ä½ å®Œæˆæœ¬ç« å­¦ä¹ ï¼Œä½ å·²ç»æŒæ¡äº†æ„å»ºå¯æ§ AI ç³»ç»Ÿçš„æ ¸å¿ƒæŠ€æœ¯ã€‚è®°ä½ï¼šæœ€å¼ºå¤§çš„ AI ä¸æ˜¯é‚£äº›èƒ½ç‹¬ç«‹å®Œæˆæ‰€æœ‰ä»»åŠ¡çš„ç³»ç»Ÿï¼Œè€Œæ˜¯é‚£äº›çŸ¥é“ä½•æ—¶éœ€è¦äººç±»ä»‹å…¥ã€å¦‚ä½•ä¼˜é›…åœ°ç­‰å¾…ã€å¹¶èƒ½ä»äººç±»åé¦ˆä¸­å­¦ä¹ çš„ç³»ç»Ÿã€‚ä½ ç°åœ¨æ‹¥æœ‰çš„ä¸ä»…æ˜¯æŠ€æœ¯å·¥å…·ï¼Œæ›´æ˜¯ä¸€å¥—å®Œæ•´çš„'å¯ä¿¡ AI'è®¾è®¡å“²å­¦ã€‚åœ¨æœªæ¥çš„ AI åº”ç”¨ä¸­ï¼ŒHuman-in-the-Loop å°†æˆä¸ºåŒºåˆ†ä¼˜ç§€ç³»ç»Ÿå’Œå“è¶Šç³»ç»Ÿçš„å…³é”®æ ‡å¿—ã€‚"
>
> â€” *å¯å‘è‡ª Alan Turing å¯¹äººæœºåä½œçš„å‰ç»æ€§æ€è€ƒ*

---

## ğŸ“‹ æœ¬ç« æ ¸å¿ƒçŸ¥è¯†å›é¡¾

### å­¦ä¹ åœ°å›¾

```mermaid
graph TB
    A[Human-in-the-Loop ä½“ç³»] --> B[Breakpoints]
    A --> C[Dynamic Breakpoints]
    A --> D[Edit State]
    A --> E[Streaming]
    A --> F[Time Travel]
    
    B --> B1[interrupt_before/after]
    B --> B2[Checkpointer]
    B --> B3[streamç»§ç»­æ‰§è¡Œ]
    
    C --> C1[NodeInterrupt]
    C --> C2[æ¡ä»¶æ€§ä¸­æ–­]
    C --> C3[ä¿¡æ¯ä¼ é€’]
    
    D --> D1[update_state]
    D --> D2[as_nodeå‚æ•°]
    D --> D3[Message IDè¦†ç›–]
    
    E --> E1[stream_modeé€‰æ‹©]
    E --> E2[Token Streaming]
    E --> E3[async/await]
    
    F --> F1[get_state_history]
    F --> F2[å›æ”¾Replaying]
    F --> F3[åˆ†å‰Forking]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1f5
    style D fill:#f5e1ff
    style E fill:#e1ffe1
    style F fill:#ffe1e1
```

### äº”å¤§æ ¸å¿ƒæŠ€æœ¯é€ŸæŸ¥è¡¨

| æŠ€æœ¯ | æ ¸å¿ƒAPI | ä¸»è¦ç”¨é€” | éš¾åº¦ |
|------|---------|---------|------|
| **Breakpoints** | `interrupt_before/after` | æ•æ„Ÿæ“ä½œå®¡æ‰¹ | â­â­â­ |
| **Dynamic Breakpoints** | `NodeInterrupt` | æ¡ä»¶æ€§æ™ºèƒ½ä¸­æ–­ | â­â­â­â­ |
| **Edit State** | `update_state(as_node=...)` | æ³¨å…¥äººç±»åé¦ˆ | â­â­â­â­ |
| **Streaming** | `astream_events` | å®æ—¶è¿›åº¦æ˜¾ç¤º | â­â­â­â­â­ |
| **Time Travel** | `get_state_history` + åˆ†å‰ | å†å²å›æº¯è°ƒè¯• | â­â­â­â­â­ |

---

## ğŸ¯ å¤ä¹ é¢˜ç›®åˆ—è¡¨

æœ¬ç« ç²¾å¿ƒè®¾è®¡äº† **10 é“ç»¼åˆæ€§é—®é¢˜**ï¼Œæ¶µç›–æ‰€æœ‰æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚å»ºè®®æŒ‰é¡ºåºå®Œæˆï¼Œæ¯é“é¢˜é¢„è®¡è€—æ—¶ 15-30 åˆ†é’Ÿã€‚

### åŸºç¡€ç†è§£ï¼ˆé—®é¢˜ 1-3ï¼‰
1. é™æ€æ–­ç‚¹ä¸åŠ¨æ€æ–­ç‚¹çš„æœ¬è´¨åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿå„è‡ªé€‚ç”¨äºä»€ä¹ˆåœºæ™¯ï¼Ÿ
2. `update_state` çš„ `as_node` å‚æ•°æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿä¸ä½¿ç”¨ä¼šæ€æ ·ï¼Ÿ
3. Streaming çš„å››ç§æ¨¡å¼ï¼ˆupdates/values/astream_events/messagesï¼‰æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

### å®æˆ˜åº”ç”¨ï¼ˆé—®é¢˜ 4-7ï¼‰
4. å¦‚ä½•å®ç°ä¸€ä¸ªæ”¯æŒæ¡ä»¶æ€§å®¡æ‰¹çš„æ™ºèƒ½å†…å®¹å®¡æ ¸ç³»ç»Ÿï¼Ÿ
5. å¦‚ä½•å®ç°ç±» ChatGPT çš„ Token çº§æµå¼è¾“å‡ºï¼Ÿ
6. å¦‚ä½•åˆ©ç”¨ Time Travel å®ç° AI å†³ç­–çš„"æ’¤é”€"åŠŸèƒ½ï¼Ÿ
7. è®¾è®¡ä¸€ä¸ªå®¢æœ Agentï¼Œéœ€è¦åœ¨æ•æ„Ÿæ“ä½œå‰æš‚åœå¹¶ä¿®æ”¹çŠ¶æ€

### é«˜çº§ç»¼åˆï¼ˆé—®é¢˜ 8-10ï¼‰
8. Message ID åœ¨çŠ¶æ€ç¼–è¾‘å’Œåˆ†å‰ä¸­çš„ä½œç”¨æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ
9. å¦‚ä½•ä¼˜åŒ– Streaming æ€§èƒ½ä»¥æ”¯æŒå¤§è§„æ¨¡å¹¶å‘ï¼Ÿ
10. è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ Human-in-the-Loop ç³»ç»Ÿæ¶æ„

---

## ğŸ“š è¯¦ç»†é—®ç­”è§£æ

### é—®é¢˜ 1: é™æ€æ–­ç‚¹ä¸åŠ¨æ€æ–­ç‚¹çš„æœ¬è´¨åŒºåˆ«

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒåŒºåˆ«

**é™æ€æ–­ç‚¹ï¼ˆStatic Breakpointsï¼‰ï¼š**
```python
# ç¼–è¯‘æ—¶å›ºå®šè®¾ç½®
graph = builder.compile(
    interrupt_before=["tools"],  # æ€»æ˜¯åœ¨ tools èŠ‚ç‚¹å‰æš‚åœ
    checkpointer=memory
)
```

**åŠ¨æ€æ–­ç‚¹ï¼ˆDynamic Breakpointsï¼‰ï¼š**
```python
# è¿è¡Œæ—¶æ¡ä»¶è§¦å‘
from langgraph.errors import NodeInterrupt

def content_check(state):
    risk_score = calculate_risk(state['content'])
    
    if risk_score > 0.7:  # åªåœ¨é«˜é£é™©æ—¶ä¸­æ–­
        raise NodeInterrupt(f"é£é™©è¯„åˆ†: {risk_score}ï¼Œéœ€è¦å®¡æ ¸")
    
    return state
```

#### å¯¹æ¯”è¡¨

| ç»´åº¦ | é™æ€æ–­ç‚¹ | åŠ¨æ€æ–­ç‚¹ |
|------|---------|---------|
| **è®¾ç½®æ—¶æœº** | ç¼–è¯‘æ—¶ï¼ˆ`compile()`ï¼‰ | è¿è¡Œæ—¶ï¼ˆèŠ‚ç‚¹å†…éƒ¨ï¼‰ |
| **è§¦å‘æ–¹å¼** | å›ºå®šèŠ‚ç‚¹ï¼Œå¿…ç„¶è§¦å‘ | æ¡ä»¶åˆ¤æ–­ï¼Œé€‰æ‹©æ€§è§¦å‘ |
| **ä¿¡æ¯ä¼ é€’** | æ— æ³•ä¼ é€’ä¸­æ–­åŸå›  | å¯ä¼ é€’è¯¦ç»†ä¿¡æ¯ |
| **çµæ´»æ€§** | ä½ï¼ˆæ¯æ¬¡éƒ½ä¸­æ–­ï¼‰ | é«˜ï¼ˆæ™ºèƒ½åˆ¤æ–­ï¼‰ |
| **å®ç°æ–¹å¼** | `interrupt_before/after` | `raise NodeInterrupt` |
| **é€‚ç”¨åœºæ™¯** | å›ºå®šå®¡æ‰¹ç‚¹ | æ™ºèƒ½å®¡æ ¸ |

#### é€‚ç”¨åœºæ™¯

**é™æ€æ–­ç‚¹é€‚ç”¨åœºæ™¯ï¼š**

1. **å›ºå®šå®¡æ‰¹æµç¨‹**
```python
# æ‰€æœ‰æ”¯ä»˜æ“ä½œéƒ½éœ€è¦äººå·¥ç¡®è®¤
graph = builder.compile(
    interrupt_before=["payment_tool"],
    checkpointer=memory
)
```

2. **è°ƒè¯•æ¨¡å¼**
```python
# å¼€å‘æ—¶åœ¨æ¯ä¸ªèŠ‚ç‚¹åæš‚åœæ£€æŸ¥
if DEBUG_MODE:
    graph = builder.compile(
        interrupt_after=["assistant", "tools", "router"],
        checkpointer=memory
    )
```

3. **ç”¨æˆ·ç¡®è®¤**
```python
# å…³é”®æ“ä½œå‰æ€»æ˜¯ç¡®è®¤
interrupt_before=["delete_data", "send_email", "deploy"]
```

**åŠ¨æ€æ–­ç‚¹é€‚ç”¨åœºæ™¯ï¼š**

1. **å†…å®¹å®¡æ ¸**
```python
def content_moderation(state):
    result = ai_moderation_api(state['content'])
    
    # é«˜é£é™©ï¼šç«‹å³æ‹’ç»
    if result.score > 0.9:
        raise NodeInterrupt(f"ä¸¥é‡è¿è§„ï¼Œå·²æ‹’ç»ã€‚é—®é¢˜: {result.issues}")
    
    # ä¸­é£é™©ï¼šäººå·¥å®¡æ ¸
    if result.score > 0.5:
        raise NodeInterrupt(f"éœ€è¦å®¡æ ¸ã€‚å¯ç–‘ç‚¹: {result.warnings}")
    
    # ä½é£é™©ï¼šè‡ªåŠ¨é€šè¿‡
    return state
```

2. **èµ„æºé™åˆ¶**
```python
def resource_check(state):
    if len(state['input']) > 10000:
        raise NodeInterrupt("è¾“å…¥è¿‡é•¿ï¼Œéœ€è¦ç¡®è®¤æ˜¯å¦å¤„ç†")
    
    if state['estimated_cost'] > 100:
        raise NodeInterrupt(f"é¢„ä¼°æˆæœ¬ ${state['estimated_cost']}ï¼Œéœ€è¦æ‰¹å‡†")
    
    return state
```

3. **å¼‚å¸¸æ£€æµ‹**
```python
def anomaly_detection(state):
    if detect_unusual_pattern(state):
        raise NodeInterrupt(
            f"æ£€æµ‹åˆ°å¼‚å¸¸æ¨¡å¼: {get_anomaly_description(state)}"
        )
    
    return state
```

#### ç»„åˆä½¿ç”¨ç¤ºä¾‹

```python
# é™æ€æ–­ç‚¹ï¼šæ‰€æœ‰å·¥å…·è°ƒç”¨å‰æš‚åœ
graph = builder.compile(
    interrupt_before=["tools"],
    checkpointer=memory
)

# åŠ¨æ€æ–­ç‚¹ï¼šåœ¨å·¥å…·èŠ‚ç‚¹å†…éƒ¨æ ¹æ®å·¥å…·ç±»å‹åˆ¤æ–­
def tools_with_dynamic_check(state):
    tool_call = state["messages"][-1].tool_calls[0]
    
    # é«˜é£é™©å·¥å…·éœ€è¦é¢å¤–å®¡æ‰¹
    if tool_call['name'] in ['delete', 'payment']:
        raise NodeInterrupt(
            f"é«˜é£é™©å·¥å…·: {tool_call['name']}ï¼Œéœ€è¦äºŒæ¬¡ç¡®è®¤"
        )
    
    # æ‰§è¡Œå·¥å…·
    return execute_tool(tool_call)
```

#### æœ€ä½³å®è·µ

**é€‰æ‹©æ ‡å‡†ï¼š**
- å›ºå®šæµç¨‹ â†’ é™æ€æ–­ç‚¹
- æ™ºèƒ½åˆ¤æ–­ â†’ åŠ¨æ€æ–­ç‚¹
- å¤šçº§å®¡æ‰¹ â†’ ç»„åˆä½¿ç”¨

**æ€§èƒ½è€ƒè™‘ï¼š**
- é™æ€æ–­ç‚¹å¼€é”€å°ï¼ˆç¼–è¯‘æ—¶è®¾ç½®ï¼‰
- åŠ¨æ€æ–­ç‚¹éœ€è¦è¿è¡Œæ—¶åˆ¤æ–­ï¼ˆä½†æ›´çµæ´»ï¼‰

</details>

---

### é—®é¢˜ 2: `update_state` çš„ `as_node` å‚æ•°ä½œç”¨

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒä½œç”¨

`as_node` å‚æ•°å‘Šè¯‰ LangGraphï¼š**è¿™ä¸ªçŠ¶æ€æ›´æ–°æ¥è‡ªå“ªä¸ªèŠ‚ç‚¹**ï¼Œä»è€Œå†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œå“ªä¸ªèŠ‚ç‚¹ã€‚

#### æœ‰æ—  `as_node` çš„åŒºåˆ«

**ä¸ä½¿ç”¨ `as_node`ï¼ˆé”™è¯¯ï¼‰ï¼š**
```python
# âŒ çŠ¶æ€æ›´æ–°äº†ï¼Œä½†å›¾ä¸çŸ¥é“ä»å“ªç»§ç»­
graph.update_state(thread, {"messages": [HumanMessage("æ–°æ¶ˆæ¯")]})

# è°ƒç”¨ stream(None, thread) ä¼šæ€æ ·ï¼Ÿ
# â†’ å›¾ä¸çŸ¥é“ä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯ä»€ä¹ˆï¼Œå¯èƒ½å‡ºé”™æˆ–é‡å¤æ‰§è¡Œ
```

**ä½¿ç”¨ `as_node`ï¼ˆæ­£ç¡®ï¼‰ï¼š**
```python
# âœ… æ¨¡æ‹Ÿ human_feedback èŠ‚ç‚¹çš„è¾“å‡º
graph.update_state(
    thread,
    {"messages": [HumanMessage("æ–°æ¶ˆæ¯")]},
    as_node="human_feedback"  # å‘Šè¯‰å›¾ï¼šè¿™æ˜¯ human_feedback èŠ‚ç‚¹çš„è¾“å‡º
)

# è°ƒç”¨ stream(None, thread)
# â†’ å›¾è®¤ä¸º human_feedback å·²å®Œæˆï¼Œæ‰§è¡Œå…¶åç»­èŠ‚ç‚¹
```

#### å·¥ä½œåŸç†

**å›¾ç»“æ„ç¤ºä¾‹ï¼š**
```
START â†’ [human_feedback] â†’ [assistant] â†’ [tools] â†’ END
```

**ä½¿ç”¨ `as_node` çš„æ‰§è¡Œæµï¼š**
```python
# 1. å›¾æš‚åœåœ¨ human_feedback å‰
for event in graph.stream(input, thread):
    print(event)

# 2. ä½œä¸º human_feedback èŠ‚ç‚¹æ›´æ–°çŠ¶æ€
graph.update_state(
    thread,
    {"messages": user_feedback},
    as_node="human_feedback"
)

# 3. ç»§ç»­æ‰§è¡Œï¼ˆä» assistant å¼€å§‹ï¼‰
for event in graph.stream(None, thread):
    print(event)  # human_feedback â†’ assistant â†’ tools â†’ END
```

**ä¸ä½¿ç”¨ `as_node` çš„æ··ä¹±ï¼š**
```python
# 1. æš‚åœåœ¨ human_feedback å‰
# 2. åªæ›´æ–°çŠ¶æ€ï¼Œä¸æŒ‡å®šèŠ‚ç‚¹
graph.update_state(thread, {"messages": user_feedback})

# 3. ç»§ç»­æ‰§è¡Œæ—¶
# â†’ å›¾å¯èƒ½ä»å¤´å¼€å§‹
# â†’ æˆ–é‡å¤æ‰§è¡Œ human_feedback
# â†’ æˆ–å‡ºç°ä¸å¯é¢„æµ‹çš„è¡Œä¸º
```

#### å®é™…åº”ç”¨ç¤ºä¾‹

**åœºæ™¯ 1ï¼šäººå·¥åé¦ˆèŠ‚ç‚¹**
```python
class State(TypedDict):
    messages: list
    user_input: str

def human_feedback(state):
    """ç©ºèŠ‚ç‚¹ï¼Œä»…ä½œä¸ºæš‚åœç‚¹"""
    pass

def assistant(state):
    return {"messages": [llm.invoke(state["messages"])]}

# æ„å»ºå›¾
builder = StateGraph(State)
builder.add_node("human_feedback", human_feedback)
builder.add_node("assistant", assistant)
builder.add_edge(START, "human_feedback")
builder.add_edge("human_feedback", "assistant")

graph = builder.compile(
    interrupt_before=["human_feedback"],
    checkpointer=memory
)

# æ‰§è¡Œæµç¨‹
thread = {"configurable": {"thread_id": "1"}}

# æš‚åœåœ¨ human_feedback å‰
for event in graph.stream({"messages": []}, thread):
    print(event)

# ç”¨æˆ·è¾“å…¥
user_input = input("ä½ çš„é—®é¢˜: ")

# ä½œä¸º human_feedback èŠ‚ç‚¹æ›´æ–°
graph.update_state(
    thread,
    {"messages": [HumanMessage(user_input)]},
    as_node="human_feedback"  # â­ å…³é”®
)

# ç»§ç»­æ‰§è¡Œï¼ˆè·³è¿‡ human_feedbackï¼Œæ‰§è¡Œ assistantï¼‰
for event in graph.stream(None, thread):
    print(event)
```

**åœºæ™¯ 2ï¼šé”™è¯¯ä¿®æ­£èŠ‚ç‚¹**
```python
def error_handler(state):
    """æ£€æŸ¥é”™è¯¯å¹¶ä¿®æ­£"""
    if state.get("error"):
        # éœ€è¦äººå·¥ä¿®æ­£
        raise NodeInterrupt("å‘ç°é”™è¯¯ï¼Œéœ€è¦ä¿®æ­£")
    return state

# é”™è¯¯å‘ç”Ÿ
for event in graph.stream(input, thread):
    print(event)

# äººå·¥ä¿®æ­£é”™è¯¯
corrected_data = fix_error(state.values)

graph.update_state(
    thread,
    corrected_data,
    as_node="error_handler"  # æ¨¡æ‹Ÿé”™è¯¯å·²ä¿®æ­£
)

# ç»§ç»­æ‰§è¡Œ
for event in graph.stream(None, thread):
    print(event)
```

#### è¿›é˜¶ç”¨æ³•ï¼šæ§åˆ¶æ‰§è¡Œæµ

**è·³è¿‡æŸäº›èŠ‚ç‚¹ï¼š**
```python
# å›¾ç»“æ„: A â†’ B â†’ C â†’ D
# æƒ³è·³è¿‡ B å’Œ Cï¼Œç›´æ¥ä» D å¼€å§‹

graph.update_state(
    thread,
    new_state,
    as_node="C"  # å‡è£… C å·²æ‰§è¡Œ
)

# ç»§ç»­æ‰§è¡Œ â†’ ç›´æ¥æ‰§è¡Œ D
```

**é‡æ–°æ‰§è¡ŒæŸä¸ªèŠ‚ç‚¹ï¼š**
```python
# è·å–å†å²çŠ¶æ€
history = list(graph.get_state_history(thread))
before_node_b = history[-3]

# ä¿®æ”¹çŠ¶æ€ï¼Œä½†æŒ‡å®šä¸º A çš„è¾“å‡º
graph.update_state(
    before_node_b.config,
    modified_state,
    as_node="A"
)

# ç»§ç»­æ‰§è¡Œ â†’ é‡æ–°æ‰§è¡Œ B
```

#### å¸¸è§é”™è¯¯

**é”™è¯¯ 1ï¼šå¿˜è®°ä½¿ç”¨ `as_node`**
```python
# âŒ é”™è¯¯
graph.update_state(thread, {"messages": [HumanMessage("hi")]})
graph.stream(None, thread)  # å¯èƒ½å‡ºé”™

# âœ… æ­£ç¡®
graph.update_state(thread, {"messages": [HumanMessage("hi")]}, as_node="human")
graph.stream(None, thread)
```

**é”™è¯¯ 2ï¼š`as_node` æŒ‡å®šäº†ä¸å­˜åœ¨çš„èŠ‚ç‚¹**
```python
# âŒ é”™è¯¯ï¼šèŠ‚ç‚¹åæ‹¼å†™é”™è¯¯
graph.update_state(thread, data, as_node="assitant")  # æ‹¼å†™é”™è¯¯

# âœ… æ­£ç¡®
graph.update_state(thread, data, as_node="assistant")
```

**é”™è¯¯ 3ï¼šä¸å›¾ç»“æ„ä¸åŒ¹é…**
```python
# å›¾ç»“æ„: A â†’ B â†’ C

# âŒ é”™è¯¯ï¼šC åé¢æ²¡æœ‰èŠ‚ç‚¹
graph.update_state(thread, data, as_node="C")
graph.stream(None, thread)  # æ— æ³•ç»§ç»­

# âœ… æ­£ç¡®ï¼šé€‰æ‹©åˆé€‚çš„èŠ‚ç‚¹
graph.update_state(thread, data, as_node="B")
graph.stream(None, thread)  # ç»§ç»­æ‰§è¡Œ C
```

</details>

---

### é—®é¢˜ 3: Streaming å››ç§æ¨¡å¼çš„åŒºåˆ«

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### å››ç§æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼ | ç²’åº¦ | è¾“å‡ºå†…å®¹ | æ•°æ®é‡ | å…¸å‹ç”¨é€” |
|------|------|---------|--------|---------|
| `updates` | èŠ‚ç‚¹çº§ | çŠ¶æ€å¢é‡æ›´æ–° | å° | è¿½è¸ªçŠ¶æ€å˜åŒ– |
| `values` | èŠ‚ç‚¹çº§ | å®Œæ•´çŠ¶æ€å¿«ç…§ | å¤§ | è°ƒè¯•ã€çŠ¶æ€æ£€æŸ¥ |
| `astream_events` | äº‹ä»¶çº§ | æ‰€æœ‰æ‰§è¡Œäº‹ä»¶ | æœ€å¤§ | Token streaming |
| `messages` (API) | Token çº§ | æ¶ˆæ¯æµ | ä¸­ | èŠå¤©åº”ç”¨ |

#### æ¨¡å¼ 1: `updates`

**ç‰¹ç‚¹ï¼š** åªè¿”å›èŠ‚ç‚¹å¯¹çŠ¶æ€çš„**å¢é‡æ›´æ–°**

```python
for chunk in graph.stream(input, thread, stream_mode="updates"):
    print(chunk)
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```python
{
    'assistant': {
        'messages': [AIMessage(content='Hello!', id='run-123')]
    }
}

{
    'tools': {
        'messages': [ToolMessage(content='6', name='multiply')]
    }
}
```

**è¾“å‡ºç»“æ„ï¼š**
```python
{
    'èŠ‚ç‚¹å': {
        'æ›´æ–°çš„å­—æ®µ': æ–°å€¼
    }
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- åªå…³å¿ƒçŠ¶æ€å˜åŒ–ï¼Œä¸éœ€è¦å®Œæ•´çŠ¶æ€
- å‡å°‘ç½‘ç»œä¼ è¾“æ•°æ®é‡
- è¿½è¸ªæ¯ä¸ªèŠ‚ç‚¹çš„è´¡çŒ®

**ä»£ç ç¤ºä¾‹ï¼š**
```python
for chunk in graph.stream(input, thread, stream_mode="updates"):
    for node_name, update in chunk.items():
        print(f"[{node_name}] æ›´æ–°äº†:")
        for field, value in update.items():
            print(f"  {field}: {value}")
```

#### æ¨¡å¼ 2: `values`

**ç‰¹ç‚¹ï¼š** è¿”å›æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œåçš„**å®Œæ•´çŠ¶æ€**

```python
for event in graph.stream(input, thread, stream_mode="values"):
    print(event)
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```python
# ç¬¬ä¸€æ¬¡è¾“å‡ºï¼ˆåˆå§‹çŠ¶æ€ï¼‰
{
    'messages': [HumanMessage(content='Multiply 2 and 3')]
}

# ç¬¬äºŒæ¬¡è¾“å‡ºï¼ˆassistant æ‰§è¡Œåï¼‰
{
    'messages': [
        HumanMessage(content='Multiply 2 and 3'),
        AIMessage(content='', tool_calls=[...])
    ]
}

# ç¬¬ä¸‰æ¬¡è¾“å‡ºï¼ˆtools æ‰§è¡Œåï¼‰
{
    'messages': [
        HumanMessage(content='Multiply 2 and 3'),
        AIMessage(content='', tool_calls=[...]),
        ToolMessage(content='6', name='multiply')
    ]
}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- è°ƒè¯•ï¼šæŸ¥çœ‹æ¯ä¸€æ­¥çš„å®Œæ•´çŠ¶æ€
- çŠ¶æ€æ£€æŸ¥ï¼šéªŒè¯æ•°æ®æ˜¯å¦æ­£ç¡®
- ç†è§£æ‰§è¡Œæµç¨‹

**ä»£ç ç¤ºä¾‹ï¼š**
```python
for event in graph.stream(input, thread, stream_mode="values"):
    print(f"å½“å‰æ¶ˆæ¯æ•°: {len(event['messages'])}")
    for msg in event['messages']:
        msg.pretty_print()
    print("=" * 50)
```

#### æ¨¡å¼ 3: `astream_events`

**ç‰¹ç‚¹ï¼š** è¾“å‡ºå›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„**æ‰€æœ‰äº‹ä»¶**ï¼Œæ”¯æŒ Token çº§æµå¼è¾“å‡º

```python
async for event in graph.astream_events(input, thread, version="v2"):
    print(event)
```

**äº‹ä»¶ç±»å‹ï¼š**
```python
# èŠ‚ç‚¹å¼€å§‹
{"event": "on_chain_start", "name": "assistant"}

# æç¤ºè¯å¤„ç†
{"event": "on_prompt_start", "name": "ChatPromptTemplate"}

# LLM å¼€å§‹
{"event": "on_chat_model_start", "name": "ChatOpenAI"}

# Token æµï¼ˆâ­ æœ€é‡è¦ï¼‰
{"event": "on_chat_model_stream", "data": {"chunk": AIMessageChunk(content="The")}}
{"event": "on_chat_model_stream", "data": {"chunk": AIMessageChunk(content=" result")}}
{"event": "on_chat_model_stream", "data": {"chunk": AIMessageChunk(content=" is")}}

# LLM ç»“æŸ
{"event": "on_chat_model_end", "data": {"output": AIMessage(...)}}
```

**Token Streaming å®ç°ï¼š**
```python
async for event in graph.astream_events(input, thread, version="v2"):
    # åªå¤„ç† Token æµäº‹ä»¶
    if event["event"] == "on_chat_model_stream":
        token = event["data"]["chunk"].content
        if token:
            print(token, end="", flush=True)
```

**è¿‡æ»¤ç‰¹å®šèŠ‚ç‚¹çš„ Tokenï¼š**
```python
target_node = "assistant"

async for event in graph.astream_events(input, thread, version="v2"):
    if (event["event"] == "on_chat_model_stream" and
        event["metadata"].get("langgraph_node") == target_node):
        token = event["data"]["chunk"].content
        print(token, end="", flush=True)
```

**é€‚ç”¨åœºæ™¯ï¼š**
- å®ç°ç±» ChatGPT çš„æ‰“å­—æœºæ•ˆæœ
- å®æ—¶æ˜¾ç¤º AI æ€è€ƒè¿‡ç¨‹
- ç»†ç²’åº¦çš„æ‰§è¡Œç›‘æ§

#### æ¨¡å¼ 4: `messages` (API ä¸“å±)

**ç‰¹ç‚¹ï¼š** LangGraph API Server æä¾›çš„ä¸“é—¨ä¼˜åŒ–æ¨¡å¼ï¼Œè‡ªåŠ¨å¤„ç†æ¶ˆæ¯å·®å¼‚

```python
# éœ€è¦ä½¿ç”¨ LangGraph API
from langgraph_sdk import get_client

client = get_client(url="http://127.0.0.1:2024")

async for event in client.runs.stream(
    thread_id,
    assistant_id="agent",
    input=input_data,
    stream_mode="messages"
):
    print(event.event, event.data)
```

**äº‹ä»¶ç±»å‹ï¼š**
```python
# å…ƒæ•°æ®
{"event": "metadata", "data": {"run_id": "..."}}

# å®Œæ•´æ¶ˆæ¯
{"event": "messages/complete", "data": [{"content": "Hello", "role": "user"}]}

# éƒ¨åˆ†æ¶ˆæ¯ï¼ˆToken æµï¼‰
{"event": "messages/partial", "data": [{"content": "The"}]}
{"event": "messages/partial", "data": [{"content": "The result"}]}
{"event": "messages/partial", "data": [{"content": "The result is"}]}

# æ¶ˆæ¯å…ƒæ•°æ®
{"event": "messages/metadata", "data": {"finish_reason": "stop"}}
```

**é€‚ç”¨åœºæ™¯ï¼š**
- ç”Ÿäº§çº§èŠå¤©åº”ç”¨
- ä¸å‰ç«¯æ¡†æ¶é›†æˆ
- æ›´é«˜æ•ˆçš„ç½‘ç»œä¼ è¾“

**å¯¹æ¯”æœ¬åœ° `astream_events`ï¼š**

| ç‰¹æ€§ | astream_events | messages (API) |
|------|----------------|----------------|
| å¯ç”¨æ€§ | æœ¬åœ°å›¾ | ä»… API Server |
| Token æµ | éœ€è¦è¿‡æ»¤äº‹ä»¶ | è‡ªåŠ¨å¤„ç† |
| æ¶ˆæ¯å·®å¼‚ | æ‰‹åŠ¨è®¡ç®— | è‡ªåŠ¨è®¡ç®— |
| ç½‘ç»œä¼˜åŒ– | æ—  | æœ‰ |

#### æ¨¡å¼é€‰æ‹©æŒ‡å—

```python
# åœºæ™¯ 1ï¼šè¿½è¸ªçŠ¶æ€å˜åŒ–
for chunk in graph.stream(input, thread, stream_mode="updates"):
    log_state_changes(chunk)

# åœºæ™¯ 2ï¼šè°ƒè¯•å’Œå¼€å‘
for event in graph.stream(input, thread, stream_mode="values"):
    debug_full_state(event)

# åœºæ™¯ 3ï¼šèŠå¤©åº”ç”¨ï¼ˆæœ¬åœ°ï¼‰
async for event in graph.astream_events(input, thread, version="v2"):
    if event["event"] == "on_chat_model_stream":
        display_token(event["data"]["chunk"].content)

# åœºæ™¯ 4ï¼šèŠå¤©åº”ç”¨ï¼ˆAPIï¼Œæ¨èï¼‰
async for event in client.runs.stream(..., stream_mode="messages"):
    if event.event == "messages/partial":
        display_token(event.data[0]["content"])
```

#### æ€§èƒ½è€ƒè™‘

| æ¨¡å¼ | æ•°æ®é‡ | ç½‘ç»œå¼€é”€ | CPU å¼€é”€ |
|------|--------|---------|---------|
| `updates` | æœ€å° | æœ€å° | æœ€å° |
| `values` | å¤§ | å¤§ | å° |
| `astream_events` | æœ€å¤§ | æœ€å¤§ | ä¸­ |
| `messages` (API) | ä¸­ | ä¸­ | å°ï¼ˆæœåŠ¡å™¨ç«¯ï¼‰ |

**ä¼˜åŒ–å»ºè®®ï¼š**
- ç”Ÿäº§ç¯å¢ƒä¼˜å…ˆä½¿ç”¨ `updates` æˆ– `messages`ï¼ˆAPIï¼‰
- è°ƒè¯•æ—¶ä½¿ç”¨ `values`
- Token streaming ä½¿ç”¨ `messages`ï¼ˆAPIï¼‰æˆ–ä¼˜åŒ–åçš„ `astream_events`

</details>

---

### é—®é¢˜ 4: å®ç°æ¡ä»¶æ€§å®¡æ‰¹çš„æ™ºèƒ½å†…å®¹å®¡æ ¸ç³»ç»Ÿ

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### éœ€æ±‚åˆ†æ

æ„å»ºä¸€ä¸ªå†…å®¹å®¡æ ¸ç³»ç»Ÿï¼Œæ ¹æ®é£é™©è¯„åˆ†æ™ºèƒ½å†³å®šæ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸ï¼š
- **é«˜é£é™©ï¼ˆ> 0.9ï¼‰**ï¼šè‡ªåŠ¨æ‹’ç»
- **ä¸­é£é™©ï¼ˆ0.5-0.9ï¼‰**ï¼šäººå·¥å®¡æ ¸
- **ä½é£é™©ï¼ˆ< 0.5ï¼‰**ï¼šè‡ªåŠ¨é€šè¿‡

#### å®Œæ•´å®ç°

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.errors import NodeInterrupt
from typing import TypedDict, Literal
import json

# å®šä¹‰çŠ¶æ€
class ContentState(TypedDict):
    content: str
    risk_score: float
    issues: list[str]
    review_decision: str  # "approve", "reject", "modify"
    status: str

# é£é™©è¯„ä¼°èŠ‚ç‚¹
def assess_risk(state: ContentState):
    """ä½¿ç”¨ AI æ¨¡å‹è¯„ä¼°å†…å®¹é£é™©"""
    content = state['content']
    
    # è°ƒç”¨å†…å®¹å®¡æ ¸ APIï¼ˆç¤ºä¾‹ï¼‰
    result = moderation_api.analyze(content)
    
    return {
        "risk_score": result.score,
        "issues": result.detected_issues
    }

# åŠ¨æ€æ–­ç‚¹èŠ‚ç‚¹
def conditional_approval(state: ContentState):
    """æ ¹æ®é£é™©è¯„åˆ†å†³å®šæ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸"""
    score = state['risk_score']
    issues = state.get('issues', [])
    
    # é«˜é£é™©ï¼šè‡ªåŠ¨æ‹’ç»
    if score > 0.9:
        return {
            "status": "auto_rejected",
            "review_decision": "reject"
        }
    
    # ä¸­é£é™©ï¼šè§¦å‘äººå·¥å®¡æ ¸
    if score > 0.5:
        # å‡†å¤‡è¯¦ç»†ä¿¡æ¯
        interrupt_info = {
            "risk_score": score,
            "issues": issues,
            "content_preview": state['content'][:200],
            "recommendation": "å»ºè®®äººå·¥å®¡æ ¸"
        }
        
        # æŠ›å‡º NodeInterrupt
        raise NodeInterrupt(json.dumps(interrupt_info, ensure_ascii=False))
    
    # ä½é£é™©ï¼šè‡ªåŠ¨é€šè¿‡
    return {
        "status": "auto_approved",
        "review_decision": "approve"
    }

# æ‰§è¡Œå†³ç­–èŠ‚ç‚¹
def execute_decision(state: ContentState):
    """æ ¹æ®å®¡æ ¸å†³ç­–æ‰§è¡Œç›¸åº”æ“ä½œ"""
    decision = state.get('review_decision')
    
    if decision == "approve":
        return {"status": "approved", "message": "å†…å®¹å·²å‘å¸ƒ"}
    elif decision == "reject":
        return {"status": "rejected", "message": "å†…å®¹å·²æ‹’ç»"}
    elif decision == "modify":
        # å¯ä»¥è¦æ±‚ç”¨æˆ·ä¿®æ”¹
        return {"status": "needs_modification"}
    else:
        return {"status": "pending"}

# æ„å»ºå›¾
builder = StateGraph(ContentState)

builder.add_node("assess_risk", assess_risk)
builder.add_node("conditional_approval", conditional_approval)
builder.add_node("execute_decision", execute_decision)

builder.add_edge(START, "assess_risk")
builder.add_edge("assess_risk", "conditional_approval")
builder.add_edge("conditional_approval", "execute_decision")
builder.add_edge("execute_decision", END)

# ç¼–è¯‘ï¼ˆéœ€è¦ checkpointer æ”¯æŒåŠ¨æ€æ–­ç‚¹ï¼‰
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)

# æ‰§è¡Œæµç¨‹
def moderate_content(content: str, thread_id: str):
    """å†…å®¹å®¡æ ¸å®Œæ•´æµç¨‹"""
    thread = {"configurable": {"thread_id": thread_id}}
    
    # 1. æäº¤å†…å®¹å®¡æ ¸
    input_data = {"content": content}
    
    for event in graph.stream(input_data, thread, stream_mode="values"):
        print(f"çŠ¶æ€: {event.get('status', 'processing')}")
    
    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸
    state = graph.get_state(thread)
    
    if state.tasks and state.tasks[0].interrupts:
        # è§£æä¸­æ–­ä¿¡æ¯
        interrupt_info = json.loads(state.tasks[0].interrupts[0].value)
        
        print("\nâš ï¸  éœ€è¦äººå·¥å®¡æ ¸:")
        print(f"  é£é™©è¯„åˆ†: {interrupt_info['risk_score']}")
        print(f"  æ£€æµ‹åˆ°çš„é—®é¢˜: {', '.join(interrupt_info['issues'])}")
        print(f"  å†…å®¹é¢„è§ˆ: {interrupt_info['content_preview']}")
        
        # 3. è·å–äººå·¥å†³ç­–
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("  1. æ‰¹å‡† (approve)")
        print("  2. æ‹’ç» (reject)")
        print("  3. è¦æ±‚ä¿®æ”¹ (modify)")
        
        choice = input("é€‰æ‹© (1/2/3): ")
        
        decision_map = {"1": "approve", "2": "reject", "3": "modify"}
        decision = decision_map.get(choice, "reject")
        
        # 4. æ›´æ–°å†³ç­–å¹¶ç»§ç»­
        graph.update_state(
            thread,
            {"review_decision": decision},
            as_node="conditional_approval"
        )
        
        # 5. ç»§ç»­æ‰§è¡Œ
        for event in graph.stream(None, thread, stream_mode="values"):
            print(f"æœ€ç»ˆçŠ¶æ€: {event.get('status')}")
            print(f"æ¶ˆæ¯: {event.get('message')}")
    
    else:
        # è‡ªåŠ¨å¤„ç†ï¼ˆæ— éœ€äººå·¥å®¡æ ¸ï¼‰
        print(f"âœ… è‡ªåŠ¨å¤„ç†: {state.values.get('status')}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯• 1ï¼šä½é£é™©å†…å®¹ï¼ˆè‡ªåŠ¨é€šè¿‡ï¼‰
    moderate_content("è¿™æ˜¯ä¸€ç¯‡å…³äºæŠ€æœ¯çš„æ–‡ç« ", "thread_1")
    
    # æµ‹è¯• 2ï¼šä¸­é£é™©å†…å®¹ï¼ˆéœ€è¦äººå·¥å®¡æ ¸ï¼‰
    moderate_content("è¿™ç¯‡æ–‡ç« åŒ…å«ä¸€äº›äº‰è®®æ€§è§‚ç‚¹", "thread_2")
    
    # æµ‹è¯• 3ï¼šé«˜é£é™©å†…å®¹ï¼ˆè‡ªåŠ¨æ‹’ç»ï¼‰
    moderate_content("åŒ…å«ä¸¥é‡è¿è§„å†…å®¹", "thread_3")
```

#### è¿›é˜¶ä¼˜åŒ–

**1. å¤šçº§é£é™©è¯„åˆ†ï¼š**
```python
def advanced_risk_assessment(state):
    """å¤šç»´åº¦é£é™©è¯„ä¼°"""
    content = state['content']
    
    # å¤šä¸ªç»´åº¦è¯„åˆ†
    violence_score = check_violence(content)
    hate_speech_score = check_hate_speech(content)
    spam_score = check_spam(content)
    
    # ç»¼åˆè¯„åˆ†
    ç»¼åˆ_score = max(violence_score, hate_speech_score, spam_score)
    
    # è¯¦ç»†é—®é¢˜åˆ—è¡¨
    issues = []
    if violence_score > 0.5:
        issues.append(f"æš´åŠ›å†…å®¹ ({violence_score:.2f})")
    if hate_speech_score > 0.5:
        issues.append(f"ä»‡æ¨è¨€è®º ({hate_speech_score:.2f})")
    if spam_score > 0.5:
        issues.append(f"åƒåœ¾ä¿¡æ¯ ({spam_score:.2f})")
    
    return {
        "risk_score": max_score,
        "issues": issues,
        "violence_score": violence_score,
        "hate_speech_score": hate_speech_score,
        "spam_score": spam_score
    }
```

**2. å®¡æ‰¹å†å²è®°å½•ï¼š**
```python
from datetime import datetime

å®¡æ‰¹_log = []

def log_approval_decision(thread_id, content, score, decision, reviewer):
    """è®°å½•å®¡æ‰¹å†å²"""
    å®¡æ‰¹_log.append({
        "timestamp": datetime.now().isoformat(),
        "thread_id": thread_id,
        "content_hash": hash(content),
        "risk_score": score,
        "decision": decision,
        "reviewer": reviewer
    })

# åœ¨äººå·¥å®¡æ ¸åè°ƒç”¨
log_approval_decision(
    thread_id,
    state['content'],
    state['risk_score'],
    decision,
    reviewer_id="admin_001"
)
```

**3. è‡ªåŠ¨å­¦ä¹ é˜ˆå€¼ï¼š**
```python
class AdaptiveThreshold:
    """è‡ªé€‚åº”é£é™©é˜ˆå€¼"""
    
    def __init__(self, initial_threshold=0.5):
        self.threshold = initial_threshold
        self.approval_history = []
    
    def update(self, score, human_decision):
        """æ ¹æ®äººå·¥å†³ç­–è°ƒæ•´é˜ˆå€¼"""
        self.approval_history.append((score, human_decision))
        
        # å¦‚æœä½åˆ†å†…å®¹è¢«æ‹’ç»ï¼Œé™ä½é˜ˆå€¼
        if score < self.threshold and human_decision == "reject":
            self.threshold = max(0.3, self.threshold - 0.05)
        
        # å¦‚æœé«˜åˆ†å†…å®¹è¢«æ‰¹å‡†ï¼Œæé«˜é˜ˆå€¼
        if score > self.threshold and human_decision == "approve":
            self.threshold = min(0.8, self.threshold + 0.05)
    
    def should_review(self, score):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸"""
        return score > self.threshold

# ä½¿ç”¨
threshold_manager = AdaptiveThreshold()

def smart_conditional_approval(state):
    score = state['risk_score']
    
    if threshold_manager.should_review(score):
        raise NodeInterrupt(f"é£é™©è¯„åˆ† {score} è¶…è¿‡é˜ˆå€¼ {threshold_manager.threshold}")
    
    return {"status": "auto_approved"}
```

</details>

---

### é—®é¢˜ 5: å®ç°ç±» ChatGPT çš„ Token çº§æµå¼è¾“å‡º

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ç¯å¢ƒ | å¤æ‚åº¦ | æ¨èåº¦ |
|------|------|--------|--------|
| `astream_events` | æœ¬åœ° | ä¸­ | â­â­â­â­ |
| `messages` (API) | LangGraph Server | ä½ | â­â­â­â­â­ |

#### æ–¹æ¡ˆ 1: æœ¬åœ° `astream_events`

```python
import asyncio
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver

# å®šä¹‰æ¨¡å‹
llm = ChatOpenAI(model="gpt-4", streaming=True)  # å¯ç”¨ streaming

# å®šä¹‰èŠ‚ç‚¹
def chatbot(state: MessagesState):
    return {"messages": [llm.invoke(state["messages"])]}

# æ„å»ºå›¾
builder = StateGraph(MessagesState)
builder.add_node("chatbot", chatbot)
builder.add_edge(START, "chatbot")
builder.add_edge("chatbot", END)

graph = builder.compile(checkpointer=MemorySaver())

# Token Streaming å®ç°
async def stream_chat_response(user_message: str, thread_id: str):
    """æµå¼è¾“å‡ºèŠå¤©å“åº”"""
    from langchain_core.messages import HumanMessage
    
    config = {"configurable": {"thread_id": thread_id}}
    input_data = {"messages": [HumanMessage(content=user_message)]}
    
    print(f"ç”¨æˆ·: {user_message}")
    print("AI: ", end="", flush=True)
    
    full_response = ""
    
    async for event in graph.astream_events(input_data, config, version="v2"):
        # è¿‡æ»¤ Token æµäº‹ä»¶
        if (event["event"] == "on_chat_model_stream" and
            event["metadata"].get("langgraph_node") == "chatbot"):
            
            token = event["data"]["chunk"].content
            
            if token:
                print(token, end="", flush=True)
                full_response += token
    
    print("\n")  # æ¢è¡Œ
    return full_response

# ä½¿ç”¨
if __name__ == "__main__":
    asyncio.run(stream_chat_response(
        "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ",
        "user_123"
    ))
```

**è¾“å‡ºæ•ˆæœï¼š**
```
ç”¨æˆ·: ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ
AI: LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„ã€å¤šæ­¥éª¤çš„ AI åº”ç”¨ç¨‹åºçš„æ¡†æ¶...
```

#### æ–¹æ¡ˆ 2: LangGraph API `messages` æ¨¡å¼ï¼ˆæ¨èï¼‰

**å¯åŠ¨ API Serverï¼š**
```bash
cd your_project
langgraph dev
```

**å®¢æˆ·ç«¯å®ç°ï¼š**
```python
from langgraph_sdk import get_client
from langchain_core.messages import HumanMessage

async def stream_chat_api(user_message: str):
    """ä½¿ç”¨ API çš„ messages æ¨¡å¼æµå¼è¾“å‡º"""
    
    # è¿æ¥åˆ° LangGraph Server
    client = get_client(url="http://127.0.0.1:2024")
    
    # åˆ›å»ºçº¿ç¨‹
    thread = await client.threads.create()
    
    print(f"ç”¨æˆ·: {user_message}")
    print("AI: ", end="", flush=True)
    
    full_response = ""
    
    async for event in client.runs.stream(
        thread["thread_id"],
        assistant_id="chatbot",
        input={"messages": [HumanMessage(content=user_message)]},
        stream_mode="messages"
    ):
        # å¤„ç†éƒ¨åˆ†æ¶ˆæ¯ï¼ˆToken æµï¼‰
        if event.event == "messages/partial":
            for item in event.data:
                if "content" in item and item.get("type") != "human":
                    # æå–æ–°å¢çš„ Token
                    content = item["content"]
                    
                    # è®¡ç®—å¢é‡ï¼ˆä¸ä¹‹å‰çš„å“åº”å¯¹æ¯”ï¼‰
                    if len(content) > len(full_response):
                        new_token = content[len(full_response):]
                        print(new_token, end="", flush=True)
                        full_response = content
    
    print("\n")
    return full_response

# ä½¿ç”¨
asyncio.run(stream_chat_api("è§£é‡Šä»€ä¹ˆæ˜¯ Token Streaming"))
```

#### ä¼˜åŒ–æŠ€å·§

**1. æ‰¹é‡è¾“å‡ºï¼ˆå‡å°‘ I/Oï¼‰ï¼š**
```python
async def batched_stream(user_message: str, batch_size=3):
    """æ‰¹é‡è¾“å‡º Token"""
    buffer = []
    
    async for event in graph.astream_events(...):
        if event["event"] == "on_chat_model_stream":
            token = event["data"]["chunk"].content
            
            if token:
                buffer.append(token)
                
                # ç¼“å†²åŒºæ»¡æ—¶è¾“å‡º
                if len(buffer) >= batch_size:
                    print("".join(buffer), end="", flush=True)
                    buffer.clear()
    
    # è¾“å‡ºå‰©ä½™ Token
    if buffer:
        print("".join(buffer), end="", flush=True)
```

**2. æ·»åŠ æ‰“å­—æœºæ•ˆæœï¼š**
```python
import time

async def typewriter_effect(user_message: str, delay=0.05):
    """æ‰“å­—æœºæ•ˆæœ"""
    async for event in graph.astream_events(...):
        if event["event"] == "on_chat_model_stream":
            token = event["data"]["chunk"].content
            
            if token:
                print(token, end="", flush=True)
                await asyncio.sleep(delay)  # å»¶è¿Ÿ
```

**3. å®æ—¶æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼š**
```python
async def stream_with_progress(user_message: str):
    """æ˜¾ç¤º AI æ€è€ƒè¿‡ç¨‹"""
    current_node = None
    
    async for event in graph.astream_events(...):
        # èŠ‚ç‚¹å¼€å§‹
        if event["event"] == "on_chain_start":
            node = event["metadata"].get("langgraph_node")
            if node and node != current_node:
                current_node = node
                print(f"\n[{node}] ", end="", flush=True)
        
        # Token è¾“å‡º
        elif event["event"] == "on_chat_model_stream":
            token = event["data"]["chunk"].content
            if token:
                print(token, end="", flush=True)
```

#### Web åº”ç”¨é›†æˆ

**FastAPI + SSE (Server-Sent Events)ï¼š**
```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse

app = FastAPI()

@app.post("/chat/stream")
async def chat_stream(message: str, thread_id: str):
    """SSE æµå¼èŠå¤©ç«¯ç‚¹"""
    
    async def event_generator():
        config = {"configurable": {"thread_id": thread_id}}
        input_data = {"messages": [HumanMessage(content=message)]}
        
        async for event in graph.astream_events(input_data, config, version="v2"):
            if event["event"] == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                
                if token:
                    # SSE æ ¼å¼
                    yield {
                        "event": "token",
                        "data": token
                    }
        
        # å‘é€å®Œæˆä¿¡å·
        yield {
            "event": "done",
            "data": "[DONE]"
        }
    
    return EventSourceResponse(event_generator())

# å‰ç«¯ä½¿ç”¨ EventSource æ¥æ”¶
```

**å‰ç«¯ JavaScript ç¤ºä¾‹ï¼š**
```javascript
const eventSource = new EventSource(`/chat/stream?message=hello&thread_id=123`);

let responseDiv = document.getElementById('response');

eventSource.addEventListener('token', (e) => {
    responseDiv.textContent += e.data;
});

eventSource.addEventListener('done', (e) => {
    eventSource.close();
    console.log('Stream completed');
});
```

#### é”™è¯¯å¤„ç†

```python
async def safe_stream(user_message: str):
    """å¸¦é”™è¯¯å¤„ç†çš„æµå¼è¾“å‡º"""
    try:
        full_response = ""
        
        async for event in graph.astream_events(...):
            if event["event"] == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                
                if token:
                    print(token, end="", flush=True)
                    full_response += token
        
        return full_response
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        return full_response
        
    except asyncio.TimeoutError:
        print("\n\nâš ï¸  å“åº”è¶…æ—¶")
        return full_response
        
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        return full_response
```

</details>

---

### é—®é¢˜ 6: åˆ©ç”¨ Time Travel å®ç°"æ’¤é”€"åŠŸèƒ½

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒæ€è·¯

åˆ©ç”¨ LangGraph çš„ checkpointer è‡ªåŠ¨ä¿å­˜çš„å†å²çŠ¶æ€ï¼Œå®ç°"æ’¤é”€åˆ°ä¸Šä¸€æ­¥"çš„åŠŸèƒ½ã€‚

#### å®Œæ•´å®ç°

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

class UndoableAgent:
    """æ”¯æŒæ’¤é”€åŠŸèƒ½çš„ Agent"""
    
    def __init__(self):
        # å®šä¹‰ LLM
        self.llm = ChatOpenAI(model="gpt-4")
        
        # æ„å»ºå›¾
        builder = StateGraph(MessagesState)
        builder.add_node("assistant", self.assistant_node)
        builder.add_edge(START, "assistant")
        builder.add_edge("assistant", END)
        
        # ä½¿ç”¨ checkpointerï¼ˆå¿…éœ€ï¼‰
        self.memory = MemorySaver()
        self.graph = builder.compile(checkpointer=self.memory)
    
    def assistant_node(self, state: MessagesState):
        """åŠ©æ‰‹èŠ‚ç‚¹"""
        return {"messages": [self.llm.invoke(state["messages"])]}
    
    def execute(self, user_message: str, thread_id: str):
        """æ‰§è¡Œå¹¶è¿”å›å“åº”"""
        thread = {"configurable": {"thread_id": thread_id}}
        input_data = {"messages": [HumanMessage(content=user_message)]}
        
        # æ‰§è¡Œå›¾
        result = None
        for event in self.graph.stream(input_data, thread, stream_mode="values"):
            result = event
        
        return result['messages'][-1].content
    
    def undo(self, thread_id: str, steps=1):
        """æ’¤é”€æœ€è¿‘çš„ N æ­¥"""
        thread = {"configurable": {"thread_id": thread_id}}
        
        # è·å–å†å²
        history = list(self.graph.get_state_history(thread))
        
        if len(history) <= steps:
            print("âš ï¸  æ— æ³•æ’¤é”€ï¼šæ²¡æœ‰è¶³å¤Ÿçš„å†å²è®°å½•")
            return None
        
        # å›åˆ° N æ­¥ä¹‹å‰çš„çŠ¶æ€
        target_state = history[steps]
        
        print(f"âœ… å·²æ’¤é”€åˆ° {steps} æ­¥ä¹‹å‰")
        print(f"   Checkpoint ID: {target_state.config['configurable']['checkpoint_id'][:8]}...")
        
        return target_state
    
    def get_history_summary(self, thread_id: str):
        """æ˜¾ç¤ºå†å²æ‘˜è¦"""
        thread = {"configurable": {"thread_id": thread_id}}
        history = list(self.graph.get_state_history(thread))
        
        print(f"\nğŸ“œ å†å²è®°å½•ï¼ˆå…± {len(history)} ä¸ªçŠ¶æ€ï¼‰:")
        for i, state in enumerate(history):
            step_num = len(history) - i - 1
            timestamp = state.created_at.strftime("%H:%M:%S") if state.created_at else "N/A"
            msg_count = len(state.values.get('messages', []))
            
            # è·å–æœ€åä¸€æ¡æ¶ˆæ¯çš„é¢„è§ˆ
            if msg_count > 0:
                last_msg = state.values['messages'][-1]
                preview = last_msg.content[:50] if hasattr(last_msg, 'content') else str(last_msg)[:50]
            else:
                preview = "(ç©º)"
            
            print(f"  [{step_num}] {timestamp} - {msg_count} æ¡æ¶ˆæ¯ - {preview}")
        
        return history
    
    def continue_from_checkpoint(self, checkpoint_config):
        """ä»æŒ‡å®š checkpoint ç»§ç»­æ‰§è¡Œ"""
        # ä»å†å²çŠ¶æ€ç»§ç»­
        result = None
        for event in self.graph.stream(None, checkpoint_config, stream_mode="values"):
            result = event
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
def demo_undo():
    """æ¼”ç¤ºæ’¤é”€åŠŸèƒ½"""
    agent = UndoableAgent()
    thread_id = "demo_thread"
    
    print("=== å¯¹è¯å¼€å§‹ ===\n")
    
    # æ­¥éª¤ 1
    print("ç”¨æˆ·: ä½ å¥½")
    response1 = agent.execute("ä½ å¥½", thread_id)
    print(f"AI: {response1}\n")
    
    # æ­¥éª¤ 2
    print("ç”¨æˆ·: æˆ‘æƒ³äº†è§£ LangGraph")
    response2 = agent.execute("æˆ‘æƒ³äº†è§£ LangGraph", thread_id)
    print(f"AI: {response2}\n")
    
    # æ­¥éª¤ 3
    print("ç”¨æˆ·: å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ")
    response3 = agent.execute("å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ", thread_id)
    print(f"AI: {response3}\n")
    
    # æ˜¾ç¤ºå†å²
    history = agent.get_history_summary(thread_id)
    
    # æ’¤é”€æœ€è¿‘çš„ 2 æ­¥
    print("\n=== æ‰§è¡Œæ’¤é”€ ===")
    target_state = agent.undo(thread_id, steps=2)
    
    if target_state:
        # æŸ¥çœ‹æ’¤é”€åçš„çŠ¶æ€
        print("\næ’¤é”€åçš„æ¶ˆæ¯:")
        for msg in target_state.values['messages']:
            print(f"  - {msg.content[:50]}")
        
        # ä»æ’¤é”€ç‚¹ç»§ç»­æ–°çš„å¯¹è¯
        print("\n=== ä»æ’¤é”€ç‚¹ç»§ç»­æ–°å¯¹è¯ ===")
        
        # ä¿®æ”¹è¾“å…¥ï¼Œåˆ›å»ºæ–°åˆ†æ”¯
        new_response = agent.execute("å…¶å®æˆ‘æƒ³äº†è§£ Python", thread_id)
        print(f"ç”¨æˆ·: å…¶å®æˆ‘æƒ³äº†è§£ Python")
        print(f"AI: {new_response}")

if __name__ == "__main__":
    demo_undo()
```

#### é«˜çº§åŠŸèƒ½ï¼šå¤šçº§æ’¤é”€æ ˆ

```python
class AdvancedUndoAgent(UndoableAgent):
    """æ”¯æŒå¤šçº§æ’¤é”€/é‡åšçš„ Agent"""
    
    def __init__(self):
        super().__init__()
        self.undo_stack = []  # æ’¤é”€æ ˆ
        self.redo_stack = []  # é‡åšæ ˆ
    
    def execute(self, user_message: str, thread_id: str):
        """æ‰§è¡Œå¹¶è®°å½•åˆ°æ’¤é”€æ ˆ"""
        # è·å–æ‰§è¡Œå‰çš„çŠ¶æ€
        thread = {"configurable": {"thread_id": thread_id}}
        before_state = self.graph.get_state(thread)
        
        # æ‰§è¡Œæ“ä½œ
        response = super().execute(user_message, thread_id)
        
        # è·å–æ‰§è¡Œåçš„çŠ¶æ€
        after_state = self.graph.get_state(thread)
        
        # ä¿å­˜åˆ°æ’¤é”€æ ˆ
        self.undo_stack.append({
            "before": before_state,
            "after": after_state,
            "action": user_message
        })
        
        # æ¸…ç©ºé‡åšæ ˆ
        self.redo_stack.clear()
        
        return response
    
    def undo_last(self, thread_id: str):
        """æ’¤é”€æœ€åä¸€æ¬¡æ“ä½œ"""
        if not self.undo_stack:
            print("âš ï¸  æ— æ³•æ’¤é”€ï¼šæ’¤é”€æ ˆä¸ºç©º")
            return False
        
        # å¼¹å‡ºæœ€åä¸€æ¬¡æ“ä½œ
        last_action = self.undo_stack.pop()
        
        # ä¿å­˜åˆ°é‡åšæ ˆ
        self.redo_stack.append(last_action)
        
        # æ¢å¤åˆ°ä¹‹å‰çš„çŠ¶æ€ï¼ˆé€šè¿‡åˆ†å‰ï¼‰
        thread = {"configurable": {"thread_id": thread_id}}
        self.graph.update_state(
            thread,
            last_action["before"].values
        )
        
        print(f"âœ… å·²æ’¤é”€æ“ä½œ: {last_action['action'][:50]}")
        return True
    
    def redo_last(self, thread_id: str):
        """é‡åšæœ€åä¸€æ¬¡æ’¤é”€çš„æ“ä½œ"""
        if not self.redo_stack:
            print("âš ï¸  æ— æ³•é‡åšï¼šé‡åšæ ˆä¸ºç©º")
            return False
        
        # å¼¹å‡ºé‡åšæ ˆ
        action = self.redo_stack.pop()
        
        # æ¢å¤åˆ°æ‰§è¡Œåçš„çŠ¶æ€
        thread = {"configurable": {"thread_id": thread_id}}
        self.graph.update_state(
            thread,
            action["after"].values
        )
        
        # ä¿å­˜å›æ’¤é”€æ ˆ
        self.undo_stack.append(action)
        
        print(f"âœ… å·²é‡åšæ“ä½œ: {action['action'][:50]}")
        return True
```

#### ç”¨æˆ·ç•Œé¢ç¤ºä¾‹

```python
def interactive_chat_with_undo():
    """äº¤äº’å¼èŠå¤©ï¼Œæ”¯æŒæ’¤é”€"""
    agent = AdvancedUndoAgent()
    thread_id = "interactive_session"
    
    print("ğŸ¤– æ”¯æŒæ’¤é”€çš„èŠå¤©æœºå™¨äºº")
    print("   å‘½ä»¤: /undo - æ’¤é”€, /redo - é‡åš, /history - å†å², /quit - é€€å‡º\n")
    
    while True:
        user_input = input("ä½ : ")
        
        # å¤„ç†å‘½ä»¤
        if user_input == "/quit":
            break
        elif user_input == "/undo":
            agent.undo_last(thread_id)
            continue
        elif user_input == "/redo":
            agent.redo_last(thread_id)
            continue
        elif user_input == "/history":
            agent.get_history_summary(thread_id)
            continue
        
        # æ­£å¸¸å¯¹è¯
        try:
            response = agent.execute(user_input, thread_id)
            print(f"AI: {response}\n")
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}\n")

if __name__ == "__main__":
    interactive_chat_with_undo()
```

#### æ—¶é—´çº¿å¯è§†åŒ–

```python
def visualize_timeline(agent, thread_id):
    """å¯è§†åŒ–å¯¹è¯æ—¶é—´çº¿"""
    thread = {"configurable": {"thread_id": thread_id}}
    history = list(agent.graph.get_state_history(thread))
    
    print("\nğŸ“Š å¯¹è¯æ—¶é—´çº¿:")
    print("=" * 60)
    
    for i, state in enumerate(reversed(history)):
        step_num = len(history) - i - 1
        indent = "    " * (step_num % 3)  # ç¼©è¿›æ˜¾ç¤ºå±‚æ¬¡
        
        # è·å–æ¶ˆæ¯æ•°
        msg_count = len(state.values.get('messages', []))
        
        # æ˜¾ç¤º
        print(f"{indent}[{step_num}] {msg_count} æ¡æ¶ˆæ¯")
        
        # æ˜¾ç¤ºæœ€åä¸€æ¡æ¶ˆæ¯
        if msg_count > 0:
            last_msg = state.values['messages'][-1]
            if hasattr(last_msg, 'content'):
                preview = last_msg.content[:40]
                print(f"{indent}    â””â”€ {preview}...")
        
        # åˆ†æ”¯æ ‡è®°
        if i < len(history) - 1:
            print(f"{indent}    â”‚")
    
    print("=" * 60)
```

</details>

---

### é—®é¢˜ 7: è®¾è®¡æ•æ„Ÿæ“ä½œå‰æš‚åœå¹¶ä¿®æ”¹çŠ¶æ€çš„å®¢æœ Agent

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### éœ€æ±‚åˆ†æ

æ„å»ºä¸€ä¸ªå®¢æœ Agentï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
1. åœ¨æ‰§è¡Œæ•æ„Ÿæ“ä½œï¼ˆå¦‚é€€æ¬¾ã€åˆ é™¤è®¢å•ï¼‰å‰æš‚åœ
2. æ˜¾ç¤ºæ“ä½œè¯¦æƒ…ä¾›äººå·¥å®¡æ ¸
3. å…è®¸ä¿®æ”¹æ“ä½œå‚æ•°
4. æ ¹æ®å®¡æ ¸ç»“æœç»§ç»­æˆ–å–æ¶ˆ

#### å®Œæ•´å®ç°

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from typing import TypedDict, List, Literal
from langchain_openai import ChatOpenAI

# å®šä¹‰å·¥å…·
defæŸ¥è¯¢è®¢å•(order_id: str) -> dict:
    """æŸ¥è¯¢è®¢å•ä¿¡æ¯"""
    return {"order_id": order_id, "status": "å·²å‘è´§", "amount": 299.00}

defç”³è¯·é€€æ¬¾(order_id: str, amount: float, reason: str) -> dict:
    """ç”³è¯·é€€æ¬¾ï¼ˆæ•æ„Ÿæ“ä½œï¼‰"""
    return {"order_id": order_id, "refund_amount": amount, "status": "é€€æ¬¾æˆåŠŸ"}

defåˆ é™¤è®¢å•(order_id: str) -> dict:
    """åˆ é™¤è®¢å•ï¼ˆæ•æ„Ÿæ“ä½œï¼‰"""
    return {"order_id": order_id, "status": "å·²åˆ é™¤"}

# å·¥å…·åˆ†ç±»
SENSITIVE_TOOLS = ["ç”³è¯·é€€æ¬¾", "åˆ é™¤è®¢å•"]

# å®šä¹‰çŠ¶æ€
class CustomerServiceState(TypedDict):
    messages: List
    pending_tool: dict  # å¾…æ‰§è¡Œçš„å·¥å…·
    approval_status: Literal["pending", "approved", "rejected"]
    modified_params: dict  # ä¿®æ”¹åçš„å‚æ•°

# æ„å»º Agent
llm = ChatOpenAI(model="gpt-4")
tools = [æŸ¥è¯¢è®¢å•, ç”³è¯·é€€æ¬¾, åˆ é™¤è®¢å•]
llm_with_tools = llm.bind_tools(tools)

def assistant(state: CustomerServiceState):
    """åŠ©æ‰‹èŠ‚ç‚¹ï¼šç”Ÿæˆå·¥å…·è°ƒç”¨"""
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

def check_if_sensitive(state: CustomerServiceState):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æ‰¹"""
    last_message = state["messages"][-1]
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        tool_call = last_message.tool_calls[0]
        
        if tool_call['name'] in SENSITIVE_TOOLS:
            # ä¿å­˜å¾…å®¡æ‰¹çš„å·¥å…·è°ƒç”¨
            return {
                "pending_tool": tool_call,
                "approval_status": "pending"
            }
    
    return {"approval_status": "approved"}

def route_after_check(state: CustomerServiceState):
    """æ ¹æ®å®¡æ‰¹çŠ¶æ€è·¯ç”±"""
    if state.get("approval_status") == "pending":
        return "wait_for_approval"  # éœ€è¦å®¡æ‰¹
    else:
        return "execute_tool"  # ç›´æ¥æ‰§è¡Œ

def wait_for_approval(state: CustomerServiceState):
    """ç­‰å¾…å®¡æ‰¹èŠ‚ç‚¹ï¼ˆç©ºèŠ‚ç‚¹ï¼Œç”¨äº breakpointï¼‰"""
    pass

def execute_tool(state: CustomerServiceState):
    """æ‰§è¡Œå·¥å…·èŠ‚ç‚¹"""
    # è·å–å·¥å…·è°ƒç”¨ï¼ˆå¯èƒ½å·²è¢«ä¿®æ”¹ï¼‰
    if state.get("modified_params"):
        # ä½¿ç”¨ä¿®æ”¹åçš„å‚æ•°
        tool_call = state["pending_tool"].copy()
        tool_call['args'] = state["modified_params"]
    else:
        # ä½¿ç”¨åŸå§‹å‚æ•°
        tool_call = state["messages"][-1].tool_calls[0]
    
    # æ‰§è¡Œå·¥å…·
    tool_name = tool_call['name']
    tool_args = tool_call['args']
    
    if tool_name == "æŸ¥è¯¢è®¢å•":
        result = æŸ¥è¯¢è®¢å•(**tool_args)
    elif tool_name == "ç”³è¯·é€€æ¬¾":
        result = ç”³è¯·é€€æ¬¾(**tool_args)
    elif tool_name == "åˆ é™¤è®¢å•":
        result = åˆ é™¤è®¢å•(**tool_args)
    
    # è¿”å›å·¥å…·ç»“æœ
    from langchain_core.messages import ToolMessage
    return {"messages": [ToolMessage(
        content=str(result),
        tool_call_id=tool_call['id']
    )]}

# æ„å»ºå›¾
builder = StateGraph(CustomerServiceState)

builder.add_node("assistant", assistant)
builder.add_node("check_if_sensitive", check_if_sensitive)
builder.add_node("wait_for_approval", wait_for_approval)
builder.add_node("execute_tool", execute_tool)

builder.add_edge(START, "assistant")
builder.add_edge("assistant", "check_if_sensitive")
builder.add_conditional_edges(
    "check_if_sensitive",
    route_after_check,
    {
        "wait_for_approval": "wait_for_approval",
        "execute_tool": "execute_tool"
    }
)
builder.add_edge("wait_for_approval", "execute_tool")
builder.add_edge("execute_tool", "assistant")

# ç¼–è¯‘ï¼ˆåœ¨ wait_for_approval å‰ä¸­æ–­ï¼‰
memory = MemorySaver()
graph = builder.compile(
    interrupt_before=["wait_for_approval"],
    checkpointer=memory
)

# å®Œæ•´æµç¨‹
def customer_service_flow():
    """å®¢æœç³»ç»Ÿå®Œæ•´æµç¨‹"""
    from langchain_core.messages import HumanMessage
    
    thread = {"configurable": {"thread_id": "customer_001"}}
    
    # ç”¨æˆ·è¯·æ±‚é€€æ¬¾
    print("=== ç”¨æˆ·è¯·æ±‚ ===")
    user_request = "æˆ‘è¦ç”³è¯·è®¢å• #12345 çš„é€€æ¬¾ï¼Œé‡‘é¢ 299 å…ƒ"
    print(f"ç”¨æˆ·: {user_request}\n")
    
    input_data = {"messages": [HumanMessage(content=user_request)]}
    
    # æ‰§è¡Œåˆ°æ–­ç‚¹
    for event in graph.stream(input_data, thread, stream_mode="values"):
        if 'pending_tool' in event:
            print("âš ï¸  æ£€æµ‹åˆ°æ•æ„Ÿæ“ä½œï¼Œç­‰å¾…å®¡æ‰¹...")
            break
    
    # è·å–çŠ¶æ€ï¼ŒæŸ¥çœ‹å¾…å®¡æ‰¹çš„æ“ä½œ
    state = graph.get_state(thread)
    pending_tool = state.values.get('pending_tool')
    
    if pending_tool:
        print("\n=== å¾…å®¡æ‰¹æ“ä½œ ===")
        print(f"å·¥å…·: {pending_tool['name']}")
        print(f"å‚æ•°: {pending_tool['args']}")
        
        # äººå·¥å®¡æ ¸
        print("\n=== å®¡æ ¸é€‰é¡¹ ===")
        print("1. æ‰¹å‡†ï¼ˆåŸå‚æ•°ï¼‰")
        print("2. æ‰¹å‡†ï¼ˆä¿®æ”¹å‚æ•°ï¼‰")
        print("3. æ‹’ç»")
        
        choice = input("\né€‰æ‹© (1/2/3): ")
        
        if choice == "1":
            # æ‰¹å‡†ï¼Œä½¿ç”¨åŸå‚æ•°
            print("\nâœ… å·²æ‰¹å‡†ï¼Œæ‰§è¡Œä¸­...")
            for event in graph.stream(None, thread, stream_mode="values"):
                if 'messages' in event:
                    last_msg = event['messages'][-1]
                    if hasattr(last_msg, 'content'):
                        print(f"ç»“æœ: {last_msg.content}")
        
        elif choice == "2":
            # ä¿®æ”¹å‚æ•°åæ‰¹å‡†
            print("\nğŸ“ ä¿®æ”¹å‚æ•°:")
            new_amount = input(f"  é€€æ¬¾é‡‘é¢ (åŸ: {pending_tool['args']['amount']}): ")
            new_reason = input(f"  é€€æ¬¾åŸå›  (åŸ: {pending_tool['args']['reason']}): ")
            
            # æ›´æ–°çŠ¶æ€
            modified_params = pending_tool['args'].copy()
            if new_amount:
                modified_params['amount'] = float(new_amount)
            if new_reason:
                modified_params['reason'] = new_reason
            
            graph.update_state(
                thread,
                {"modified_params": modified_params},
                as_node="wait_for_approval"
            )
            
            print("\nâœ… å·²æ‰¹å‡†ï¼ˆä¿®æ”¹åå‚æ•°ï¼‰ï¼Œæ‰§è¡Œä¸­...")
            for event in graph.stream(None, thread, stream_mode="values"):
                if 'messages' in event:
                    last_msg = event['messages'][-1]
                    if hasattr(last_msg, 'content'):
                        print(f"ç»“æœ: {last_msg.content}")
        
        else:
            # æ‹’ç»
            print("\nâŒ å·²æ‹’ç»æ“ä½œ")
            # å¯ä»¥æ·»åŠ æ‹’ç»åŸå› åˆ°çŠ¶æ€

if __name__ == "__main__":
    customer_service_flow()
```

#### è¿›é˜¶ï¼šå®¡æ‰¹æ—¥å¿—ç³»ç»Ÿ

```python
from datetime import datetime
import json

class ApprovalLogger:
    """å®¡æ‰¹æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_file="approval_log.jsonl"):
        self.log_file = log_file
    
    def log(self, thread_id, tool_call, decision, modifier=None, reviewer=None):
        """è®°å½•å®¡æ‰¹æ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "thread_id": thread_id,
            "tool": tool_call['name'],
            "original_args": tool_call['args'],
            "modified_args": modifier,
            "decision": decision,  # "approved", "rejected", "modified"
            "reviewer": reviewer or "system"
        }
        
        with open(self.log_file, "a") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
        
        print(f"ğŸ“ å·²è®°å½•å®¡æ‰¹æ—¥å¿—: {decision}")
    
    def get_logs(self, thread_id=None):
        """è·å–å®¡æ‰¹æ—¥å¿—"""
        logs = []
        
        with open(self.log_file, "r") as f:
            for line in f:
                log = json.loads(line)
                if thread_id is None or log["thread_id"] == thread_id:
                    logs.append(log)
        
        return logs

# åœ¨å®¡æ‰¹æµç¨‹ä¸­ä½¿ç”¨
logger = ApprovalLogger()

# è®°å½•æ‰¹å‡†
logger.log(
    thread_id="customer_001",
    tool_call=pending_tool,
    decision="approved",
    reviewer="admin_user"
)

# è®°å½•ä¿®æ”¹åæ‰¹å‡†
logger.log(
    thread_id="customer_001",
    tool_call=pending_tool,
    decision="modified",
    modifier=modified_params,
    reviewer="admin_user"
)
```

</details>

---

### é—®é¢˜ 8: Message ID åœ¨çŠ¶æ€ç¼–è¾‘å’Œåˆ†å‰ä¸­çš„ä½œç”¨æœºåˆ¶

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒæœºåˆ¶

Message ID æ˜¯ LangChain æ¶ˆæ¯ç³»ç»Ÿçš„å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œåœ¨ `add_messages` reducer ä¸­èµ·åˆ°å…³é”®ä½œç”¨ï¼š

**Reducer è¡Œä¸ºè§„åˆ™ï¼š**
- **æœ‰ IDï¼ŒID ç›¸åŒ** â†’ è¦†ç›–ï¼ˆæ›´æ–°ï¼‰
- **æœ‰ IDï¼ŒID ä¸åŒ** â†’ è¿½åŠ 
- **æ—  ID** â†’ è¿½åŠ ï¼ˆè‡ªåŠ¨ç”Ÿæˆæ–° IDï¼‰

#### æ¶ˆæ¯è¦†ç›–æœºåˆ¶

```python
from langchain_core.messages import HumanMessage

# åŸå§‹æ¶ˆæ¯
original = HumanMessage(content="Hello", id="msg_001")

# åœºæ™¯ 1ï¼šè¦†ç›–ï¼ˆä¿ç•™ç›¸åŒ IDï¼‰
updated = HumanMessage(content="Hi there", id="msg_001")  # ç›¸åŒ ID
graph.update_state(thread, {"messages": [updated]})
# ç»“æœï¼šoriginal è¢« updated è¦†ç›–

# åœºæ™¯ 2ï¼šè¿½åŠ ï¼ˆä½¿ç”¨ä¸åŒ IDï¼‰
new_msg = HumanMessage(content="Another message", id="msg_002")  # ä¸åŒ ID
graph.update_state(thread, {"messages": [new_msg]})
# ç»“æœï¼šnew_msg è¿½åŠ åˆ°åˆ—è¡¨æœ«å°¾

# åœºæ™¯ 3ï¼šè¿½åŠ ï¼ˆä¸æä¾› IDï¼‰
no_id_msg = HumanMessage(content="No ID message")  # æ—  ID
graph.update_state(thread, {"messages": [no_id_msg]})
# ç»“æœï¼šè‡ªåŠ¨ç”Ÿæˆ IDï¼Œè¿½åŠ åˆ°åˆ—è¡¨æœ«å°¾
```

#### åœ¨åˆ†å‰ä¸­çš„åº”ç”¨

**åˆ†å‰çš„æ ¸å¿ƒï¼šè¦†ç›–å†å²æ¶ˆæ¯**

```python
# è·å–å†å²çŠ¶æ€
history = list(graph.get_state_history(thread))
fork_point = history[-2]

# è·å–è¦ä¿®æ”¹çš„æ¶ˆæ¯
original_msg = fork_point.values['messages'][0]
print(f"åŸå§‹æ¶ˆæ¯ ID: {original_msg.id}")
print(f"åŸå§‹å†…å®¹: {original_msg.content}")

# åˆ›å»ºåˆ†å‰ï¼šä¿ç•™ IDï¼Œä¿®æ”¹å†…å®¹
forked_msg = HumanMessage(
    content="ä¿®æ”¹åçš„å†…å®¹",
    id=original_msg.id  # â­ å…³é”®ï¼šä¿ç•™ç›¸åŒ ID
)

# æ›´æ–°çŠ¶æ€
fork_config = graph.update_state(
    fork_point.config,
    {"messages": [forked_msg]}
)

# ä»åˆ†å‰ç»§ç»­æ‰§è¡Œ
for event in graph.stream(None, fork_config):
    print(event)
```

**å¦‚æœä¸ä¿ç•™ ID ä¼šæ€æ ·ï¼Ÿ**

```python
# âŒ é”™è¯¯ï¼šä¸ä¿ç•™ ID
forked_msg = HumanMessage(content="ä¿®æ”¹åçš„å†…å®¹")  # æ–° ID

graph.update_state(fork_point.config, {"messages": [forked_msg]})

# ç»“æœï¼š
# - åŸå§‹æ¶ˆæ¯ä»ç„¶å­˜åœ¨
# - æ–°æ¶ˆæ¯è¿½åŠ åˆ°æœ«å°¾
# - æ¶ˆæ¯åˆ—è¡¨å˜æˆ: [åŸå§‹æ¶ˆæ¯, ä¿®æ”¹åçš„å†…å®¹]
# - ä¸æ˜¯çœŸæ­£çš„"åˆ†å‰"ï¼Œè€Œæ˜¯"è¿½åŠ "
```

#### å®é™…ç¤ºä¾‹ï¼šA/B æµ‹è¯•

```python
def ab_test_prompts(graph, base_checkpoint):
    """A/B æµ‹è¯•ä¸åŒçš„ç”¨æˆ·è¾“å…¥"""
    
    # è·å–åŸå§‹æ¶ˆæ¯
    original_msg = base_checkpoint.values['messages'][0]
    original_id = original_msg.id
    
    # ç‰ˆæœ¬ Aï¼šåŸå§‹æç¤º
    print("=== ç‰ˆæœ¬ Aï¼ˆåŸå§‹ï¼‰ ===")
    for event in graph.stream(None, base_checkpoint.config):
        print(event)
    
    # ç‰ˆæœ¬ Bï¼šä¿®æ”¹æç¤º
    print("\n=== ç‰ˆæœ¬ Bï¼ˆä¿®æ”¹ï¼‰ ===")
    variant_b = HumanMessage(
        content="è¯·ç”¨ç®€æ´çš„æ–¹å¼å›ç­”ï¼š" + original_msg.content,
        id=original_id  # ä¿ç•™ ID ä»¥è¦†ç›–
    )
    
    fork_b = graph.update_state(
        base_checkpoint.config,
        {"messages": [variant_b]}
    )
    
    for event in graph.stream(None, fork_b):
        print(event)
    
    # ç‰ˆæœ¬ Cï¼šå®Œå…¨ä¸åŒçš„é—®é¢˜
    print("\n=== ç‰ˆæœ¬ Cï¼ˆæ›¿ä»£ï¼‰ ===")
    variant_c = HumanMessage(
        content="æ¢ä¸ªé—®é¢˜ï¼šè§£é‡Šä»€ä¹ˆæ˜¯ Time Travel",
        id=original_id  # ä¿ç•™ ID ä»¥è¦†ç›–
    )
    
    fork_c = graph.update_state(
        base_checkpoint.config,
        {"messages": [variant_c]}
    )
    
    for event in graph.stream(None, fork_c):
        print(event)
```

#### Message ID ç”Ÿæˆè§„åˆ™

```python
from langchain_core.messages import HumanMessage

# è‡ªåŠ¨ç”Ÿæˆ IDï¼ˆUUIDï¼‰
msg1 = HumanMessage(content="Hello")
print(msg1.id)  # è¾“å‡º: run-abc123...

# æ‰‹åŠ¨æŒ‡å®š ID
msg2 = HumanMessage(content="Hello", id="custom_id_001")
print(msg2.id)  # è¾“å‡º: custom_id_001

# å¤åˆ¶æ¶ˆæ¯æ—¶ä¿ç•™ ID
msg3 = msg1.copy()
print(msg3.id == msg1.id)  # True

# ä¿®æ”¹å†…å®¹æ—¶å¯ä»¥é€‰æ‹©ä¿ç•™æˆ–æ›´æ¢ ID
msg4 = HumanMessage(content="New content", id=msg1.id)
print(msg4.id == msg1.id)  # True
```

#### é«˜çº§æŠ€å·§ï¼šæ‰¹é‡æ¶ˆæ¯æ›´æ–°

```python
def batch_update_messages(graph, thread, updates):
    """æ‰¹é‡æ›´æ–°å¤šæ¡æ¶ˆæ¯"""
    
    state = graph.get_state(thread)
    messages = state.values['messages']
    
    # åˆ›å»ºæ›´æ–°æ˜ å°„
    updates_map = {update.id: update for update in updates}
    
    # åº”ç”¨æ›´æ–°
    updated_messages = []
    for msg in messages:
        if msg.id in updates_map:
            updated_messages.append(updates_map[msg.id])
        else:
            updated_messages.append(msg)
    
    # æ›´æ–°çŠ¶æ€
    graph.update_state(thread, {"messages": updated_messages})

# ä½¿ç”¨
updates = [
    HumanMessage(content="ä¿®æ”¹åçš„æ¶ˆæ¯ 1", id="msg_001"),
    HumanMessage(content="ä¿®æ”¹åçš„æ¶ˆæ¯ 2", id="msg_003")
]

batch_update_messages(graph, thread, updates)
```

#### è°ƒè¯•æŠ€å·§ï¼šè¿½è¸ª Message ID

```python
def trace_message_ids(state):
    """è¿½è¸ªæ¶ˆæ¯ ID å˜åŒ–"""
    messages = state.values.get('messages', [])
    
    print("\nğŸ“‹ æ¶ˆæ¯ ID è¿½è¸ª:")
    for i, msg in enumerate(messages):
        msg_type = type(msg).__name__
        content_preview = msg.content[:30] if hasattr(msg, 'content') else str(msg)[:30]
        print(f"  [{i}] {msg_type} | ID: {msg.id[:8]}... | {content_preview}...")

# åœ¨çŠ¶æ€æ›´æ–°å‰åè°ƒç”¨
print("æ›´æ–°å‰:")
trace_message_ids(state_before)

print("\næ›´æ–°å:")
trace_message_ids(state_after)
```

</details>

---

### é—®é¢˜ 9: ä¼˜åŒ– Streaming æ€§èƒ½ä»¥æ”¯æŒå¤§è§„æ¨¡å¹¶å‘

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ€§èƒ½ç“¶é¢ˆåˆ†æ

| ç“¶é¢ˆç‚¹ | å½±å“ | ä¼˜åŒ–æ–¹å‘ |
|--------|------|---------|
| äº‹ä»¶è¿‡æ»¤å¼€é”€ | CPU å¯†é›† | æå‰ç¼–è¯‘è¿‡æ»¤æ¡ä»¶ |
| é¢‘ç¹ I/O è¾“å‡º | ç½‘ç»œå»¶è¿Ÿ | æ‰¹é‡ç¼“å†²è¾“å‡º |
| å¤§é‡å¹¶å‘è¿æ¥ | å†…å­˜/è¿æ¥æ•° | è¿æ¥æ± ã€é™æµ |
| Token é€ä¸ªä¼ è¾“ | ç½‘ç»œå¾€è¿” | æ‰¹é‡ä¼ è¾“ |

#### ä¼˜åŒ–æ–¹æ¡ˆ 1ï¼šäº‹ä»¶è¿‡æ»¤ä¼˜åŒ–

```python
# âŒ ä½æ•ˆï¼šæ¯æ¬¡éƒ½è¿›è¡Œå¤æ‚æ¡ä»¶åˆ¤æ–­
async for event in graph.astream_events(...):
    if (event.get("event") == "on_chat_model_stream" and
        event.get("metadata", {}).get("langgraph_node") == "assistant" and
        "data" in event and
        event["data"].get("chunk") is not None):
        token = event["data"]["chunk"].content
        if token and len(token) > 0:
            print(token, end="")

# âœ… é«˜æ•ˆï¼šæå‰å‡†å¤‡è¿‡æ»¤æ¡ä»¶
TARGET_EVENT = "on_chat_model_stream"
TARGET_NODE = "assistant"

async for event in graph.astream_events(...):
    # å¿«é€Ÿæ£€æŸ¥äº‹ä»¶ç±»å‹
    if event.get("event") != TARGET_EVENT:
        continue
    
    # å¿«é€Ÿæ£€æŸ¥èŠ‚ç‚¹
    if event.get("metadata", {}).get("langgraph_node") != TARGET_NODE:
        continue
    
    # æå– token
    chunk = event.get("data", {}).get("chunk")
    if chunk and chunk.content:
        print(chunk.content, end="", flush=True)
```

#### ä¼˜åŒ–æ–¹æ¡ˆ 2ï¼šæ‰¹é‡ç¼“å†²è¾“å‡º

```python
class TokenBuffer:
    """Token ç¼“å†²å™¨"""
    
    def __init__(self, buffer_size=5, flush_interval=0.1):
        self.buffer_size = buffer_size
        self.flush_interval = flush_interval
        self.buffer = []
        self.last_flush = time.time()
    
    def add(self, token):
        """æ·»åŠ  Token"""
        self.buffer.append(token)
        
        # è¾¾åˆ°ç¼“å†²åŒºå¤§å°æˆ–è¶…æ—¶åˆ™åˆ·æ–°
        if (len(self.buffer) >= self.buffer_size or
            time.time() - self.last_flush >= self.flush_interval):
            return self.flush()
        
        return None
    
    def flush(self):
        """åˆ·æ–°ç¼“å†²åŒº"""
        if not self.buffer:
            return None
        
        batch = "".join(self.buffer)
        self.buffer.clear()
        self.last_flush = time.time()
        return batch

# ä½¿ç”¨
async def buffered_stream(graph, input_data, config):
    """å¸¦ç¼“å†²çš„æµå¼è¾“å‡º"""
    buffer = TokenBuffer(buffer_size=5)
    
    async for event in graph.astream_events(input_data, config, version="v2"):
        if event["event"] == "on_chat_model_stream":
            token = event["data"]["chunk"].content
            
            if token:
                batch = buffer.add(token)
                if batch:
                    print(batch, end="", flush=True)
    
    # åˆ·æ–°å‰©ä½™ Token
    final_batch = buffer.flush()
    if final_batch:
        print(final_batch, end="", flush=True)
```

#### ä¼˜åŒ–æ–¹æ¡ˆ 3ï¼šè¿æ¥æ± ç®¡ç†

```python
import asyncio
from asyncio import Semaphore

class ConcurrentStreamManager:
    """å¹¶å‘æµç®¡ç†å™¨"""
    
    def __init__(self, max_concurrent=100):
        self.semaphore = Semaphore(max_concurrent)
        self.active_streams = {}
    
    async def stream_for_user(self, user_id, graph, input_data, config):
        """ä¸ºç‰¹å®šç”¨æˆ·æµå¼è¾“å‡ºï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
        
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            try:
                self.active_streams[user_id] = {
                    "status": "streaming",
                    "started_at": time.time()
                }
                
                async for event in graph.astream_events(input_data, config, version="v2"):
                    if event["event"] == "on_chat_model_stream":
                        token = event["data"]["chunk"].content
                        if token:
                            yield token
                
                self.active_streams[user_id]["status"] = "completed"
                
            except Exception as e:
                self.active_streams[user_id]["status"] = "error"
                self.active_streams[user_id]["error"] = str(e)
                raise
            
            finally:
                # æ¸…ç†
                if user_id in self.active_streams:
                    del self.active_streams[user_id]

# ä½¿ç”¨
manager = ConcurrentStreamManager(max_concurrent=50)

async def handle_multiple_users():
    """å¤„ç†å¤šç”¨æˆ·å¹¶å‘"""
    tasks = []
    
    for user_id in range(100):
        task = asyncio.create_task(
            process_user_stream(manager, user_id)
        )
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œï¼ˆæœ€å¤š 50 ä¸ªåŒæ—¶è¿›è¡Œï¼‰
    await asyncio.gather(*tasks)

async def process_user_stream(manager, user_id):
    """å¤„ç†å•ä¸ªç”¨æˆ·çš„æµ"""
    async for token in manager.stream_for_user(
        user_id,
        graph,
        input_data,
        config
    ):
        # å¤„ç† token
        await send_to_user(user_id, token)
```

#### ä¼˜åŒ–æ–¹æ¡ˆ 4ï¼šä½¿ç”¨ LangGraph APIï¼ˆæ¨èï¼‰

```python
from langgraph_sdk import get_client

class ProductionStreamService:
    """ç”Ÿäº§çº§æµå¼æœåŠ¡"""
    
    def __init__(self, api_url="http://localhost:2024"):
        self.client = get_client(url=api_url)
        self.connection_pool = {}
    
    async def stream_chat(self, user_id, message, thread_id):
        """æµå¼èŠå¤©ï¼ˆä½¿ç”¨ messages æ¨¡å¼ï¼‰"""
        
        try:
            full_response = ""
            
            async for event in self.client.runs.stream(
                thread_id,
                assistant_id="chatbot",
                input={"messages": [{"role": "user", "content": message}]},
                stream_mode="messages"  # API ä¼˜åŒ–çš„æ¨¡å¼
            ):
                if event.event == "messages/partial":
                    for item in event.data:
                        if "content" in item:
                            content = item["content"]
                            
                            # è®¡ç®—å¢é‡
                            if len(content) > len(full_response):
                                new_tokens = content[len(full_response):]
                                full_response = content
                                
                                # å‘é€å¢é‡
                                yield new_tokens
            
        except asyncio.CancelledError:
            # ç”¨æˆ·å–æ¶ˆ
            await self.cleanup_stream(thread_id)
            raise
    
    async def cleanup_stream(self, thread_id):
        """æ¸…ç†æµèµ„æº"""
        # å–æ¶ˆè¿è¡Œä¸­çš„è¯·æ±‚
        pass

# FastAPI é›†æˆ
from fastapi import FastAPI, WebSocket

app = FastAPI()
service = ProductionStreamService()

@app.websocket("/ws/chat/{user_id}")
async def websocket_chat(websocket: WebSocket, user_id: str):
    """WebSocket æµå¼èŠå¤©"""
    await websocket.accept()
    
    try:
        while True:
            # æ¥æ”¶ç”¨æˆ·æ¶ˆæ¯
            message = await websocket.receive_text()
            
            # æµå¼å“åº”
            async for token in service.stream_chat(
                user_id,
                message,
                thread_id=f"user_{user_id}"
            ):
                await websocket.send_text(token)
            
            # å‘é€å®Œæˆæ ‡è®°
            await websocket.send_text("[DONE]")
            
    except WebSocketDisconnect:
        print(f"User {user_id} disconnected")
```

#### ä¼˜åŒ–æ–¹æ¡ˆ 5ï¼šç¼“å­˜å’Œé¢„åŠ è½½

```python
from functools import lru_cache

class CachedStreamService:
    """å¸¦ç¼“å­˜çš„æµå¼æœåŠ¡"""
    
    def __init__(self):
        self.response_cache = {}
    
    @lru_cache(maxsize=1000)
    def get_cached_response(self, message_hash):
        """è·å–ç¼“å­˜çš„å“åº”"""
        return self.response_cache.get(message_hash)
    
    async def stream_with_cache(self, message):
        """å¸¦ç¼“å­˜çš„æµå¼è¾“å‡º"""
        message_hash = hash(message)
        
        # æ£€æŸ¥ç¼“å­˜
        cached = self.get_cached_response(message_hash)
        if cached:
            # æ¨¡æ‹Ÿæµå¼è¾“å‡ºç¼“å­˜çš„å“åº”
            for token in cached.split():
                yield token + " "
                await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
            return
        
        # ç”Ÿæˆæ–°å“åº”
        full_response = ""
        async for event in graph.astream_events(...):
            if event["event"] == "on_chat_model_stream":
                token = event["data"]["chunk"].content
                if token:
                    full_response += token
                    yield token
        
        # ç¼“å­˜å“åº”
        self.response_cache[message_hash] = full_response
```

#### æ€§èƒ½ç›‘æ§

```python
import time
from dataclasses import dataclass

@dataclass
class StreamMetrics:
    """æµå¼è¾“å‡ºæ€§èƒ½æŒ‡æ ‡"""
    user_id: str
    start_time: float
    end_time: float = None
    token_count: int = 0
    total_bytes: int = 0
    
    @property
    def duration(self):
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time
    
    @property
    def tokens_per_second(self):
        duration = self.duration
        return self.token_count / duration if duration > 0 else 0

class MonitoredStreamService:
    """å¸¦ç›‘æ§çš„æµå¼æœåŠ¡"""
    
    def __init__(self):
        self.metrics = []
    
    async def monitored_stream(self, user_id, graph, input_data, config):
        """å¸¦æ€§èƒ½ç›‘æ§çš„æµå¼è¾“å‡º"""
        metric = StreamMetrics(user_id=user_id, start_time=time.time())
        
        try:
            async for event in graph.astream_events(input_data, config, version="v2"):
                if event["event"] == "on_chat_model_stream":
                    token = event["data"]["chunk"].content
                    
                    if token:
                        metric.token_count += 1
                        metric.total_bytes += len(token.encode('utf-8'))
                        yield token
        
        finally:
            metric.end_time = time.time()
            self.metrics.append(metric)
            
            # è®°å½•æ€§èƒ½
            print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"   ç”¨æˆ·: {metric.user_id}")
            print(f"   è€—æ—¶: {metric.duration:.2f}s")
            print(f"   Token æ•°: {metric.token_count}")
            print(f"   é€Ÿåº¦: {metric.tokens_per_second:.1f} tokens/s")
```

</details>

---

### é—®é¢˜ 10: è®¾è®¡å®Œæ•´çš„ Human-in-the-Loop ç³»ç»Ÿæ¶æ„

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     å®¢æˆ·ç«¯å±‚ï¼ˆClient Layerï¼‰                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Web UI   â”‚  â”‚ Mobile   â”‚  â”‚  API     â”‚  â”‚ CLI Tool â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   API ç½‘å…³ï¼ˆRate Limitingï¼‰      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æœåŠ¡å±‚ï¼ˆService Layerï¼‰                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Stream Managerï¼ˆæµç®¡ç†ï¼‰                      â”‚    â”‚
â”‚  â”‚  - å¹¶å‘æ§åˆ¶ï¼ˆSemaphoreï¼‰                                   â”‚    â”‚
â”‚  â”‚  - è¿æ¥æ± ç®¡ç†                                              â”‚    â”‚
â”‚  â”‚  - ç¼“å†²ä¼˜åŒ–                                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          Approval Managerï¼ˆå®¡æ‰¹ç®¡ç†ï¼‰                       â”‚    â”‚
â”‚  â”‚  - é™æ€æ–­ç‚¹ (interrupt_before)                            â”‚    â”‚
â”‚  â”‚  - åŠ¨æ€æ–­ç‚¹ (NodeInterrupt)                               â”‚    â”‚
â”‚  â”‚  - å®¡æ‰¹é˜Ÿåˆ—                                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          State Managerï¼ˆçŠ¶æ€ç®¡ç†ï¼‰                          â”‚    â”‚
â”‚  â”‚  - update_state                                          â”‚    â”‚
â”‚  â”‚  - get_state_history                                     â”‚    â”‚
â”‚  â”‚  - åˆ†å‰ç®¡ç†                                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LangGraph å±‚ï¼ˆGraph Layerï¼‰                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Assistant â”‚  â”‚  Tools   â”‚  â”‚  Router  â”‚  â”‚ Feedback â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 æŒä¹…åŒ–å±‚ï¼ˆPersistence Layerï¼‰                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Checkpointer  â”‚  â”‚  Audit Logger  â”‚  â”‚  Metrics DB    â”‚   â”‚
â”‚  â”‚  (PostgreSQL)  â”‚  â”‚  (Elasticsearch)â”‚  â”‚  (Prometheus)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å®Œæ•´å®ç°

```python
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import asyncio
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.errors import NodeInterrupt

@dataclass
class ApprovalRequest:
    """å®¡æ‰¹è¯·æ±‚"""
    request_id: str
    thread_id: str
    tool_name: str
    tool_args: dict
    risk_level: str  # "low", "medium", "high"
    created_at: datetime
    status: str  # "pending", "approved", "rejected"

class HumanInTheLoopSystem:
    """å®Œæ•´çš„ Human-in-the-Loop ç³»ç»Ÿ"""
    
    def __init__(
        self,
        postgres_connection: str,
        max_concurrent_streams: int = 100,
        buffer_size: int = 5
    ):
        # ç»„ä»¶åˆå§‹åŒ–
        self.checkpointer = PostgresSaver(connection_string=postgres_connection)
        self.stream_semaphore = asyncio.Semaphore(max_concurrent_streams)
        self.buffer_size = buffer_size
        
        # å®¡æ‰¹é˜Ÿåˆ—
        self.approval_queue: Dict[str, ApprovalRequest] = {}
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            "total_requests": 0,
            "approved": 0,
            "rejected": 0,
            "auto_approved": 0
        }
        
        # æ„å»ºå›¾
        self.graph = self._build_graph()
    
    def _build_graph(self):
        """æ„å»º LangGraph"""
        builder = StateGraph(MessagesState)
        
        # æ·»åŠ èŠ‚ç‚¹
        builder.add_node("assistant", self._assistant_node)
        builder.add_node("risk_assessment", self._risk_assessment)
        builder.add_node("approval_gate", self._approval_gate)
        builder.add_node("tools", self._tools_node)
        
        # æ·»åŠ è¾¹
        builder.add_edge(START, "assistant")
        builder.add_edge("assistant", "risk_assessment")
        builder.add_conditional_edges(
            "risk_assessment",
            self._route_after_risk,
            {
                "auto_approve": "tools",
                "need_approval": "approval_gate"
            }
        )
        builder.add_edge("approval_gate", "tools")
        builder.add_edge("tools", "assistant")
        
        # ç¼–è¯‘
        return builder.compile(
            interrupt_before=["approval_gate"],
            checkpointer=self.checkpointer
        )
    
    def _assistant_node(self, state):
        """åŠ©æ‰‹èŠ‚ç‚¹"""
        # LLM è°ƒç”¨
        pass
    
    def _risk_assessment(self, state):
        """é£é™©è¯„ä¼°èŠ‚ç‚¹"""
        last_message = state["messages"][-1]
        
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            tool_call = last_message.tool_calls[0]
            
            # è¯„ä¼°é£é™©
            risk_level = self._calculate_risk(tool_call)
            
            return {
                "risk_level": risk_level,
                "pending_tool": tool_call
            }
        
        return {"risk_level": "none"}
    
    def _calculate_risk(self, tool_call) -> str:
        """è®¡ç®—é£é™©ç­‰çº§"""
        high_risk_tools = ["delete", "payment", "refund"]
        medium_risk_tools = ["update", "modify"]
        
        if tool_call['name'] in high_risk_tools:
            return "high"
        elif tool_call['name'] in medium_risk_tools:
            return "medium"
        else:
            return "low"
    
    def _route_after_risk(self, state):
        """é£é™©è¯„ä¼°åè·¯ç”±"""
        risk = state.get("risk_level", "low")
        
        if risk in ["high", "medium"]:
            return "need_approval"
        else:
            self.metrics["auto_approved"] += 1
            return "auto_approve"
    
    def _approval_gate(self, state):
        """å®¡æ‰¹é—¨ï¼ˆç©ºèŠ‚ç‚¹ï¼Œç”¨äºæ–­ç‚¹ï¼‰"""
        pass
    
    def _tools_node(self, state):
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        # æ‰§è¡Œå·¥å…·
        pass
    
    async def stream_with_hitl(
        self,
        user_id: str,
        message: str,
        thread_id: str
    ):
        """å¸¦ HITL çš„æµå¼æ‰§è¡Œ"""
        
        async with self.stream_semaphore:  # å¹¶å‘æ§åˆ¶
            self.metrics["total_requests"] += 1
            
            # é…ç½®
            config = {"configurable": {"thread_id": thread_id}}
            input_data = {"messages": [{"role": "user", "content": message}]}
            
            # ç¼“å†²å™¨
            buffer = []
            
            try:
                async for event in self.graph.astream_events(
                    input_data,
                    config,
                    version="v2"
                ):
                    # Token æµ
                    if event["event"] == "on_chat_model_stream":
                        token = event["data"]["chunk"].content
                        
                        if token:
                            buffer.append(token)
                            
                            # æ‰¹é‡è¾“å‡º
                            if len(buffer) >= self.buffer_size:
                                yield "".join(buffer)
                                buffer.clear()
                
                # åˆ·æ–°å‰©ä½™
                if buffer:
                    yield "".join(buffer)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å®¡æ‰¹
                state = self.graph.get_state(config)
                
                if state.tasks and state.tasks[0].interrupts:
                    # åˆ›å»ºå®¡æ‰¹è¯·æ±‚
                    request = self._create_approval_request(
                        thread_id,
                        state
                    )
                    
                    # ç­‰å¾…å®¡æ‰¹
                    yield f"\n\nâ¸ï¸  ç­‰å¾…å®¡æ‰¹ï¼ˆè¯·æ±‚ ID: {request.request_id}ï¼‰"
                    
            except Exception as e:
                yield f"\n\nâŒ é”™è¯¯: {e}"
    
    def _create_approval_request(
        self,
        thread_id: str,
        state
    ) -> ApprovalRequest:
        """åˆ›å»ºå®¡æ‰¹è¯·æ±‚"""
        import uuid
        
        pending_tool = state.values.get('pending_tool')
        risk_level = state.values.get('risk_level', 'medium')
        
        request = ApprovalRequest(
            request_id=str(uuid.uuid4()),
            thread_id=thread_id,
            tool_name=pending_tool['name'],
            tool_args=pending_tool['args'],
            risk_level=risk_level,
            created_at=datetime.now(),
            status="pending"
        )
        
        self.approval_queue[request.request_id] = request
        return request
    
    async def approve_request(
        self,
        request_id: str,
        approved: bool,
        modified_args: Optional[dict] = None
    ):
        """å®¡æ‰¹è¯·æ±‚"""
        request = self.approval_queue.get(request_id)
        
        if not request:
            raise ValueError(f"Request {request_id} not found")
        
        if approved:
            request.status = "approved"
            self.metrics["approved"] += 1
            
            # æ›´æ–°çŠ¶æ€
            config = {"configurable": {"thread_id": request.thread_id}}
            
            if modified_args:
                self.graph.update_state(
                    config,
                    {"modified_params": modified_args},
                    as_node="approval_gate"
                )
            
            # ç»§ç»­æ‰§è¡Œ
            result = []
            async for event in self.graph.astream_events(None, config, version="v2"):
                if event["event"] == "on_chat_model_stream":
                    token = event["data"]["chunk"].content
                    if token:
                        result.append(token)
            
            return "".join(result)
        
        else:
            request.status = "rejected"
            self.metrics["rejected"] += 1
            return "æ“ä½œå·²æ‹’ç»"
    
    def get_metrics(self) -> dict:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        return {
            **self.metrics,
            "approval_rate": self.metrics["approved"] / max(self.metrics["total_requests"], 1),
            "pending_approvals": len([r for r in self.approval_queue.values() if r.status == "pending"])
        }

# FastAPI é›†æˆç¤ºä¾‹
from fastapi import FastAPI, WebSocket, HTTPException

app = FastAPI()
hitl_system = HumanInTheLoopSystem(
    postgres_connection="postgresql://user:pass@localhost/db",
    max_concurrent_streams=100
)

@app.websocket("/ws/chat/{user_id}")
async def websocket_chat(websocket: WebSocket, user_id: str):
    """WebSocket èŠå¤©ç«¯ç‚¹"""
    await websocket.accept()
    
    try:
        while True:
            message = await websocket.receive_text()
            
            async for token in hitl_system.stream_with_hitl(
                user_id,
                message,
                thread_id=f"user_{user_id}"
            ):
                await websocket.send_text(token)
    
    except WebSocketDisconnect:
        pass

@app.post("/approve/{request_id}")
async def approve_request(
    request_id: str,
    approved: bool,
    modified_args: Optional[dict] = None
):
    """å®¡æ‰¹ç«¯ç‚¹"""
    try:
        result = await hitl_system.approve_request(
            request_id,
            approved,
            modified_args
        )
        return {"status": "success", "result": result}
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """è·å–ç³»ç»ŸæŒ‡æ ‡"""
    return hitl_system.get_metrics()
```

</details>

---

## ğŸ‰ å¤ä¹ å®Œæˆï¼

### çŸ¥è¯†æŒæ¡åº¦è‡ªæµ‹

å®Œæˆä»¥ä¸Š 10 é“é—®é¢˜åï¼Œè¯·è¯„ä¼°ä½ çš„æŒæ¡ç¨‹åº¦ï¼š

- [ ] **Breakpoints**ï¼šèƒ½ç‹¬ç«‹å®ç°å·¥å…·è°ƒç”¨å®¡æ‰¹æµç¨‹
- [ ] **Dynamic Breakpoints**ï¼šèƒ½æ ¹æ®ä¸šåŠ¡é€»è¾‘æ¡ä»¶æ€§ä¸­æ–­
- [ ] **Edit State**ï¼šç†è§£ `as_node` å’Œ Message ID æœºåˆ¶
- [ ] **Streaming**ï¼šèƒ½å®ç°ç”Ÿäº§çº§çš„ Token æµå¼è¾“å‡º
- [ ] **Time Travel**ï¼šæŒæ¡å†å²æµè§ˆã€å›æ”¾å’Œåˆ†å‰æŠ€æœ¯

### ä¸‹ä¸€æ­¥å­¦ä¹ è·¯å¾„

æ ¹æ®ä½ çš„æŒæ¡æƒ…å†µï¼Œé€‰æ‹©ä¸‹ä¸€æ­¥æ–¹å‘ï¼š

**å·©å›ºåŸºç¡€ï¼ˆæŒæ¡åº¦ < 70%ï¼‰ï¼š**
- é‡æ–°å­¦ä¹ æœªæŒæ¡çš„ç« èŠ‚
- å®Œæˆæ¯èŠ‚è¯¾çš„å®æˆ˜ç»ƒä¹ 
- æ„å»ºå°å‹ç¤ºä¾‹é¡¹ç›®éªŒè¯ç†è§£

**è¿›é˜¶å­¦ä¹ ï¼ˆæŒæ¡åº¦ 70%-90%ï¼‰ï¼š**
- å­¦ä¹  Module-5ï¼šé«˜çº§å›¾æ¨¡å¼ï¼ˆå¹¶è¡Œã€å­å›¾ã€åŠ¨æ€è·¯ç”±ï¼‰
- å­¦ä¹  Module-6ï¼šç”Ÿäº§éƒ¨ç½²ä¸ä¼˜åŒ–
- æ„å»ºä¸­å‹ç»¼åˆé¡¹ç›®

**ä¸“å®¶è·¯å¾„ï¼ˆæŒæ¡åº¦ > 90%ï¼‰ï¼š**
- å­¦ä¹  Module-7ï¼šå¤§å‹å®æˆ˜é¡¹ç›®
- ç ”ç©¶ LangGraph æºç 
- è´¡çŒ®å¼€æºé¡¹ç›®æˆ–æ’°å†™æŠ€æœ¯åšå®¢

### å®è·µå»ºè®®

**é¡¹ç›® 1ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿï¼ˆä¸­çº§ï¼‰**
- éœ€æ±‚ï¼šæ”¯æŒè®¢å•æŸ¥è¯¢ã€é€€æ¬¾å®¡æ‰¹ã€æŠ•è¯‰å¤„ç†
- æŠ€æœ¯ï¼šBreakpoints + Edit State + Streaming
- é¢„è®¡æ—¶é—´ï¼š3-5 å¤©

**é¡¹ç›® 2ï¼šå†…å®¹å®¡æ ¸å¹³å°ï¼ˆé«˜çº§ï¼‰**
- éœ€æ±‚ï¼šå¤šç»´åº¦é£é™©è¯„ä¼°ã€æ¡ä»¶æ€§å®¡æ‰¹ã€å®¡æ‰¹æ—¥å¿—
- æŠ€æœ¯ï¼šDynamic Breakpoints + å®Œæ•´å®¡è®¡ç³»ç»Ÿ
- é¢„è®¡æ—¶é—´ï¼š5-7 å¤©

**é¡¹ç›® 3ï¼šAI å†³ç­–è°ƒè¯•å·¥å…·ï¼ˆä¸“å®¶çº§ï¼‰**
- éœ€æ±‚ï¼šå†å²æµè§ˆã€å›æ”¾ã€A/B æµ‹è¯•ã€æ€§èƒ½ç›‘æ§
- æŠ€æœ¯ï¼šTime Travel + æ‰€æœ‰ HITL æŠ€æœ¯
- é¢„è®¡æ—¶é—´ï¼š7-10 å¤©

### å‚è€ƒèµ„æº

**å®˜æ–¹æ–‡æ¡£ï¼š**
- [LangGraph Human-in-the-Loop æŒ‡å—](https://langchain-ai.github.io/langgraph/how-tos/#human-in-the-loop)
- [LangGraph Streaming æ–‡æ¡£](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming)
- [LangGraph API å‚è€ƒ](https://langchain-ai.github.io/langgraph/cloud/)

**ç¤¾åŒºèµ„æºï¼š**
- [LangGraph GitHub](https://github.com/langchain-ai/langgraph)
- [LangChain ä¸­æ–‡ç¤¾åŒº](https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide)

**è¿›é˜¶é˜…è¯»ï¼š**
- Python Async/Await æ·±å…¥æ•™ç¨‹
- WebSocket å®æ—¶é€šä¿¡
- PostgreSQL æ€§èƒ½ä¼˜åŒ–

---

**æ­å–œä½ å®Œæˆ Module-4 çš„å¤ä¹ ï¼** ğŸŠ

ä½ ç°åœ¨å·²ç»æŒæ¡äº†æ„å»ºå¯æ§ã€å¯ä¿¡ã€å¯é çš„ Human-in-the-Loop AI ç³»ç»Ÿçš„å…¨éƒ¨æŠ€èƒ½ã€‚è¿™äº›æŠ€æœ¯ä¸ä»…æ˜¯å·¥å…·ï¼Œæ›´æ˜¯ä¸€å¥—å®Œæ•´çš„"å¯ä¿¡ AI"è®¾è®¡å“²å­¦ã€‚åœ¨æœªæ¥çš„ AI åº”ç”¨å¼€å‘ä¸­ï¼ŒHuman-in-the-Loop å°†æˆä¸ºåŒºåˆ†ä¼˜ç§€ç³»ç»Ÿå’Œå“è¶Šç³»ç»Ÿçš„å…³é”®æ ‡å¿—ã€‚

ç»§ç»­åŠ æ²¹ï¼Œå‘ç€ç²¾é€š LangGraph çš„ç›®æ ‡å‰è¿›ï¼ ğŸš€
