# Module-5 å°ç»“å’Œå¤ä¹ ï¼šé«˜çº§å›¾æ¨¡å¼ç²¾é€šæŒ‡å—

> **æ¥è‡ªå›¾çµå¥–è·å¾—è€…çš„æ€»ç»“å¯„è¯­**
>
> "å½“ä½ å®Œæˆæœ¬ç« å­¦ä¹ ,ä½ å·²ç»æŒæ¡äº†æ„å»ºå¤æ‚ç³»ç»Ÿçš„å…³é”®æŠ€èƒ½ã€‚è®°ä½:ä¼˜ç§€çš„æ¶æ„å¸ˆä¸æ˜¯é€šè¿‡å¢åŠ å¤æ‚æ€§æ¥è§£å†³é—®é¢˜,è€Œæ˜¯é€šè¿‡æ­£ç¡®çš„æŠ½è±¡æ¥ç®€åŒ–å¤æ‚æ€§ã€‚ä½ ç°åœ¨æ‹¥æœ‰çš„å¹¶è¡ŒåŒ–ã€æ¨¡å—åŒ–ã€åˆ†æ²»ç­–ç•¥ç­‰å·¥å…·,æ­£æ˜¯å°†å¤æ‚ AI ç³»ç»Ÿå˜å¾—å¯ç®¡ç†ã€å¯æ‰©å±•çš„æ ¸å¿ƒæ­¦å™¨ã€‚åœ¨æœªæ¥çš„å·¥ä½œä¸­,å½“é¢å¯¹çœ‹ä¼¼æ— è§£çš„å¤æ‚éœ€æ±‚æ—¶,å›åˆ°è¿™äº›åŸºç¡€æ¨¡å¼,ä½ ä¼šå‘ç°é—®é¢˜å…¶å®æœ‰ç€ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆã€‚"
>
> â€” *å¯å‘è‡ª Tony Hoare å¯¹è½¯ä»¶å·¥ç¨‹æœ¬è´¨çš„æ´å¯Ÿ*

---

## ğŸ“‹ æœ¬ç« æ ¸å¿ƒçŸ¥è¯†å›é¡¾

### å­¦ä¹ åœ°å›¾

```mermaid
graph TB
    A[é«˜çº§å›¾æ¨¡å¼ä½“ç³»] --> B[Parallelization]
    A --> C[Sub-graph]
    A --> D[Map-Reduce]
    A --> E[Research Assistant]
    
    B --> B1[Fan-out/Fan-in]
    B --> B2[Reduceræœºåˆ¶]
    B --> B3[å¹¶è¡ŒåŒæ­¥]
    
    C --> C1[çŠ¶æ€éš”ç¦»]
    C --> C2[output_schema]
    C --> C3[å­å›¾åµŒå¥—]
    
    D --> D1[Send API]
    D --> D2[åŠ¨æ€åˆ†å‘]
    D --> D3[Map/Reduceé˜¶æ®µ]
    
    E --> E1[Human-in-the-Loopé›†æˆ]
    E --> E2[å¤šå­å›¾ååŒ]
    E --> E3[RAGæ¨¡å¼]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1f5
    style D fill:#f5e1ff
    style E fill:#e1ffe1
```

### å››å¤§æ ¸å¿ƒæŠ€æœ¯é€ŸæŸ¥è¡¨

| æŠ€æœ¯ | æ ¸å¿ƒAPI | ä¸»è¦ç”¨é€” | éš¾åº¦ |
|------|---------|---------|------|
| **Parallelization** | `Annotated[list, operator.add]` | å¹¶è¡Œæ‰§è¡Œæå‡æ€§èƒ½ | â­â­â­ |
| **Sub-graph** | `StateGraph(state, output_schema)` | æ¨¡å—åŒ–è®¾è®¡ | â­â­â­â­ |
| **Map-Reduce** | `Send("node", state)` | å¤§è§„æ¨¡ä»»åŠ¡åˆ†è§£ | â­â­â­â­â­ |
| **Research Assistant** | æ‰€æœ‰æ¨¡å¼é›†æˆ | ç”Ÿäº§çº§ç³»ç»Ÿæ¶æ„ | â­â­â­â­â­ |

---

## ğŸ¯ å¤ä¹ é¢˜ç›®åˆ—è¡¨

æœ¬ç« ç²¾å¿ƒè®¾è®¡äº† **10 é“ç»¼åˆæ€§é—®é¢˜**,æ¶µç›–æ‰€æœ‰æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚å»ºè®®æŒ‰é¡ºåºå®Œæˆ,æ¯é“é¢˜é¢„è®¡è€—æ—¶ 20-40 åˆ†é’Ÿã€‚

### åŸºç¡€ç†è§£ï¼ˆé—®é¢˜ 1-3ï¼‰
1. Reducer æœºåˆ¶çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆå¹¶è¡Œæ‰§è¡Œå¿…é¡»ä½¿ç”¨ Reducerï¼Ÿ
2. Sub-graph çš„ state_schema å’Œ output_schema æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
3. Send API ä¸ä¼ ç»Ÿçš„ add_edge æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ

### å®æˆ˜åº”ç”¨ï¼ˆé—®é¢˜ 4-7ï¼‰
4. å¦‚ä½•å®ç°ä¸€ä¸ªæ”¯æŒå¹¶è¡Œæ£€ç´¢å¤šä¸ªæ•°æ®æºçš„é—®ç­”ç³»ç»Ÿï¼Ÿ
5. å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ¨¡å—åŒ–çš„å¤šæ­¥éª¤å®¡æ‰¹æµç¨‹ï¼ˆä½¿ç”¨å­å›¾ï¼‰ï¼Ÿ
6. å¦‚ä½•ä½¿ç”¨ Map-Reduce å¤„ç†å¤§è§„æ¨¡æ–‡æ¡£æ‰¹é‡æ‘˜è¦ä»»åŠ¡ï¼Ÿ
7. å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ Research Assistant ç³»ç»Ÿ

### é«˜çº§ç»¼åˆï¼ˆé—®é¢˜ 8-10ï¼‰
8. å¦‚ä½•ä¼˜åŒ– Map-Reduce çš„æ€§èƒ½ä»¥æ”¯æŒæ•°åƒä¸ªå¹¶å‘ä»»åŠ¡ï¼Ÿ
9. å­å›¾åµŒå¥—çš„æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•é¿å…å¸¸è§é™·é˜±ï¼Ÿ
10. è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿæ¶æ„

---

## ğŸ“š è¯¦ç»†é—®ç­”è§£æ

### é—®é¢˜ 1: Reducer æœºåˆ¶çš„å·¥ä½œåŸç†

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒé—®é¢˜

**ä¸ºä»€ä¹ˆéœ€è¦ Reducerï¼Ÿ**

å½“å¤šä¸ªå¹¶è¡ŒèŠ‚ç‚¹è¯•å›¾åŒæ—¶æ›´æ–°çŠ¶æ€çš„åŒä¸€ä¸ªå­—æ®µæ—¶,LangGraph éœ€è¦çŸ¥é“å¦‚ä½•åˆå¹¶è¿™äº›æ›´æ–°ã€‚æ²¡æœ‰ Reducer,ç³»ç»Ÿä¼šæŠ›å‡º `InvalidUpdateError`ã€‚

#### Reducer å·¥ä½œæœºåˆ¶

**åŸºæœ¬åŸç†:**
```python
# å¹¶è¡ŒèŠ‚ç‚¹ B è¿”å›
update_b = {"results": ["result_from_B"]}

# å¹¶è¡ŒèŠ‚ç‚¹ C è¿”å›
update_c = {"results": ["result_from_C"]}

# Reducer å‡½æ•°è¢«è°ƒç”¨
def reducer(current_value, new_value):
    return current_value + new_value

# æœ€ç»ˆçŠ¶æ€
state["results"] = reducer(["result_from_B"], ["result_from_C"])
# ç»“æœ: ["result_from_B", "result_from_C"]
```

#### å®Œæ•´ç¤ºä¾‹ï¼šå¹¶è¡Œæœç´¢

```python
import operator
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

# çŠ¶æ€å®šä¹‰
class SearchState(TypedDict):
    query: str
    results: Annotated[list, operator.add]  # â­ Reducer

# èŠ‚ç‚¹å‡½æ•°
def search_wikipedia(state):
    query = state["query"]
    results = wikipedia_api.search(query)
    return {"results": [f"Wikipedia: {results}"]}

def search_web(state):
    query = state["query"]
    results = web_search_api.search(query)
    return {"results": [f"Web: {results}"]}

def search_database(state):
    query = state["query"]
    results = database.query(query)
    return {"results": [f"DB: {results}"]}

# æ„å»ºå›¾
builder = StateGraph(SearchState)
builder.add_node("search_wikipedia", search_wikipedia)
builder.add_node("search_web", search_web)
builder.add_node("search_database", search_database)

# å¹¶è¡Œæ‰§è¡Œ
builder.add_edge(START, "search_wikipedia")
builder.add_edge(START, "search_web")
builder.add_edge(START, "search_database")
builder.add_edge("search_wikipedia", END)
builder.add_edge("search_web", END)
builder.add_edge("search_database", END)

graph = builder.compile()

# æ‰§è¡Œ
result = graph.invoke({"query": "LangGraph"})
print(result["results"])
# è¾“å‡º: [
#   "Wikipedia: ...",
#   "Web: ...",
#   "DB: ..."
# ]
```

#### å¸¸ç”¨ Reducer ç±»å‹

**1. operator.add - åˆ—è¡¨æ‹¼æ¥**
```python
from operator import add

class State(TypedDict):
    items: Annotated[list, add]

# [1, 2] + [3, 4] = [1, 2, 3, 4]
```

**2. add_messages - æ¶ˆæ¯åˆå¹¶**
```python
from langgraph.graph import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

# ç‰¹æ®ŠåŠŸèƒ½:
# - ç›¸åŒ ID çš„æ¶ˆæ¯ä¼šè¢«è¦†ç›–
# - RemoveMessage ä¼šåˆ é™¤æ¶ˆæ¯
# - è‡ªåŠ¨å»é‡å’Œæ’åº
```

**3. è‡ªå®šä¹‰ Reducer - æ’åºåˆå¹¶**
```python
def sorted_merge(left, right):
    """æŒ‰ä¼˜å…ˆçº§æ’åºåˆå¹¶"""
    left = left if isinstance(left, list) else [left]
    right = right if isinstance(right, list) else [right]
    
    combined = left + right
    return sorted(combined, key=lambda x: x.get("priority", 0), reverse=True)

class State(TypedDict):
    tasks: Annotated[list, sorted_merge]
```

**4. è‡ªå®šä¹‰ Reducer - åªä¿ç•™æœ€æ–° N ä¸ª**
```python
def keep_last_n(n=5):
    def reducer(left, right):
        left = left if isinstance(left, list) else [left]
        right = right if isinstance(right, list) else [right]
        combined = left + right
        return combined[-n:]  # åªä¿ç•™æœ€å n ä¸ª
    return reducer

class State(TypedDict):
    history: Annotated[list, keep_last_n(10)]
```

**5. è‡ªå®šä¹‰ Reducer - å»é‡**
```python
def unique_merge(left, right):
    """å»é‡åˆå¹¶"""
    left = left if isinstance(left, list) else [left]
    right = right if isinstance(right, list) else [right]
    
    # ä½¿ç”¨å­—å…¸å»é‡,ä¿æŒé¡ºåº
    seen = {}
    for item in left + right:
        key = item.get("id", str(item))
        if key not in seen:
            seen[key] = item
    
    return list(seen.values())

class State(TypedDict):
    unique_results: Annotated[list, unique_merge]
```

#### æ‰§è¡Œé¡ºåºé—®é¢˜

**é—®é¢˜:** Reducer åˆå¹¶çš„é¡ºåºæ˜¯å¦ç¡®å®šï¼Ÿ

```python
# èŠ‚ç‚¹ B å’Œ C å¹¶è¡Œæ‰§è¡Œ
# å“ªä¸ªå…ˆå®Œæˆï¼Ÿé¡ºåºä¸ç¡®å®š

# å¦‚æœéœ€è¦ç¡®å®šé¡ºåº,ä½¿ç”¨è‡ªå®šä¹‰ Reducer
def ordered_merge(left, right):
    left = left if isinstance(left, list) else [left]
    right = right if isinstance(right, list) else [right]
    
    # æŒ‰æ—¶é—´æˆ³æˆ– ID æ’åº
    combined = left + right
    return sorted(combined, key=lambda x: x.get("timestamp"))
```

#### æœ€ä½³å®è·µ

**1. æ€»æ˜¯ä½¿ç”¨ Reducer å¤„ç†å¹¶è¡Œæ›´æ–°**
```python
# âŒ é”™è¯¯:æ²¡æœ‰ Reducer
class State(TypedDict):
    results: list  # å¹¶è¡Œæ›´æ–°ä¼šå¤±è´¥

# âœ… æ­£ç¡®:ä½¿ç”¨ Reducer
class State(TypedDict):
    results: Annotated[list, operator.add]
```

**2. é€‰æ‹©åˆé€‚çš„ Reducer**
```python
# ç®€å•è¿½åŠ  â†’ operator.add
# æ¶ˆæ¯ç®¡ç† â†’ add_messages
# å¤æ‚é€»è¾‘ â†’ è‡ªå®šä¹‰ Reducer
```

**3. Reducer åº”è¯¥æ˜¯å¹‚ç­‰çš„**
```python
# âœ… å¹‚ç­‰:å¤šæ¬¡è°ƒç”¨ç»“æœç›¸åŒ
def idempotent_reducer(left, right):
    # ä½¿ç”¨ ID å»é‡
    return list({item["id"]: item for item in left + right}.values())

# âŒ éå¹‚ç­‰:ä¾èµ–å¤–éƒ¨çŠ¶æ€
global_counter = 0
def non_idempotent_reducer(left, right):
    global global_counter
    global_counter += 1  # å‰¯ä½œç”¨
    return left + right
```

</details>

---

### é—®é¢˜ 2: state_schema å’Œ output_schema çš„åŒºåˆ«

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒæ¦‚å¿µ

**state_schema:** å­å›¾å†…éƒ¨ä½¿ç”¨çš„å®Œæ•´çŠ¶æ€  
**output_schema:** å­å›¾è¿”å›ç»™ä¸»å›¾çš„è¾“å‡ºçŠ¶æ€(é€šå¸¸æ˜¯ state_schema çš„å­é›†)

#### è¯¦ç»†å¯¹æ¯”

| ç»´åº¦ | state_schema | output_schema |
|------|--------------|---------------|
| **ä½œç”¨åŸŸ** | å­å›¾å†…éƒ¨ | å­å›¾ä¸ä¸»å›¾ä¹‹é—´ |
| **å­—æ®µæ•°é‡** | å®Œæ•´(åŒ…å«æ‰€æœ‰ä¸­é—´å˜é‡) | éƒ¨åˆ†(åªåŒ…å«éœ€è¦è¿”å›çš„) |
| **å¯è§æ€§** | åªåœ¨å­å›¾å†…å¯è§ | ä¸»å›¾å¯ä»¥è®¿é—® |
| **å¿…éœ€æ€§** | å¿…éœ€ | å¯é€‰(é»˜è®¤è¿”å›æ‰€æœ‰å­—æ®µ) |

#### ä¸ºä»€ä¹ˆéœ€è¦ output_schemaï¼Ÿ

**é—®é¢˜åœºæ™¯:**
```python
# å­å›¾æœ‰å¾ˆå¤šä¸­é—´å˜é‡
class SubGraphState(TypedDict):
    input: str
    temp1: str       # ä¸­é—´å˜é‡
    temp2: int       # ä¸­é—´å˜é‡
    temp3: list      # ä¸­é—´å˜é‡
    cache: dict      # ä¸­é—´å˜é‡
    debug_info: str  # ä¸­é—´å˜é‡
    result: str      # æœ€ç»ˆç»“æœ

# å¦‚æœæ²¡æœ‰ output_schema,æ‰€æœ‰å­—æ®µéƒ½ä¼šè¿”å›ä¸»å›¾
# ä¸»å›¾çŠ¶æ€ä¼šè¢«æ±¡æŸ“,å……æ»¡ä¸éœ€è¦çš„å­—æ®µ
```

**è§£å†³æ–¹æ¡ˆ:**
```python
# å®šä¹‰è¾“å‡ºçŠ¶æ€
class SubGraphOutput(TypedDict):
    result: str  # åªè¿”å›è¿™ä¸ª

# åˆ›å»ºå­å›¾
sub_graph = StateGraph(
    state_schema=SubGraphState,    # å†…éƒ¨ä½¿ç”¨å®Œæ•´çŠ¶æ€
    output_schema=SubGraphOutput   # åªè¿”å› result
)
```

#### å®Œæ•´ç¤ºä¾‹:æ—¥å¿—åˆ†æç³»ç»Ÿ

```python
from typing_extensions import TypedDict
from typing import List
from langgraph.graph import StateGraph, START, END

# ============ å­å›¾ 1: å¤±è´¥åˆ†æ ============

# å†…éƒ¨çŠ¶æ€(å®Œæ•´)
class FailureAnalysisState(TypedDict):
    logs: List[dict]          # è¾“å…¥
    failed_logs: List[dict]   # ä¸­é—´:ç­›é€‰å‡ºçš„å¤±è´¥æ—¥å¿—
    error_patterns: dict      # ä¸­é—´:é”™è¯¯æ¨¡å¼ç»Ÿè®¡
    temp_cache: dict          # ä¸­é—´:ä¸´æ—¶ç¼“å­˜
    summary: str              # è¾“å‡º:å¤±è´¥æ‘˜è¦

# è¾“å‡ºçŠ¶æ€(åªè¿”å›æ‘˜è¦)
class FailureAnalysisOutput(TypedDict):
    summary: str

# èŠ‚ç‚¹å‡½æ•°
def filter_failures(state):
    failed = [log for log in state["logs"] if log.get("status") == "error"]
    return {"failed_logs": failed}

def analyze_patterns(state):
    patterns = {}
    for log in state["failed_logs"]:
        error_type = log.get("error_type", "unknown")
        patterns[error_type] = patterns.get(error_type, 0) + 1
    return {"error_patterns": patterns}

def generate_summary(state):
    patterns = state["error_patterns"]
    summary = f"Found {sum(patterns.values())} failures. "
    summary += f"Top issues: {list(patterns.keys())}"
    return {"summary": summary}

# æ„å»ºå­å›¾
fa_builder = StateGraph(
    state_schema=FailureAnalysisState,
    output_schema=FailureAnalysisOutput  # â­ åªè¿”å› summary
)

fa_builder.add_node("filter_failures", filter_failures)
fa_builder.add_node("analyze_patterns", analyze_patterns)
fa_builder.add_node("generate_summary", generate_summary)

fa_builder.add_edge(START, "filter_failures")
fa_builder.add_edge("filter_failures", "analyze_patterns")
fa_builder.add_edge("analyze_patterns", "generate_summary")
fa_builder.add_edge("generate_summary", END)

failure_analysis_graph = fa_builder.compile()

# ============ å­å›¾ 2: æ€§èƒ½åˆ†æ ============

class PerformanceAnalysisState(TypedDict):
    logs: List[dict]
    slow_logs: List[dict]     # ä¸­é—´
    latency_stats: dict       # ä¸­é—´
    report: str               # è¾“å‡º

class PerformanceAnalysisOutput(TypedDict):
    report: str

def filter_slow(state):
    slow = [log for log in state["logs"] if log.get("latency", 0) > 1000]
    return {"slow_logs": slow}

def calculate_stats(state):
    latencies = [log["latency"] for log in state["slow_logs"]]
    stats = {
        "avg": sum(latencies) / len(latencies) if latencies else 0,
        "max": max(latencies) if latencies else 0
    }
    return {"latency_stats": stats}

def generate_report(state):
    stats = state["latency_stats"]
    report = f"Avg latency: {stats['avg']}ms, Max: {stats['max']}ms"
    return {"report": report}

pa_builder = StateGraph(
    PerformanceAnalysisState,
    output_schema=PerformanceAnalysisOutput
)

pa_builder.add_node("filter_slow", filter_slow)
pa_builder.add_node("calculate_stats", calculate_stats)
pa_builder.add_node("generate_report", generate_report)

pa_builder.add_edge(START, "filter_slow")
pa_builder.add_edge("filter_slow", "calculate_stats")
pa_builder.add_edge("calculate_stats", "generate_report")
pa_builder.add_edge("generate_report", END)

performance_analysis_graph = pa_builder.compile()

# ============ ä¸»å›¾:æ•´åˆä¸¤ä¸ªå­å›¾ ============

from operator import add
from typing import Annotated

class MainState(TypedDict):
    raw_logs: List[dict]
    summary: str      # æ¥è‡ªå¤±è´¥åˆ†æå­å›¾
    report: str       # æ¥è‡ªæ€§èƒ½åˆ†æå­å›¾
    final_output: str

def prepare_logs(state):
    # å‡†å¤‡æ—¥å¿—æ•°æ®
    return {"raw_logs": state["raw_logs"]}

def finalize(state):
    output = f"Failure Summary: {state['summary']}\n"
    output += f"Performance Report: {state['report']}"
    return {"final_output": output}

# æ„å»ºä¸»å›¾
main_builder = StateGraph(MainState)
main_builder.add_node("prepare", prepare_logs)
main_builder.add_node("failure_analysis", failure_analysis_graph)  # â­ å­å›¾ä½œä¸ºèŠ‚ç‚¹
main_builder.add_node("performance_analysis", performance_analysis_graph)
main_builder.add_node("finalize", finalize)

main_builder.add_edge(START, "prepare")
main_builder.add_edge("prepare", "failure_analysis")
main_builder.add_edge("prepare", "performance_analysis")
main_builder.add_edge("failure_analysis", "finalize")
main_builder.add_edge("performance_analysis", "finalize")
main_builder.add_edge("finalize", END)

main_graph = main_builder.compile()

# æ‰§è¡Œ
logs = [
    {"id": 1, "status": "error", "error_type": "timeout", "latency": 5000},
    {"id": 2, "status": "success", "latency": 200},
    {"id": 3, "status": "error", "error_type": "404", "latency": 1500}
]

result = main_graph.invoke({"raw_logs": logs})
print(result["final_output"])
```

#### å…³é”®æ”¶ç›Š

**1. çŠ¶æ€éš”ç¦»**
```python
# å­å›¾çš„ä¸­é—´å˜é‡ä¸ä¼šæ±¡æŸ“ä¸»å›¾
# failed_logs, error_patterns, temp_cache ç­‰éƒ½ä¸ä¼šå‡ºç°åœ¨ä¸»å›¾çŠ¶æ€ä¸­
```

**2. æ¸…æ™°çš„æ¥å£**
```python
# æ˜ç¡®å­å›¾çš„è¾“å…¥å’Œè¾“å‡º
# å°±åƒå‡½æ•°ç­¾åä¸€æ ·æ¸…æ™°
def failure_analysis(logs: List) -> str:  # è¾“å…¥ logs,è¾“å‡º summary
    ...
```

**3. æ˜“äºæµ‹è¯•**
```python
# å¯ä»¥ç‹¬ç«‹æµ‹è¯•å­å›¾
sub_result = failure_analysis_graph.invoke({"logs": test_logs})
assert "summary" in sub_result
assert "failed_logs" not in sub_result  # ä¸­é—´å˜é‡ä¸ä¼šè¿”å›
```

#### å¸¸è§é”™è¯¯

**é”™è¯¯ 1:å¿˜è®°ä½¿ç”¨ output_schema**
```python
# âŒ æ‰€æœ‰å­—æ®µéƒ½ä¼šè¿”å›,æ±¡æŸ“ä¸»å›¾
sub_graph = StateGraph(SubState)

# âœ… æ˜ç¡®æŒ‡å®šè¾“å‡º
sub_graph = StateGraph(SubState, output_schema=Output)
```

**é”™è¯¯ 2:output_schema åŒ…å«ä¸å­˜åœ¨çš„å­—æ®µ**
```python
# âŒ output_schema ä¸­çš„å­—æ®µå¿…é¡»åœ¨ state_schema ä¸­å­˜åœ¨
class State(TypedDict):
    input: str
    result: str

class Output(TypedDict):
    result: str
    extra_field: str  # âŒ State ä¸­æ²¡æœ‰è¿™ä¸ªå­—æ®µ

# âœ… åªåŒ…å« State ä¸­å­˜åœ¨çš„å­—æ®µ
class Output(TypedDict):
    result: str
```

**é”™è¯¯ 3:ä¸»å›¾çŠ¶æ€ä¸åŒ…å«å­å›¾è¾“å‡ºå­—æ®µ**
```python
# å­å›¾è¾“å‡º
class SubOutput(TypedDict):
    result: str

# âŒ ä¸»å›¾çŠ¶æ€ç¼ºå°‘ result å­—æ®µ
class MainState(TypedDict):
    input: str
    # ç¼ºå°‘ result

# âœ… ä¸»å›¾çŠ¶æ€åŒ…å«å­å›¾çš„è¾“å‡ºå­—æ®µ
class MainState(TypedDict):
    input: str
    result: str  # æ¥æ”¶å­å›¾çš„è¾“å‡º
```

</details>

---

### é—®é¢˜ 3: Send API ä¸ä¼ ç»Ÿ add_edge çš„æœ¬è´¨åŒºåˆ«

<details>
<summary><b>å±•å¼€æŸ¥çœ‹å®Œæ•´è§£æ</b></summary>

#### æ ¸å¿ƒåŒºåˆ«

| ç»´åº¦ | add_edge | Send API |
|------|----------|----------|
| **ä»»åŠ¡æ•°é‡** | é™æ€å›ºå®š | åŠ¨æ€å¯å˜ |
| **å†³å®šæ—¶æœº** | ç¼–è¯‘æ—¶ | è¿è¡Œæ—¶ |
| **çŠ¶æ€ä¼ é€’** | å®Œæ•´çŠ¶æ€ | å¯è‡ªå®šä¹‰éƒ¨åˆ†çŠ¶æ€ |
| **å¹¶è¡Œåº¦** | å›ºå®š | æ ¹æ®æ•°æ®åŠ¨æ€è°ƒæ•´ |

#### è¯¦ç»†å¯¹æ¯”ç¤ºä¾‹

**åœºæ™¯:å¤„ç†å¤šä¸ªå­ä¸»é¢˜**

**æ–¹æ¡ˆ 1:ä½¿ç”¨ add_edge (é™æ€)**
```python
# âŒ é—®é¢˜:å¿…é¡»é¢„å…ˆçŸ¥é“æœ‰å¤šå°‘ä¸ªå­ä¸»é¢˜
builder.add_node("process_topic_1", process_func)
builder.add_node("process_topic_2", process_func)
builder.add_node("process_topic_3", process_func)

builder.add_edge("generate_topics", "process_topic_1")
builder.add_edge("generate_topics", "process_topic_2")
builder.add_edge("generate_topics", "process_topic_3")

# å¦‚æœå®é™…æœ‰ 5 ä¸ªä¸»é¢˜æ€ä¹ˆåŠï¼Ÿ
# å¦‚æœåªæœ‰ 2 ä¸ªä¸»é¢˜,ç¬¬ 3 ä¸ªèŠ‚ç‚¹ä¼šæµªè´¹ï¼Ÿ
```

**æ–¹æ¡ˆ 2:ä½¿ç”¨ Send API (åŠ¨æ€)**
```python
from langgraph.types import Send

def dispatch_topics(state):
    topics = state["topics"]  # å¯èƒ½æ˜¯ 2 ä¸ª,ä¹Ÿå¯èƒ½æ˜¯ 10 ä¸ª
    
    # âœ… è‡ªåŠ¨ä¸ºæ¯ä¸ªä¸»é¢˜åˆ›å»ºå¤„ç†ä»»åŠ¡
    return [Send("process_topic", {"topic": t}) for t in topics]

builder.add_conditional_edges(
    "generate_topics",
    dispatch_topics,
    ["process_topic"]
)

# æ— è®ºæœ‰å¤šå°‘ä¸»é¢˜,éƒ½èƒ½è‡ªåŠ¨å¤„ç†
```

#### Send API å®Œæ•´ç¤ºä¾‹

```python
from langgraph.types import Send
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from typing import Annotated, List
import operator

# ============ çŠ¶æ€å®šä¹‰ ============

# å…¨å±€çŠ¶æ€
class OverallState(TypedDict):
    topic: str
    subtopics: List[str]
    analyses: Annotated[List[str], operator.add]  # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    final_report: str

# Map èŠ‚ç‚¹çš„å±€éƒ¨çŠ¶æ€
class AnalysisState(TypedDict):
    subtopic: str

# ============ èŠ‚ç‚¹å‡½æ•° ============

def generate_subtopics(state: OverallState):
    """ç”Ÿæˆå­ä¸»é¢˜"""
    topic = state["topic"]
    
    # å‡è®¾ LLM è¿”å› 3-10 ä¸ªå­ä¸»é¢˜(æ•°é‡ä¸å›ºå®š)
    subtopics = llm_generate_subtopics(topic)
    
    return {"subtopics": subtopics}

def dispatch_analyses(state: OverallState):
    """åŠ¨æ€åˆ†å‘åˆ†æä»»åŠ¡"""
    subtopics = state["subtopics"]
    
    # â­ å…³é”®:ä¸ºæ¯ä¸ªå­ä¸»é¢˜åˆ›å»º Send ä»»åŠ¡
    return [
        Send("analyze_subtopic", {"subtopic": st})
        for st in subtopics
    ]

def analyze_subtopic(state: AnalysisState):
    """åˆ†æå•ä¸ªå­ä¸»é¢˜"""
    subtopic = state["subtopic"]
    
    # æ‰§è¡Œåˆ†æ
    analysis = llm_analyze(subtopic)
    
    # è¿”å›ç»“æœ(ä¼šè¢« operator.add åˆå¹¶åˆ° analyses åˆ—è¡¨)
    return {"analyses": [analysis]}

def write_report(state: OverallState):
    """æ±‡æ€»æ‰€æœ‰åˆ†æ"""
    analyses = state["analyses"]
    
    # æ•´åˆæˆæœ€ç»ˆæŠ¥å‘Š
    report = llm_summarize(analyses)
    
    return {"final_report": report}

# ============ æ„å»ºå›¾ ============

builder = StateGraph(OverallState)

builder.add_node("generate_subtopics", generate_subtopics)
builder.add_node("analyze_subtopic", analyze_subtopic)
builder.add_node("write_report", write_report)

builder.add_edge(START, "generate_subtopics")

# â­ ä½¿ç”¨ Send åŠ¨æ€åˆ†å‘
builder.add_conditional_edges(
    "generate_subtopics",
    dispatch_analyses,
    ["analyze_subtopic"]  # ç›®æ ‡èŠ‚ç‚¹
)

builder.add_edge("analyze_subtopic", "write_report")
builder.add_edge("write_report", END)

graph = builder.compile()

# ============ æ‰§è¡Œ ============

result = graph.invoke({"topic": "AI Safety"})

# æµç¨‹:
# 1. generate_subtopics â†’ ç”Ÿæˆ 5 ä¸ªå­ä¸»é¢˜
# 2. dispatch_analyses â†’ åˆ›å»º 5 ä¸ª Send ä»»åŠ¡
# 3. analyze_subtopic Ã— 5 â†’ å¹¶è¡Œåˆ†æ 5 ä¸ªå­ä¸»é¢˜
# 4. write_report â†’ æ±‡æ€» 5 ä¸ªåˆ†æç»“æœ
```

#### Send API é«˜çº§ç”¨æ³•

**1. æ¡ä»¶æ€§åˆ†å‘**
```python
def conditional_dispatch(state):
    tasks = state["tasks"]
    
    # åªå¤„ç†é«˜ä¼˜å…ˆçº§ä»»åŠ¡
    return [
        Send("process_task", {"task": t})
        for t in tasks
        if t.get("priority") == "high"
    ]
```

**2. åˆ†å‘åˆ°ä¸åŒèŠ‚ç‚¹**
```python
def multi_target_dispatch(state):
    items = state["items"]
    sends = []
    
    for item in items:
        if item["type"] == "text":
            sends.append(Send("process_text", {"item": item}))
        elif item["type"] == "image":
            sends.append(Send("process_image", {"item": item}))
    
    return sends
```

**3. ä¼ é€’é¢å¤–ä¸Šä¸‹æ–‡**
```python
def dispatch_with_context(state):
    subtopics = state["subtopics"]
    original_topic = state["topic"]
    
    return [
        Send("analyze", {
            "subtopic": st,
            "context": original_topic,  # ä¼ é€’é¢å¤–ä¿¡æ¯
            "timestamp": time.time()
        })
        for st in subtopics
    ]
```

**4. æ§åˆ¶å¹¶è¡Œåº¦**
```python
def limited_dispatch(state, max_parallel=5):
    tasks = state["tasks"]
    
    # åªåˆ†å‘å‰ max_parallel ä¸ªä»»åŠ¡
    return [
        Send("process", {"task": t})
        for t in tasks[:max_parallel]
    ]
```

#### æ€§èƒ½å¯¹æ¯”

**åœºæ™¯:å¤„ç† 100 ä¸ªæ–‡æ¡£**

**ä¼ ç»Ÿæ–¹å¼(é¡ºåº):**
```python
# 100 ä¸ªæ–‡æ¡£é¡ºåºå¤„ç†
for doc in documents:
    result = process(doc)  # æ¯ä¸ª 2 ç§’
# æ€»æ—¶é—´: 200 ç§’
```

**ä½¿ç”¨ Send API(å¹¶è¡Œ):**
```python
def dispatch_docs(state):
    return [Send("process", {"doc": d}) for d in state["documents"]]

# 100 ä¸ªæ–‡æ¡£å¹¶è¡Œå¤„ç†
# æ€»æ—¶é—´: ~2 ç§’ (å‡è®¾æœ‰è¶³å¤Ÿçš„è®¡ç®—èµ„æº)
```

#### æœ€ä½³å®è·µ

**1. åˆç†æ§åˆ¶å¹¶è¡Œåº¦**
```python
# âŒ æ— é™åˆ¶:å¯èƒ½è€—å°½èµ„æº
return [Send("process", {"item": i}) for i in huge_list]

# âœ… åˆ†æ‰¹å¤„ç†
def batched_dispatch(state, batch_size=10):
    items = state["items"]
    return [
        Send("process", {"item": i})
        for i in items[:batch_size]
    ]
```

**2. ä¼ é€’æœ€å°å¿…éœ€çŠ¶æ€**
```python
# âŒ ä¼ é€’æ•´ä¸ªçŠ¶æ€:æµªè´¹
Send("process", state)

# âœ… åªä¼ é€’éœ€è¦çš„å­—æ®µ
Send("process", {"item": specific_item, "context": minimal_context})
```

**3. ä½¿ç”¨æœ‰æ„ä¹‰çš„èŠ‚ç‚¹å**
```python
# âŒ ä¸æ¸…æ™°
Send("node_1", data)

# âœ… æ¸…æ™°
Send("analyze_sentiment", data)
Send("translate_text", data)
```

</details>

---

