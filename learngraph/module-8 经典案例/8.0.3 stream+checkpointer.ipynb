{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9358144",
   "metadata": {},
   "source": [
    "# 案例一（stream）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee2fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== 1) stream_mode = 'values' =====================\n",
      "{'count': 1}\n",
      "{'count': 2}\n",
      "\n",
      "===================== 2) stream_mode = 'updates' =====================\n",
      "{'node1': {'count': 2}}\n",
      "\n",
      "===================== 3) stream_mode = 'debug' =====================\n",
      "{'step': 1, 'timestamp': '2025-11-29T06:57:33.935174+00:00', 'type': 'task', 'payload': {'id': 'fe5fafb5-c0ef-ef6f-9e5b-9d00c37b55f1', 'name': 'node1', 'input': {'count': 1}, 'triggers': ('branch:to:node1',)}}\n",
      "{'step': 1, 'timestamp': '2025-11-29T06:57:33.935222+00:00', 'type': 'task_result', 'payload': {'id': 'fe5fafb5-c0ef-ef6f-9e5b-9d00c37b55f1', 'name': 'node1', 'error': None, 'result': {'count': 2}, 'interrupts': []}}\n",
      "\n",
      "===================== 4) stream_mode = 'messages' =====================\n",
      "(AIMessage(content='Node executed, count = 2', additional_kwargs={}, response_metadata={}, id='2be20e4a-1a48-4e54-a62c-c24c9d33a316'), {'langgraph_step': 1, 'langgraph_node': 'node1', 'langgraph_triggers': ('branch:to:node1',), 'langgraph_path': ('__pregel_pull', 'node1'), 'langgraph_checkpoint_ns': 'node1:35e8c7e0-b45d-0642-c4b2-2d68740d2220'})\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.constants import START, END\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1. 定义 State（图的状态结构）\n",
    "# ------------------------------------------------------\n",
    "# 这里只有一个字段 count，节点会把 count + 1。\n",
    "# ======================================================\n",
    "class State(TypedDict):\n",
    "    count: int\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. 定义一个简单节点 node1\n",
    "# ------------------------------------------------------\n",
    "# 每次执行 node1：\n",
    "#   - count + 1\n",
    "#   - 返回 {\"count\": 新值}\n",
    "#\n",
    "# 它也会输出一条 AIMessage，用于 messages 模式演示。\n",
    "# ======================================================\n",
    "def node1(state: State, config: RunnableConfig):\n",
    "    new_count = state[\"count\"] + 1\n",
    "    msg = AIMessage(content=f\"Node executed, count = {new_count}\")\n",
    "    return {\n",
    "        \"count\": new_count,\n",
    "        \"messages\": [msg]     # 用于 stream_mode=\"messages\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. 构造图\n",
    "# ------------------------------------------------------\n",
    "# 流程：\n",
    "#   START → node1 → END\n",
    "# ======================================================\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node1\", node1)\n",
    "builder.add_edge(START, \"node1\")\n",
    "builder.add_edge(\"node1\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. 演示四种 stream_mode\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n===================== 1) stream_mode = 'values' =====================\")\n",
    "# 输出最终 state，不展示中间过程\n",
    "for value in graph.stream({\"count\": 1}, stream_mode=\"values\"):\n",
    "    print(value)\n",
    "# 输出示例： {'count': 2}\n",
    "\n",
    "\n",
    "print(\"\\n===================== 2) stream_mode = 'updates' =====================\")\n",
    "# 每次节点更新状态时输出增量（diff）\n",
    "for update in graph.stream({\"count\": 1}, stream_mode=\"updates\"):\n",
    "    print(update)\n",
    "# 输出示例：\n",
    "# {'count': 2}\n",
    "\n",
    "\n",
    "print(\"\\n===================== 3) stream_mode = 'debug' =====================\")\n",
    "# 输出最详细调试信息，例如：\n",
    "#   - 执行的节点名\n",
    "#   - 输入状态\n",
    "#   - 输出状态\n",
    "#   - 路由信息\n",
    "#   - 缓存命中情况\n",
    "for info in graph.stream({\"count\": 1}, stream_mode=\"debug\"):\n",
    "    print(info)\n",
    "# 输出类似：\n",
    "# {\n",
    "#   'node': 'node1',\n",
    "#   'input': {'count': 1},\n",
    "#   'output': {'count': 2, 'messages': [...]},\n",
    "#   'metadata': {...}\n",
    "# }\n",
    "\n",
    "\n",
    "print(\"\\n===================== 4) stream_mode = 'messages' =====================\")\n",
    "# 用于 LLM 消息流式输出\n",
    "# node1 返回了 AIMessage，因此这里会 token-by-token 输出\n",
    "for msg in graph.stream({\"count\": 1}, stream_mode=\"messages\"):\n",
    "    print(msg)\n",
    "# 输出类似：\n",
    "# AIMessageChunk(content=\"Node executed, count = 2\")\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87b280",
   "metadata": {},
   "source": [
    "# 案例二（stream： custom）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112f665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Streaming Output (custom) =====\n",
      "{'自定义key': '在节点内部自定义输出信息'}\n"
     ]
    }
   ],
   "source": [
    "#stream_mode=\"custom\" 的含义\n",
    "#自定义流式输出如何工作\n",
    "#StateGraph 如何流式推送数据\n",
    "#这是 LangGraph 自定义流模式（custom stream mode） 的最小可复现示例。\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.config import get_stream_writer\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1. 定义状态结构（State）\n",
    "# ------------------------------------------------------\n",
    "# query : 输入的查询\n",
    "# answer: 节点执行后生成的回答\n",
    "# State 是 LangGraph 中每个节点接受 / 返回的状态\n",
    "# ======================================================\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. 定义一个节点 node\n",
    "# ------------------------------------------------------\n",
    "# 在 node 内部使用 get_stream_writer():\n",
    "#   - 可以向外推送自定义的“流式消息”\n",
    "#   - 不依赖 LLM，不依赖 AIMessage\n",
    "#   - 你可以自己定义输出结构，例如 dict、string\n",
    "#\n",
    "# writer({\"key\": \"value\"}) 会在 stream_mode=\"custom\" 时被捕获\n",
    "# 并立即被发送给客户端（用户）\n",
    "# ======================================================\n",
    "def node(state: State):\n",
    "    # 获取一个 writer，该 writer 只在 stream_mode=\"custom\" 时生效\n",
    "    writer = get_stream_writer()\n",
    "\n",
    "    # 推送一条自定义流信息（类似“中间进度消息”）\n",
    "    writer({\"自定义key\": \"在节点内部自定义输出信息\"})\n",
    "\n",
    "    # 节点最终返回新的 State 增量\n",
    "    return {\"answer\": \"some data\"}\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. 构建 Graph\n",
    "# ------------------------------------------------------\n",
    "# 图结构：\n",
    "#   START → node → END\n",
    "# ======================================================\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(\"node\", node)\n",
    "    .add_edge(START, \"node\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. 输入数据\n",
    "# ======================================================\n",
    "inputs = {\"query\": \"example\"}\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5. 使用 stream_mode = \"custom\"\n",
    "# ------------------------------------------------------\n",
    "# 图执行时会有两类流输出：\n",
    "#\n",
    "# (1) writer(...) 推送的自定义 streaming 内容\n",
    "#     → 会被作为 chunk 输出\n",
    "#\n",
    "# (2) 节点最终返回的 State 更新结果\n",
    "#\n",
    "# 你可以看到整个执行过程中的“实时信息”\n",
    "# ======================================================\n",
    "print(\"\\n===== Streaming Output (custom) =====\")\n",
    "for chunk in graph.stream(inputs, stream_mode=\"custom\"):\n",
    "    print(chunk)\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f675541",
   "metadata": {},
   "source": [
    "# 案例三（Stream：messages + LLMs）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcdac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Streaming tokens (gpt-5-mini) ===\n",
      "(AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='湖南', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='省', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='的', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='省', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='会', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='是', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='长沙', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='。', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-5-mini-2025-08-07', 'service_tier': 'default'}, id='run--058416bd-350f-4190-9916-8b2005548d29'), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n",
      "(AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run--058416bd-350f-4190-9916-8b2005548d29', usage_metadata={'input_tokens': 13, 'output_tokens': 145, 'total_tokens': 158, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}), {'langgraph_step': 1, 'langgraph_node': 'call_model', 'langgraph_triggers': ('branch:to:call_model',), 'langgraph_path': ('__pregel_pull', 'call_model'), 'langgraph_checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'checkpoint_ns': 'call_model:6232be95-0ef2-dad5-d8b4-3aa62875ca92', 'ls_provider': 'openai', 'ls_model_name': 'gpt-5-mini', 'ls_model_type': 'chat', 'ls_temperature': None})\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "\n",
    "\n",
    "# 1. 获取 API Key\n",
    "OPENAI_API_KEY = getpass(\"请输入你的 OpenAI API Key： \")\n",
    "\n",
    "\n",
    "# 2. 初始化 GPT-5-mini（流式输出）\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "\n",
    "# 3. 定义节点\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# 4. 创建 LangGraph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# 5. 输入消息\n",
    "inputs = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"湖南的省会是哪里？\"}]\n",
    "}\n",
    "\n",
    "\n",
    "# 6. 流式输出大模型 Token（重点）\n",
    "print(\"\\n=== Streaming tokens (gpt-5-mini) ===\")\n",
    "for chunk in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    print(chunk)\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a085475",
   "metadata": {},
   "source": [
    "# 案例四（Checkpointer-短期记忆）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f307fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "湖南的省会是哪里？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "湖南省的省会是长沙（长沙市）。\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "湖北呢？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "湖北省的省会是武汉（武汉市）。\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0. 输入你的 OpenAI API Key（手动输入，不会显示）\n",
    "# ============================================================\n",
    "from getpass import getpass\n",
    "OPENAI_API_KEY = getpass(\"请输入你的 OpenAI API Key：\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 使用 OpenAI 的 gpt-5-mini 模型\n",
    "# ------------------------------------------------------------\n",
    "# ChatOpenAI 是 LangChain 对 OpenAI Chat Completions 的封装，\n",
    "# 与 ChatTongyi 用法保持完全兼容。\n",
    "# streaming=True 允许流式输出。\n",
    "# ============================================================\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",           # ← 按你的要求：使用 gpt-5-mini\n",
    "    api_key=OPENAI_API_KEY,       # ← 用户手动输入\n",
    "    streaming=True                # ← 支持流式 Token（与原例一致）\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 定义一个 LangGraph 节点：调用大模型\n",
    "# ------------------------------------------------------------\n",
    "# 输入：MessagesState（包含历史对话 messages）\n",
    "# 输出：MessagesState（新增模型回复）\n",
    "#\n",
    "# OpenAI 的 ChatGPT 模型和通义千问一样，都兼容 messages 数组格式。\n",
    "# ============================================================\n",
    "def call_model(state: MessagesState) -> MessagesState:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. 定义并构建 Graph（状态结构为 MessagesState）\n",
    "# ------------------------------------------------------------\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. 初始化 Checkpointer（状态保存器）\n",
    "# ------------------------------------------------------------\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. 配置 config（向 Graph 注入 thread_id）\n",
    "# ------------------------------------------------------------\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"1\"   # 使用同一线程 ID，实现连续对话\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. 第一次调用：问“湖南的省会是哪里？”\n",
    "# ------------------------------------------------------------\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"湖南的省会是哪里？\"}\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. 第二次调用：继续问“湖北呢？”\n",
    "# ------------------------------------------------------------\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"湖北呢？\"}\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
