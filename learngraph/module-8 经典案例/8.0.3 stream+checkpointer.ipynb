{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9358144",
   "metadata": {},
   "source": [
    "# 案例一：Stream 流式输出模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee2fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.constants import START, END\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1. 定义 State（图的状态结构）\n",
    "# ------------------------------------------------------\n",
    "# progress：表示任务进度百分比\n",
    "# ======================================================\n",
    "class State(TypedDict):\n",
    "    progress: int\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. 定义一个简单节点 task_runner\n",
    "# ------------------------------------------------------\n",
    "# 每次执行 task_runner：\n",
    "#   - progress + 25\n",
    "#   - 返回 {\"progress\": 新值}\n",
    "#\n",
    "# 同时输出一条 AIMessage，用于 messages 模式演示。\n",
    "# ======================================================\n",
    "def task_runner(state: State, config: RunnableConfig):\n",
    "    new_progress = state[\"progress\"] + 25\n",
    "    msg = AIMessage(content=f\"任务执行中，当前进度 = {new_progress}%\")\n",
    "    return {\n",
    "        \"progress\": new_progress,\n",
    "        \"messages\": [msg]     # 用于 stream_mode=\"messages\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. 构造图\n",
    "# ------------------------------------------------------\n",
    "# 流程：\n",
    "#   START → task_runner → END\n",
    "# ======================================================\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"task_runner\", task_runner)\n",
    "builder.add_edge(START, \"task_runner\")\n",
    "builder.add_edge(\"task_runner\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. 演示四种 stream_mode\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n===================== 1) stream_mode = 'values' =====================\")\n",
    "# 输出每一步的完整 state\n",
    "for value in graph.stream({\"progress\": 0}, stream_mode=\"values\"):\n",
    "    print(value)\n",
    "# 输出示例： {'progress': 25}\n",
    "\n",
    "\n",
    "print(\"\\n===================== 2) stream_mode = 'updates' =====================\")\n",
    "# 每次节点更新状态时输出增量（diff）\n",
    "for update in graph.stream({\"progress\": 0}, stream_mode=\"updates\"):\n",
    "    print(update)\n",
    "# 输出示例：\n",
    "# {'task_runner': {'progress': 25}}\n",
    "\n",
    "\n",
    "print(\"\\n===================== 3) stream_mode = 'debug' =====================\")\n",
    "# 输出最详细调试信息，例如：\n",
    "#   - 执行的节点名\n",
    "#   - 输入状态\n",
    "#   - 输出状态\n",
    "#   - 路由信息\n",
    "for info in graph.stream({\"progress\": 0}, stream_mode=\"debug\"):\n",
    "    print(info)\n",
    "\n",
    "\n",
    "print(\"\\n===================== 4) stream_mode = 'messages' =====================\")\n",
    "# 用于 LLM 消息流式输出\n",
    "# task_runner 返回了 AIMessage，因此这里会输出消息\n",
    "for msg in graph.stream({\"progress\": 0}, stream_mode=\"messages\"):\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87b280",
   "metadata": {},
   "source": [
    "# 案例二：Custom 自定义流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_mode=\"custom\" 的含义\n",
    "# 自定义流式输出如何工作\n",
    "# StateGraph 如何流式推送数据\n",
    "# 这是 LangGraph 自定义流模式（custom stream mode）的最小可复现示例。\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.config import get_stream_writer\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 1. 定义状态结构（State）\n",
    "# ------------------------------------------------------\n",
    "# request : 输入的请求内容\n",
    "# response: 节点执行后生成的响应\n",
    "# ======================================================\n",
    "class State(TypedDict):\n",
    "    request: str\n",
    "    response: str\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 2. 定义一个节点 processor\n",
    "# ------------------------------------------------------\n",
    "# 在 processor 内部使用 get_stream_writer():\n",
    "#   - 可以向外推送自定义的\"流式消息\"\n",
    "#   - 不依赖 LLM，不依赖 AIMessage\n",
    "#   - 你可以自己定义输出结构，例如 dict、string\n",
    "#\n",
    "# writer({\"key\": \"value\"}) 会在 stream_mode=\"custom\" 时被捕获\n",
    "# 并立即被发送给客户端（用户）\n",
    "# ======================================================\n",
    "def processor(state: State):\n",
    "    # 获取一个 writer，该 writer 只在 stream_mode=\"custom\" 时生效\n",
    "    writer = get_stream_writer()\n",
    "\n",
    "    # 推送自定义的中间状态消息\n",
    "    writer({\"状态\": \"开始处理请求\"})\n",
    "    writer({\"进度\": \"50%\"})\n",
    "    writer({\"状态\": \"处理完成\"})\n",
    "\n",
    "    # 节点最终返回新的 State 增量\n",
    "    return {\"response\": \"请求已处理完毕\"}\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 3. 构建 Graph\n",
    "# ------------------------------------------------------\n",
    "# 图结构：\n",
    "#   START → processor → END\n",
    "# ======================================================\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(\"processor\", processor)\n",
    "    .add_edge(START, \"processor\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 4. 输入数据\n",
    "# ======================================================\n",
    "inputs = {\"request\": \"处理订单 #12345\"}\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# 5. 使用 stream_mode = \"custom\"\n",
    "# ------------------------------------------------------\n",
    "# 图执行时会有两类流输出：\n",
    "#\n",
    "# (1) writer(...) 推送的自定义 streaming 内容\n",
    "#     → 会被作为 chunk 输出\n",
    "#\n",
    "# (2) 节点最终返回的 State 更新结果\n",
    "#\n",
    "# 你可以看到整个执行过程中的\"实时信息\"\n",
    "# ======================================================\n",
    "print(\"\\n===== 自定义流式输出 (custom) =====\")\n",
    "for chunk in graph.stream(inputs, stream_mode=\"custom\"):\n",
    "    print(chunk)\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f675541",
   "metadata": {},
   "source": [
    "# 案例三：LLM Token 流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcdac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from getpass import getpass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "\n",
    "\n",
    "# 1. 获取 API Key\n",
    "OPENAI_API_KEY = getpass(\"请输入你的 OpenAI API Key：\")\n",
    "\n",
    "\n",
    "# 2. 初始化 GPT-4o-mini（流式输出）\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "\n",
    "# 3. 定义节点\n",
    "def chat_node(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# 4. 创建 LangGraph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_node\", chat_node)\n",
    "builder.add_edge(START, \"chat_node\")\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# 5. 输入消息\n",
    "inputs = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"请用一句话介绍 Python 语言\"}]\n",
    "}\n",
    "\n",
    "\n",
    "# 6. 流式输出大模型 Token（重点）\n",
    "print(\"\\n=== 流式输出 Token (GPT-4o-mini) ===\")\n",
    "for chunk in graph.stream(inputs, stream_mode=\"messages\"):\n",
    "    # chunk 是一个元组 (AIMessageChunk, metadata)\n",
    "    if hasattr(chunk[0], 'content') and chunk[0].content:\n",
    "        print(chunk[0].content, end=\"\", flush=True)\n",
    "\n",
    "print()  # 换行\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a085475",
   "metadata": {},
   "source": [
    "# 案例四：Checkpointer 对话记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f307fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0. 输入你的 OpenAI API Key（手动输入，不会显示）\n",
    "# ============================================================\n",
    "from getpass import getpass\n",
    "OPENAI_API_KEY = getpass(\"请输入你的 OpenAI API Key：\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. 使用 OpenAI 的 GPT-4o-mini 模型\n",
    "# ------------------------------------------------------------\n",
    "# ChatOpenAI 是 LangChain 对 OpenAI Chat Completions 的封装\n",
    "# streaming=True 允许流式输出\n",
    "# ============================================================\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. 定义一个 LangGraph 节点：调用大模型\n",
    "# ------------------------------------------------------------\n",
    "# 输入：MessagesState（包含历史对话 messages）\n",
    "# 输出：MessagesState（新增模型回复）\n",
    "# ============================================================\n",
    "def assistant(state: MessagesState) -> MessagesState:\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. 定义并构建 Graph（状态结构为 MessagesState）\n",
    "# ------------------------------------------------------------\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_edge(START, \"assistant\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. 初始化 Checkpointer（状态保存器）\n",
    "# ------------------------------------------------------------\n",
    "# InMemorySaver 会把每轮对话的状态保存在内存中\n",
    "# 这样后续的对话可以\"记住\"之前说过的话\n",
    "# ============================================================\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. 配置 config（向 Graph 注入 thread_id）\n",
    "# ------------------------------------------------------------\n",
    "# thread_id 是\"会话标识符\"\n",
    "# 相同的 thread_id 表示同一个对话，会共享历史记录\n",
    "# ============================================================\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"chat_session_001\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. 第一次调用：问\"什么是机器学习？\"\n",
    "# ------------------------------------------------------------\n",
    "print(\"【第一轮对话】\")\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"什么是机器学习？请简短回答\"}\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. 第二次调用：继续问\"它和深度学习有什么区别？\"\n",
    "# ------------------------------------------------------------\n",
    "# 注意：我们没有重新提供\"机器学习\"这个上下文\n",
    "# 但是模型能理解\"它\"指的是机器学习\n",
    "# 这就是 Checkpointer 的作用——保持对话上下文\n",
    "# ============================================================\n",
    "print(\"\\n【第二轮对话】\")\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"它和深度学习有什么区别？\"}\n",
    "        ]\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# 可视化图结构\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d52c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
