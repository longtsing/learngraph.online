# 流式输出与状态持久化

在实际的 AI 应用中，有两个非常重要的需求：

1. **流式输出（Streaming）**：让用户看到 AI 的"思考过程"，而不是等待最终结果
2. **状态持久化（Checkpointer）**：让 AI 记住之前的对话，实现连续交互

本节将深入讲解 LangGraph 1.x 中的 `stream()` 方法和 `Checkpointer` 机制。

---

## 什么是 Stream（流式输出）？

### 通俗解释

想象你在和一个人聊天：

- **非流式**：对方想好所有话之后，一次性说完（你要等很久）
- **流式**：对方边想边说，一个字一个字蹦出来（你能实时看到进展）

在 AI 应用中，流式输出让用户不必等待整个响应生成完毕，而是**实时看到输出过程**。这对于 LLM 尤其重要，因为生成一段长文本可能需要几秒甚至十几秒。

### LangGraph 的 stream() 方法

```python
# 非流式调用（等待完整结果）
result = graph.invoke(inputs)

# 流式调用（逐步获取输出）
for chunk in graph.stream(inputs, stream_mode="values"):
    print(chunk)
```

`stream_mode` 参数决定了流式输出的**粒度和内容**。

---

## 案例一：四种 Stream Mode

LangGraph 提供了四种流式输出模式，适用于不同场景：

| stream_mode | 输出内容 | 适用场景 |
|-------------|----------|----------|
| `values` | 每一步的完整状态 | 查看状态变化 |
| `updates` | 每次更新的增量 | 只关心改变的部分 |
| `debug` | 详细调试信息 | 开发调试 |
| `messages` | AI 消息流 | LLM Token 流式输出 |

### 1. stream_mode="values"

```python
for value in graph.stream({"progress": 0}, stream_mode="values"):
    print(value)

# 输出：
# {'progress': 0}    ← 初始状态
# {'progress': 25}   ← 节点执行后的状态
```

**特点**：输出每一步的**完整状态快照**。

**类比**：就像拍照片记录每一个时刻的全貌。

### 2. stream_mode="updates"

```python
for update in graph.stream({"progress": 0}, stream_mode="updates"):
    print(update)

# 输出：
# {'task_runner': {'progress': 25}}
```

**特点**：只输出**发生变化的部分**（增量）。

**类比**：就像 Git diff，只显示改动的内容。

### 3. stream_mode="debug"

```python
for info in graph.stream({"progress": 0}, stream_mode="debug"):
    print(info)

# 输出：
# {'step': 1, 'type': 'task', 'payload': {'name': 'task_runner', 'input': {...}}}
# {'step': 1, 'type': 'task_result', 'payload': {'name': 'task_runner', 'result': {...}}}
```

**特点**：输出**详细的执行信息**，包括：
- 执行的节点名称
- 输入和输出状态
- 执行时间戳
- 触发条件

**类比**：就像打开了"开发者模式"，能看到内部运作。

### 4. stream_mode="messages"

```python
for msg in graph.stream({"progress": 0}, stream_mode="messages"):
    print(msg)

# 输出：
# (AIMessage(content='任务执行中，当前进度 = 25%'), {...metadata...})
```

**特点**：专门用于输出 **AI 消息**，支持 LLM 的 Token 流式输出。

**类比**：就像看到 AI 一个字一个字地打出回复。

### 案例一完整代码

```python
from typing import TypedDict
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.constants import START, END
from langchain_core.messages import AIMessage


class State(TypedDict):
    progress: int


def task_runner(state: State, config: RunnableConfig):
    new_progress = state["progress"] + 25
    msg = AIMessage(content=f"任务执行中，当前进度 = {new_progress}%")
    return {
        "progress": new_progress,
        "messages": [msg]
    }


builder = StateGraph(State)
builder.add_node("task_runner", task_runner)
builder.add_edge(START, "task_runner")
builder.add_edge("task_runner", END)

graph = builder.compile()


# 演示四种模式
print("=== values ===")
for value in graph.stream({"progress": 0}, stream_mode="values"):
    print(value)

print("\n=== updates ===")
for update in graph.stream({"progress": 0}, stream_mode="updates"):
    print(update)

print("\n=== debug ===")
for info in graph.stream({"progress": 0}, stream_mode="debug"):
    print(info)

print("\n=== messages ===")
for msg in graph.stream({"progress": 0}, stream_mode="messages"):
    print(msg)

# 可视化图结构
from IPython.display import Image, display
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

---

## 案例二：Custom 自定义流式输出

有时候，你想在节点执行过程中**主动推送消息**给用户，比如：

- 显示处理进度：`"正在分析文档... 50%"`
- 发送中间结果：`"找到 3 个匹配项"`
- 报告状态变化：`"连接数据库成功"`

LangGraph 提供了 `get_stream_writer()` 来实现这个功能。

### 核心概念：StreamWriter

```python
from langgraph.config import get_stream_writer

def processor(state: State):
    writer = get_stream_writer()

    # 推送自定义消息（会立即发送给用户）
    writer({"状态": "开始处理"})
    writer({"进度": "50%"})
    writer({"状态": "处理完成"})

    return {"response": "完成"}
```

**工作原理**：
1. `get_stream_writer()` 获取一个写入器
2. 调用 `writer({...})` 可以随时推送消息
3. 只有在 `stream_mode="custom"` 时，这些消息才会被输出

### 使用场景

| 场景 | 示例消息 |
|------|----------|
| 长时间任务 | `{"progress": "30%", "step": "下载数据"}` |
| 多步骤流程 | `{"stage": "验证", "status": "通过"}` |
| 调试信息 | `{"debug": "缓存命中", "key": "abc"}` |

### 案例二完整代码

```python
from typing import TypedDict
from langgraph.config import get_stream_writer
from langgraph.graph import StateGraph, START


class State(TypedDict):
    request: str
    response: str


def processor(state: State):
    writer = get_stream_writer()

    # 推送自定义的中间状态消息
    writer({"状态": "开始处理请求"})
    writer({"进度": "50%"})
    writer({"状态": "处理完成"})

    return {"response": "请求已处理完毕"}


graph = (
    StateGraph(State)
    .add_node("processor", processor)
    .add_edge(START, "processor")
    .compile()
)


# 使用 custom 模式获取自定义消息
for chunk in graph.stream({"request": "处理订单 #12345"}, stream_mode="custom"):
    print(chunk)

# 输出：
# {'状态': '开始处理请求'}
# {'进度': '50%'}
# {'状态': '处理完成'}

# 可视化图结构
from IPython.display import Image, display
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

---

## 案例三：LLM Token 流式输出

当 Graph 中包含 LLM 调用时，我们通常希望**逐字显示 AI 的回复**，而不是等待整个回复生成完毕。

### 核心概念：AIMessageChunk

LLM 的流式输出会产生一系列 `AIMessageChunk`，每个 chunk 包含**部分内容**：

```
AIMessageChunk(content='Python')  ← 第 1 个 chunk
AIMessageChunk(content='是')      ← 第 2 个 chunk
AIMessageChunk(content='一种')    ← 第 3 个 chunk
...
```

### 实现方式

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START


# 1. 初始化 LLM（开启 streaming）
llm = ChatOpenAI(model="gpt-4o-mini", streaming=True)


# 2. 定义节点
def chat_node(state: MessagesState):
    response = llm.invoke(state["messages"])
    return {"messages": response}


# 3. 构建 Graph
builder = StateGraph(MessagesState)
builder.add_node("chat_node", chat_node)
builder.add_edge(START, "chat_node")
graph = builder.compile()


# 4. 流式输出 Token
for chunk in graph.stream(inputs, stream_mode="messages"):
    if chunk[0].content:
        print(chunk[0].content, end="", flush=True)
```

### 关键点

1. **LLM 必须开启 `streaming=True`**
2. **使用 `stream_mode="messages"`**
3. **chunk 是元组 `(AIMessageChunk, metadata)`**

### 案例三完整代码

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START


llm = ChatOpenAI(
    model="gpt-4o-mini",
    api_key="your-api-key",
    streaming=True
)


def chat_node(state: MessagesState):
    response = llm.invoke(state["messages"])
    return {"messages": response}


builder = StateGraph(MessagesState)
builder.add_node("chat_node", chat_node)
builder.add_edge(START, "chat_node")
graph = builder.compile()


# 流式输出
inputs = {"messages": [{"role": "user", "content": "请用一句话介绍 Python 语言"}]}

for chunk in graph.stream(inputs, stream_mode="messages"):
    if hasattr(chunk[0], 'content') and chunk[0].content:
        print(chunk[0].content, end="", flush=True)

# 可视化图结构
from IPython.display import Image, display
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

---

---

## 扩展：用 messages 模式统计 LLM 调用成本

在生产环境中，监控 LLM 的调用成本至关重要。`stream_mode="messages"` 模式不仅能实现流式输出，还能获取**Token 使用量**等元数据。

### 获取 Token 统计

```python
for chunk in graph.stream(inputs, stream_mode="messages"):
    msg, metadata = chunk

    # 检查是否是最后一个 chunk（包含完整统计）
    if hasattr(msg, 'response_metadata') and msg.response_metadata:
        token_usage = msg.response_metadata.get('token_usage', {})
        if token_usage:
            print(f"输入 Token: {token_usage.get('input_tokens', 0)}")
            print(f"输出 Token: {token_usage.get('output_tokens', 0)}")
            print(f"总计 Token: {token_usage.get('total_tokens', 0)}")
```

### 实际应用场景

| 场景 | 做法 |
|------|------|
| **成本控制** | 累计 Token 数，设置预算上限 |
| **性能分析** | 统计平均响应时间和 Token 消耗 |
| **计费系统** | 根据 Token 数向用户收费 |
| **异常检测** | 发现异常高的 Token 消耗 |

---

## 什么是 Checkpointer（状态持久化）？

### 通俗解释

想象你和 AI 聊天：

- **没有 Checkpointer**：每次对话都是"第一次见面"，AI 不记得你说过什么
- **有 Checkpointer**：AI 记得之前的对话，可以"接着聊"

Checkpointer 就是 LangGraph 的"记忆系统"，它会**保存每轮对话的状态**，让 Graph 可以：

1. **记住历史消息**：知道用户之前问了什么
2. **维护上下文**：理解"它"、"那个"等指代词
3. **实现多轮对话**：像真正的聊天一样连续交流

### 核心概念

#### 1. thread_id（会话 ID）

`thread_id` 是对话的唯一标识符：

```python
config = {
    "configurable": {
        "thread_id": "chat_session_001"  # 会话标识
    }
}
```

- **相同 thread_id** → 共享历史记录（同一个对话）
- **不同 thread_id** → 独立的对话（互不影响）

**类比**：就像微信里的不同聊天窗口，每个窗口有自己的聊天记录。

#### 2. InMemorySaver（内存保存器）

```python
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()
graph = builder.compile(checkpointer=checkpointer)
```

`InMemorySaver` 把状态保存在内存中：
- **优点**：简单、快速
- **缺点**：程序重启后数据丢失

#### 3. 其他 Checkpointer

| Checkpointer | 存储位置 | 适用场景 |
|--------------|----------|----------|
| `InMemorySaver` | 内存 | 开发测试 |
| `SqliteSaver` | SQLite 数据库 | 单机持久化 |
| `PostgresSaver` | PostgreSQL | 生产环境 |

---

## 深入理解：短期记忆 vs 长期记忆

在 AI 应用中，"记忆"有不同的层次和用途。LangGraph 提供了两种记忆机制，适用于不同场景。

### 短期记忆（Checkpointer）

**特点**：
- 基于 `thread_id` 区分不同会话
- 保存完整的对话历史
- 适合**单次会话**的上下文维护

**典型用法**：
```python
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()
graph = builder.compile(checkpointer=checkpointer)

# 同一个 thread_id = 同一个对话
config = {"configurable": {"thread_id": "session_001"}}
```

**类比**：就像你和朋友的一次聊天——聊完就结束了，下次见面可能不记得具体细节。

### 长期记忆（Store）

**特点**：
- 基于 `namespace` 和 `key` 组织数据
- 支持**语义检索**（结合向量数据库）
- 适合**跨会话**的知识积累

**典型用法**：
```python
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()
graph = builder.compile(checkpointer=checkpointer, store=store)

# 在节点中使用 store
def my_node(state, config, *, store):
    # 写入长期记忆
    store.put(("user", user_id), "preferences", {"theme": "dark"})

    # 读取长期记忆
    prefs = store.get(("user", user_id), "preferences")
```

**类比**：就像你的个人档案——记录了你的偏好、历史行为，无论何时访问都能获取。

### 选择指南

| 需求 | 选择 | 示例 |
|------|------|------|
| 记住本次对话说了什么 | 短期记忆 | 多轮问答、上下文理解 |
| 记住用户的偏好设置 | 长期记忆 | 个性化推荐、用户画像 |
| 记住历史交互模式 | 长期记忆 | 学习用户习惯、智能建议 |
| 调试和回溯 | 短期记忆 | Time Travel、断点续传 |

### 组合使用

实际项目中，两种记忆经常**配合使用**：

```python
def personalized_assistant(state, config, *, store):
    user_id = config["configurable"]["user_id"]

    # 从长期记忆获取用户偏好
    prefs = store.get(("user", user_id), "preferences")

    # 结合短期记忆（对话历史）和长期记忆（用户偏好）
    # 生成个性化回复
    ...
```

---

## 案例四：多轮对话记忆

### 工作流程

```
第一轮对话:
  用户: "什么是机器学习？"
  AI: "机器学习是..."

  → Checkpointer 保存: [用户问题, AI回答]

第二轮对话:
  用户: "它和深度学习有什么区别？"

  → Checkpointer 加载: [用户问题, AI回答]
  → AI 看到完整历史，理解"它"指机器学习
  → AI: "机器学习和深度学习的区别是..."
```

### 关键代码

```python
from langgraph.checkpoint.memory import InMemorySaver

# 1. 创建 Checkpointer
checkpointer = InMemorySaver()

# 2. 编译 Graph 时传入 checkpointer
graph = builder.compile(checkpointer=checkpointer)

# 3. 调用时传入 thread_id
config = {"configurable": {"thread_id": "chat_session_001"}}

# 第一轮
graph.stream({"messages": [{"role": "user", "content": "什么是机器学习？"}]}, config)

# 第二轮（同一个 thread_id，会记住第一轮）
graph.stream({"messages": [{"role": "user", "content": "它和深度学习有什么区别？"}]}, config)
```

### 案例四完整代码

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.checkpoint.memory import InMemorySaver


llm = ChatOpenAI(model="gpt-4o-mini", api_key="your-api-key", streaming=True)


def assistant(state: MessagesState) -> MessagesState:
    response = llm.invoke(state["messages"])
    return {"messages": response}


builder = StateGraph(MessagesState)
builder.add_node("assistant", assistant)
builder.add_edge(START, "assistant")

# 关键：添加 Checkpointer
checkpointer = InMemorySaver()
graph = builder.compile(checkpointer=checkpointer)

# 会话配置
config = {"configurable": {"thread_id": "chat_session_001"}}


# 第一轮对话
print("【第一轮对话】")
for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "什么是机器学习？请简短回答"}]},
    config,
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()


# 第二轮对话（AI 会记住第一轮）
print("\n【第二轮对话】")
for chunk in graph.stream(
    {"messages": [{"role": "user", "content": "它和深度学习有什么区别？"}]},
    config,
    stream_mode="values",
):
    chunk["messages"][-1].pretty_print()

# 可视化图结构
from IPython.display import Image, display
display(Image(graph.get_graph(xray=True).draw_mermaid_png()))
```

---

## Stream vs Invoke 对比

| 特性 | invoke() | stream() |
|------|----------|----------|
| **返回方式** | 一次性返回完整结果 | 逐步产出 chunks |
| **用户体验** | 需要等待 | 实时看到进展 |
| **适用场景** | 简单任务、批处理 | 交互式应用、LLM 对话 |
| **代码复杂度** | 简单 | 稍复杂（需要循环） |

---

## 本章总结

本节介绍了 LangGraph 的两大核心能力：

| 功能 | 核心概念 | 作用 |
|------|----------|------|
| **Stream** | `stream_mode` | 流式获取执行过程 |
| **Custom Stream** | `get_stream_writer()` | 主动推送自定义消息 |
| **LLM Streaming** | `AIMessageChunk` | 逐字显示 AI 回复 |
| **Checkpointer** | `thread_id` | 保存对话历史，实现多轮对话 |

### 使用建议

1. **开发调试**：用 `stream_mode="debug"` 查看详细信息
2. **生产环境**：用 `stream_mode="messages"` 实现流式 UI
3. **长任务**：用 `stream_mode="custom"` 报告进度
4. **多轮对话**：必须配置 `Checkpointer` + `thread_id`

---

## 思考题

1. 如果同时使用两种 stream_mode，会发生什么？（提示：可以传列表）
2. Checkpointer 保存了哪些数据？只有 messages 吗？
3. 如果用户关闭浏览器再打开，如何恢复之前的对话？
4. `stream_mode="values"` 和 `stream_mode="updates"` 在多节点 Graph 中有什么区别？

---

## 下一步

掌握了流式输出和状态持久化后，你已经可以构建**真正可交互的 AI 应用**了。接下来的章节会介绍：

- **Human-in-the-loop**：人类参与决策
- **Breakpoints**：流程暂停与恢复
- **Time Travel**：回溯到历史状态

这些高级特性将让你的 Agent 更加强大和可控！
