# ä»£ç åŠ©æ‰‹ï¼šRAG + è‡ªæˆ‘çº æ­£çš„ä»£ç ç”Ÿæˆ

> æœ¬æ–‡åŸºäº LangGraph å®˜æ–¹æ•™ç¨‹è¿›è¡Œè§£è¯»ï¼ŒåŸå§‹ Notebook åœ°å€ï¼š[langgraph_code_assistant.ipynb](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/code_assistant/langgraph_code_assistant.ipynb)

---

## ä¸€ã€è¿™ä¸ªæ¡ˆä¾‹è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

AlphaCodium æå‡ºäº†ä¸€ç§ä½¿ç”¨æ§åˆ¶æµè¿›è¡Œä»£ç ç”Ÿæˆçš„æ–¹æ³•ã€‚æ ¸å¿ƒæ€æƒ³æ¥è‡ª Andrej Karpathy çš„è§‚ç‚¹ï¼š[**è¿­ä»£åœ°æ„å»ºç¼–ç¨‹é—®é¢˜çš„ç­”æ¡ˆ**](https://x.com/karpathy/status/1748043513156272416?s=20)ã€‚

[AlphaCodium](https://github.com/Codium-ai/AlphaCodium) ä¼šåœ¨å…¬å…±æµ‹è¯•å’Œ AI ç”Ÿæˆçš„æµ‹è¯•ä¸Šè¿­ä»£åœ°æµ‹è¯•å’Œæ”¹è¿›ç­”æ¡ˆã€‚

æœ¬æ¡ˆä¾‹å°†ä½¿ç”¨ [LangGraph](https://langchain-ai.github.io/langgraph/) ä»é›¶å®ç°è¿™äº›æƒ³æ³•ï¼š

1. ä»ç”¨æˆ·æŒ‡å®šçš„ä¸€ç»„æ–‡æ¡£å¼€å§‹
2. ä½¿ç”¨é•¿ä¸Šä¸‹æ–‡ LLM æ‘„å–æ–‡æ¡£ï¼Œå¹¶åŸºäº RAG å›ç­”é—®é¢˜
3. è°ƒç”¨å·¥å…·ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
4. åœ¨è¿”å›ç»™ç”¨æˆ·ä¹‹å‰æ‰§è¡Œä¸¤é¡¹å•å…ƒæµ‹è¯•ï¼ˆæ£€æŸ¥å¯¼å…¥å’Œä»£ç æ‰§è¡Œï¼‰

### ç³»ç»Ÿæ¶æ„å›¾

![ä»£ç åŠ©æ‰‹æ¶æ„å›¾](./8.3-code-assistant-architecture.png)

ä¸Šå›¾å±•ç¤ºäº†æ•´ä¸ªç³»ç»Ÿçš„å·¥ä½œæµç¨‹ï¼šç”Ÿæˆä»£ç  â†’ æ£€æŸ¥ä»£ç  â†’ æ ¹æ®ç»“æœå†³å®šæ˜¯å¦é‡è¯•æˆ–åæ€ã€‚

---

## äºŒã€ç¯å¢ƒå‡†å¤‡

### 2.1 å®‰è£…ä¾èµ–

```python
! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4
```

### 2.2 è®¾ç½® API Key

```python
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
_set_env("ANTHROPIC_API_KEY")
```

> **æç¤º**: å»ºè®®è®¾ç½® [LangSmith](https://smith.langchain.com) æ¥è¿½è¸ªå’Œè°ƒè¯• LangGraph é¡¹ç›®ã€‚LangSmith å¯ä»¥å¸®åŠ©ä½ ä½¿ç”¨è¿½è¸ªæ•°æ®æ¥è°ƒè¯•ã€æµ‹è¯•å’Œç›‘æ§ä½ çš„ LLM åº”ç”¨ã€‚[äº†è§£æ›´å¤š](https://docs.smith.langchain.com)

---

## ä¸‰ã€åŠ è½½æ–‡æ¡£

æœ¬æ¡ˆä¾‹ä½¿ç”¨ [LangChain Expression Language](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel) (LCEL) æ–‡æ¡£ä½œä¸ºç¤ºä¾‹ã€‚

```python
from bs4 import BeautifulSoup as Soup
from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader

# LCEL docs
url = "https://python.langchain.com/docs/concepts/lcel/"
loader = RecursiveUrlLoader(
    url=url, max_depth=20, extractor=lambda x: Soup(x, "html.parser").text
)
docs = loader.load()

# Sort the list based on the URLs and get the text
d_sorted = sorted(docs, key=lambda x: x.metadata["source"])
d_reversed = list(reversed(d_sorted))
concatenated_content = "\n\n\n --- \n\n\n".join(
    [doc.page_content for doc in d_reversed]
)
```

**ä»£ç è§£è¯»ï¼š**

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| `RecursiveUrlLoader` | é€’å½’çˆ¬å– URL é¡µé¢ï¼Œæœ€å¤§æ·±åº¦ 20 å±‚ |
| `BeautifulSoup` | æå– HTML ä¸­çš„çº¯æ–‡æœ¬ |
| `concatenated_content` | å°†æ‰€æœ‰æ–‡æ¡£æ‹¼æ¥æˆä¸€ä¸ªé•¿å­—ç¬¦ä¸²ï¼Œç”¨äº RAG |

---

## å››ã€ä»£ç ç”Ÿæˆé“¾

### 4.1 å®šä¹‰ä»£ç ç»“æ„

é¦–å…ˆå®šä¹‰ä»£ç è¾“å‡ºçš„ç»“æ„åŒ–æ ¼å¼ï¼š

> **æ³¨æ„**ï¼šæœ¬ notebook ä½¿ç”¨ Pydantic v2 `BaseModel`ï¼Œéœ€è¦ `langchain-core >= 0.3`ã€‚ä½¿ç”¨ `langchain-core < 0.3` ä¼šå› ä¸ºæ··ç”¨ Pydantic v1 å’Œ v2 çš„ `BaseModel` è€ŒæŠ¥é”™ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field


# Data model
class code(BaseModel):
    """Schema for code solutions to questions about LCEL."""

    prefix: str = Field(description="Description of the problem and approach")
    imports: str = Field(description="Code block import statements")
    code: str = Field(description="Code block not including import statements")
```

**ç»“æ„è§£è¯»ï¼š**

| å­—æ®µ | è¯´æ˜ |
|------|------|
| `prefix` | é—®é¢˜æè¿°å’Œè§£å†³æ–¹æ¡ˆæ¦‚è¿° |
| `imports` | ä»£ç çš„å¯¼å…¥è¯­å¥ï¼ˆå•ç‹¬åˆ†ç¦»ä¾¿äºæµ‹è¯•ï¼‰ |
| `code` | ä¸»ä½“ä»£ç ï¼ˆä¸åŒ…å«å¯¼å…¥ï¼‰ |

### 4.2 OpenAI ä»£ç ç”Ÿæˆé“¾

```python
### OpenAI

# Grader prompt
code_gen_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """You are a coding assistant with expertise in LCEL, LangChain expression language. \n
    Here is a full set of LCEL documentation:  \n ------- \n  {context} \n ------- \n Answer the user
    question based on the above provided documentation. Ensure any code you provide can be executed \n
    with all required imports and variables defined. Structure your answer with a description of the code solution. \n
    Then list the imports. And finally list the functioning code block. Here is the user question:""",
        ),
        ("placeholder", "{messages}"),
    ]
)


expt_llm = "gpt-4o-mini"
llm = ChatOpenAI(temperature=0, model=expt_llm)
code_gen_chain_oai = code_gen_prompt | llm.with_structured_output(code)
question = "How do I build a RAG chain in LCEL?"
solution = code_gen_chain_oai.invoke(
    {"context": concatenated_content, "messages": [("user", question)]}
)
solution
```

**è¾“å‡ºç»“æœï¼š**

```
code(prefix='To build a Retrieval-Augmented Generation (RAG) chain in LCEL, you will need to set up a chain that combines a retriever and a language model (LLM). The retriever will fetch relevant documents based on a query, and the LLM will generate a response using the retrieved documents as context. Here's how you can do it:', imports='from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.retrievers import MyRetriever', code='# Define the retriever\nretriever = MyRetriever()  # Replace with your specific retriever implementation\n\n# Define the LLM model\nmodel = ChatOpenAI(model="gpt-4")\n\n# Create a prompt template for the LLM\nprompt_template = ChatPromptTemplate.from_template("Given the following documents, answer the question: {question}\nDocuments: {documents}")\n\n# Create the RAG chain\nrag_chain = prompt_template | retriever | model | StrOutputParser()\n\n# Example usage\nquery = "What are the benefits of using RAG?"\nresponse = rag_chain.invoke({"question": query})\nprint(response)')
```

### 4.3 Anthropic Claude ä»£ç ç”Ÿæˆé“¾

Claude éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå·¥å…·è°ƒç”¨å¯èƒ½ä¼šå¤±è´¥ã€‚æˆ‘ä»¬æ·»åŠ äº†é‡è¯•æœºåˆ¶ï¼š

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate

### Anthropic

# Prompt to enforce tool use
code_gen_prompt_claude = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """<instructions> You are a coding assistant with expertise in LCEL, LangChain expression language. \n
    Here is the LCEL documentation:  \n ------- \n  {context} \n ------- \n Answer the user  question based on the \n
    above provided documentation. Ensure any code you provide can be executed with all required imports and variables \n
    defined. Structure your answer: 1) a prefix describing the code solution, 2) the imports, 3) the functioning code block. \n
    Invoke the code tool to structure the output correctly. </instructions> \n Here is the user question:""",
        ),
        ("placeholder", "{messages}"),
    ]
)


# LLM
expt_llm = "claude-3-opus-20240229"
llm = ChatAnthropic(
    model=expt_llm,
    default_headers={"anthropic-beta": "tools-2024-04-04"},
)

structured_llm_claude = llm.with_structured_output(code, include_raw=True)


# Optional: Check for errors in case tool use is flaky
def check_claude_output(tool_output):
    """Check for parse error or failure to call the tool"""

    # Error with parsing
    if tool_output["parsing_error"]:
        # Report back output and parsing errors
        print("Parsing error!")
        raw_output = str(tool_output["raw"].content)
        error = tool_output["parsing_error"]
        raise ValueError(
            f"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \n Parse error: {error}"
        )

    # Tool was not invoked
    elif not tool_output["parsed"]:
        print("Failed to invoke tool!")
        raise ValueError(
            "You did not use the provided tool! Be sure to invoke the tool to structure the output."
        )
    return tool_output


# Chain with output check
code_chain_claude_raw = (
    code_gen_prompt_claude | structured_llm_claude | check_claude_output
)


def insert_errors(inputs):
    """Insert errors for tool parsing in the messages"""

    # Get errors
    error = inputs["error"]
    messages = inputs["messages"]
    messages += [
        (
            "assistant",
            f"Retry. You are required to fix the parsing errors: {error} \n\n You must invoke the provided tool.",
        )
    ]
    return {
        "messages": messages,
        "context": inputs["context"],
    }


# This will be run as a fallback chain
fallback_chain = insert_errors | code_chain_claude_raw
N = 3  # Max re-tries
code_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(
    fallbacks=[fallback_chain] * N, exception_key="error"
)


def parse_output(solution):
    """When we add 'include_raw=True' to structured output,
    it will return a dict w 'raw', 'parsed', 'parsing_error'."""

    return solution["parsed"]


# Optional: With re-try to correct for failure to invoke tool
code_gen_chain = code_gen_chain_re_try | parse_output

# No re-try
code_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output
```

**ä»£ç è§£è¯»ï¼š**

| ç»„ä»¶ | ä½œç”¨ |
|------|------|
| `check_claude_output` | æ£€æŸ¥ Claude æ˜¯å¦æ­£ç¡®è°ƒç”¨äº†å·¥å…· |
| `insert_errors` | å°†é”™è¯¯ä¿¡æ¯æ’å…¥æ¶ˆæ¯ä¸­ç”¨äºé‡è¯• |
| `with_fallbacks` | è®¾ç½®é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯• 3 æ¬¡ |
| `parse_output` | ä»ç»“æ„åŒ–è¾“å‡ºä¸­æå–è§£æåçš„ç»“æœ |

### 4.4 æµ‹è¯• Claude ç”Ÿæˆ

```python
# Test
question = "How do I build a RAG chain in LCEL?"
solution = code_gen_chain.invoke(
    {"context": concatenated_content, "messages": [("user", question)]}
)
solution
```

**è¾“å‡ºç»“æœï¼š**

```
code(prefix="To build a RAG (Retrieval Augmented Generation) chain in LCEL, you can use a retriever to fetch relevant documents and then pass those documents to a chat model to generate a response based on the retrieved context. Here's an example of how to do this:", imports='from langchain_expressions import retrieve, chat_completion', code='question = "What is the capital of France?"\n\nrelevant_docs = retrieve(question)\n\nresult = chat_completion(\n    model=\'openai-gpt35\', \n    messages=[\n        {{{"role": "system", "content": "Answer the question based on the retrieved context.}}},\n        {{{"role": "user", "content": \'\'\'\n            Context: {relevant_docs}\n            Question: {question}\n        \'\'\'}}\n    ]\n)\n\nprint(result)')
```

---

## äº”ã€å®šä¹‰å›¾çŠ¶æ€

çŠ¶æ€æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä»£ç ç”Ÿæˆç›¸å…³çš„é”®ï¼ˆé”™è¯¯ã€é—®é¢˜ã€ä»£ç ç”Ÿæˆç»“æœï¼‰ï¼š

```python
from typing import List
from typing_extensions import TypedDict


class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        error : Binary flag for control flow to indicate whether test error was tripped
        messages : With user question, error messages, reasoning
        generation : Code solution
        iterations : Number of tries
    """

    error: str
    messages: List
    generation: str
    iterations: int
```

**çŠ¶æ€å­—æ®µè¯´æ˜ï¼š**

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `error` | str | äºŒå…ƒæ ‡å¿—ï¼Œè¡¨ç¤ºæ˜¯å¦è§¦å‘äº†æµ‹è¯•é”™è¯¯ |
| `messages` | List | åŒ…å«ç”¨æˆ·é—®é¢˜ã€é”™è¯¯æ¶ˆæ¯ã€æ¨ç†è¿‡ç¨‹ |
| `generation` | str | ä»£ç è§£å†³æ–¹æ¡ˆ |
| `iterations` | int | å°è¯•æ¬¡æ•° |

---

## å…­ã€æ„å»ºå›¾

### 6.1 å®šä¹‰å‚æ•°å’ŒèŠ‚ç‚¹

```python
### Parameter

# Max tries
max_iterations = 3
# Reflect
# flag = 'reflect'
flag = "do not reflect"

### Nodes


def generate(state: GraphState):
    """
    Generate a code solution

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation
    """

    print("---GENERATING CODE SOLUTION---")

    # State
    messages = state["messages"]
    iterations = state["iterations"]
    error = state["error"]

    # We have been routed back to generation with an error
    if error == "yes":
        messages += [
            (
                "user",
                "Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:",
            )
        ]

    # Solution
    code_solution = code_gen_chain.invoke(
        {"context": concatenated_content, "messages": messages}
    )
    messages += [
        (
            "assistant",
            f"{code_solution.prefix} \n Imports: {code_solution.imports} \n Code: {code_solution.code}",
        )
    ]

    # Increment
    iterations = iterations + 1
    return {"generation": code_solution, "messages": messages, "iterations": iterations}


def code_check(state: GraphState):
    """
    Check code

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, error
    """

    print("---CHECKING CODE---")

    # State
    messages = state["messages"]
    code_solution = state["generation"]
    iterations = state["iterations"]

    # Get solution components
    imports = code_solution.imports
    code = code_solution.code

    # Check imports
    try:
        exec(imports)
    except Exception as e:
        print("---CODE IMPORT CHECK: FAILED---")
        error_message = [("user", f"Your solution failed the import test: {e}")]
        messages += error_message
        return {
            "generation": code_solution,
            "messages": messages,
            "iterations": iterations,
            "error": "yes",
        }

    # Check execution
    try:
        exec(imports + "\n" + code)
    except Exception as e:
        print("---CODE BLOCK CHECK: FAILED---")
        error_message = [("user", f"Your solution failed the code execution test: {e}")]
        messages += error_message
        return {
            "generation": code_solution,
            "messages": messages,
            "iterations": iterations,
            "error": "yes",
        }

    # No errors
    print("---NO CODE TEST FAILURES---")
    return {
        "generation": code_solution,
        "messages": messages,
        "iterations": iterations,
        "error": "no",
    }


def reflect(state: GraphState):
    """
    Reflect on errors

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation
    """

    print("---GENERATING CODE SOLUTION---")

    # State
    messages = state["messages"]
    iterations = state["iterations"]
    code_solution = state["generation"]

    # Prompt reflection

    # Add reflection
    reflections = code_gen_chain.invoke(
        {"context": concatenated_content, "messages": messages}
    )
    messages += [("assistant", f"Here are reflections on the error: {reflections}")]
    return {"generation": code_solution, "messages": messages, "iterations": iterations}


### Edges


def decide_to_finish(state: GraphState):
    """
    Determines whether to finish.

    Args:
        state (dict): The current graph state

    Returns:
        str: Next node to call
    """
    error = state["error"]
    iterations = state["iterations"]

    if error == "no" or iterations == max_iterations:
        print("---DECISION: FINISH---")
        return "end"
    else:
        print("---DECISION: RE-TRY SOLUTION---")
        if flag == "reflect":
            return "reflect"
        else:
            return "generate"
```

**èŠ‚ç‚¹åŠŸèƒ½è§£è¯»ï¼š**

| èŠ‚ç‚¹ | åŠŸèƒ½ |
|------|------|
| `generate` | ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆ |
| `code_check` | æ£€æŸ¥ä»£ç ï¼ˆå¯¼å…¥æµ‹è¯• + æ‰§è¡Œæµ‹è¯•ï¼‰ |
| `reflect` | åæ€é”™è¯¯ï¼ˆå¯é€‰ï¼‰ |
| `decide_to_finish` | å†³å®šæ˜¯å¦å®Œæˆæˆ–é‡è¯• |

**ä»£ç æ£€æŸ¥çš„ä¸¤ä¸ªé˜¶æ®µï¼š**

1. **å¯¼å…¥æ£€æŸ¥**ï¼šä½¿ç”¨ `exec(imports)` éªŒè¯å¯¼å…¥è¯­å¥æ˜¯å¦æœ‰æ•ˆ
2. **æ‰§è¡Œæ£€æŸ¥**ï¼šä½¿ç”¨ `exec(imports + "\n" + code)` éªŒè¯å®Œæ•´ä»£ç æ˜¯å¦å¯æ‰§è¡Œ

### 6.2 æ„å»ºå›¾ç»“æ„

```python
from langgraph.graph import END, StateGraph, START

workflow = StateGraph(GraphState)

# Define the nodes
workflow.add_node("generate", generate)  # generation solution
workflow.add_node("check_code", code_check)  # check code
workflow.add_node("reflect", reflect)  # reflect

# Build graph
workflow.add_edge(START, "generate")
workflow.add_edge("generate", "check_code")
workflow.add_conditional_edges(
    "check_code",
    decide_to_finish,
    {
        "end": END,
        "reflect": "reflect",
        "generate": "generate",
    },
)
workflow.add_edge("reflect", "generate")
app = workflow.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))
```

**å›¾ç»“æ„è§£è¯»ï¼š**

```
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                         â”‚
          â–¼                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   generate    â”‚â”€â”€â”€â”€â–ºâ”‚ check_code  â”‚â”€â”€â”€â”€â–ºâ”‚ decide_to_finishâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â–²                                         â”‚
          â”‚                                         â”‚
          â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   reflect   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          (if flag == "reflect")
```

---

## ä¸ƒã€è¿è¡Œæµ‹è¯•

```python
question = "How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?"
solution = app.invoke({"messages": [("user", question)], "iterations": 0, "error": ""})
```

**è¿è¡Œè¾“å‡ºï¼š**

```
---GENERATING CODE SOLUTION---
---CHECKING CODE---
---CODE IMPORT CHECK: FAILED---
---DECISION: RE-TRY SOLUTION---
---GENERATING CODE SOLUTION---
---CHECKING CODE---
---CODE IMPORT CHECK: FAILED---
---DECISION: RE-TRY SOLUTION---
---GENERATING CODE SOLUTION---
---CHECKING CODE---
---CODE BLOCK CHECK: FAILED---
---DECISION: FINISH---
```

**æŸ¥çœ‹æœ€ç»ˆç”Ÿæˆç»“æœï¼š**

```python
solution["generation"]
```

**è¾“å‡ºï¼š**

```
code(prefix='To directly pass a string to a runnable and use it to construct the input needed for a prompt, you can use the `_from_value` method on a PromptTemplate in LCEL. Create a PromptTemplate with the desired template string, then call `_from_value` on it with a dictionary mapping the input variable names to their values. This will return a PromptValue that you can pass directly to any chain or model that accepts a prompt input.', imports='from langchain_core.prompts import PromptTemplate', code='user_string = "langchain is awesome"\n\nprompt_template = PromptTemplate.from_template("Tell me more about how {user_input}.")\n\nprompt_value = prompt_template._from_value({"user_input": user_string})\n\n# Pass the PromptValue directly to a model or chain \nchain.run(prompt_value)')
```

**è¿è¡Œåˆ†æï¼š**

å¯ä»¥çœ‹åˆ°ç³»ç»Ÿè¿›è¡Œäº† 3 æ¬¡å°è¯•ï¼š
1. ç¬¬ 1 æ¬¡ï¼šå¯¼å…¥æ£€æŸ¥å¤±è´¥
2. ç¬¬ 2 æ¬¡ï¼šå¯¼å…¥æ£€æŸ¥å¤±è´¥
3. ç¬¬ 3 æ¬¡ï¼šä»£ç æ‰§è¡Œæ£€æŸ¥å¤±è´¥ï¼Œä½†è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç»“æŸ

è¿™å±•ç¤ºäº†è‡ªæˆ‘çº æ­£æœºåˆ¶çš„å·¥ä½œè¿‡ç¨‹â€”â€”ç³»ç»Ÿä¼šå°è¯•å¤šæ¬¡ä¿®å¤ä»£ç ä¸­çš„é—®é¢˜ã€‚

---

## å…«ã€è¯„ä¼°ï¼ˆEvalï¼‰

### 8.1 æ•°æ®é›†

[è¿™é‡Œ](https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d) æ˜¯ä¸€ä¸ªå…¬å¼€çš„ LCEL é—®é¢˜æ•°æ®é›†ã€‚

æ•°æ®é›†åç§°ä¸º `lcel-teacher-eval`ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨ [GitHub](https://github.com/langchain-ai/lcel-teacher/blob/main/eval/eval.csv) æ‰¾åˆ° CSV æ–‡ä»¶ã€‚

```python
import langsmith

client = langsmith.Client()
```

```python
# Clone the dataset to your tenant to use it
try:
    public_dataset = (
        "https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d"
    )
    client.clone_public_dataset(public_dataset)
except:
    print("Please setup LangSmith")
```

**è¾“å‡ºï¼š**

```
Dataset(name='lcel-teacher-eval', description='Eval set for LCEL teacher', data_type=<DataType.kv: 'kv'>, id=UUID('8b57696d-14ea-4f00-9997-b3fc74a16846'), created_at=datetime.datetime(2024, 9, 16, 22, 50, 4, 169288, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 9, 16, 22, 50, 4, 169288, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None)
```

### 8.2 è‡ªå®šä¹‰è¯„ä¼°å™¨

```python
from langsmith.schemas import Example, Run


def check_import(run: Run, example: Example) -> dict:
    imports = run.outputs.get("imports")
    try:
        exec(imports)
        return {"key": "import_check", "score": 1}
    except Exception:
        return {"key": "import_check", "score": 0}


def check_execution(run: Run, example: Example) -> dict:
    imports = run.outputs.get("imports")
    code = run.outputs.get("code")
    try:
        exec(imports + "\n" + code)
        return {"key": "code_execution_check", "score": 1}
    except Exception:
        return {"key": "code_execution_check", "score": 0}
```

**è¯„ä¼°å™¨è¯´æ˜ï¼š**

| è¯„ä¼°å™¨ | æ£€æŸ¥å†…å®¹ | è¯„åˆ† |
|--------|---------|------|
| `check_import` | å¯¼å…¥è¯­å¥æ˜¯å¦æœ‰æ•ˆ | 1ï¼ˆé€šè¿‡ï¼‰/ 0ï¼ˆå¤±è´¥ï¼‰ |
| `check_execution` | å®Œæ•´ä»£ç æ˜¯å¦å¯æ‰§è¡Œ | 1ï¼ˆé€šè¿‡ï¼‰/ 0ï¼ˆå¤±è´¥ï¼‰ |

### 8.3 å¯¹æ¯”å®éªŒ

å¯¹æ¯” LangGraphï¼ˆå¸¦é‡è¯•å¾ªç¯ï¼‰å’Œ Context Stuffingï¼ˆåŸºç¡€æ–¹æ¡ˆï¼‰ï¼š

```python
def predict_base_case(example: dict):
    """Context stuffing"""
    solution = code_gen_chain.invoke(
        {"context": concatenated_content, "messages": [("user", example["question"])]}
    )
    return {"imports": solution.imports, "code": solution.code}


def predict_langgraph(example: dict):
    """LangGraph"""
    graph = app.invoke(
        {"messages": [("user", example["question"])], "iterations": 0, "error": ""}
    )
    solution = graph["generation"]
    return {"imports": solution.imports, "code": solution.code}
```

```python
from langsmith.evaluation import evaluate

# Evaluator
code_evalulator = [check_import, check_execution]

# Dataset
dataset_name = "lcel-teacher-eval"
```

**è¿è¡ŒåŸºç¡€æ–¹æ¡ˆè¯„ä¼°ï¼š**

```python
# Run base case
try:
    experiment_results_ = evaluate(
        predict_base_case,
        data=dataset_name,
        evaluators=code_evalulator,
        experiment_prefix=f"test-without-langgraph-{expt_llm}",
        max_concurrency=2,
        metadata={
            "llm": expt_llm,
        },
    )
except:
    print("Please setup LangSmith")
```

**è¿è¡Œ LangGraph æ–¹æ¡ˆè¯„ä¼°ï¼š**

```python
# Run with langgraph
try:
    experiment_results = evaluate(
        predict_langgraph,
        data=dataset_name,
        evaluators=code_evalulator,
        experiment_prefix=f"test-with-langgraph-{expt_llm}-{flag}",
        max_concurrency=2,
        metadata={
            "llm": expt_llm,
            "feedback": flag,
        },
    )
except:
    print("Please setup LangSmith")
```

---

## ä¹ã€å®éªŒç»“æœ

è¯„ä¼°ç»“æœå¯ä»¥åœ¨ LangSmith ä¸ŠæŸ¥çœ‹ï¼šhttps://smith.langchain.com/public/78a3d858-c811-4e46-91cb-0f10ef56260b/d

### å…³é”®å‘ç°ï¼š

| å‘ç° | è¯´æ˜ |
|------|------|
| **LangGraph ä¼˜äºåŸºç¡€æ–¹æ¡ˆ** | æ·»åŠ é‡è¯•å¾ªç¯èƒ½æ˜¾è‘—æå‡æ€§èƒ½ |
| **åæ€æœºåˆ¶æ²¡æœ‰å¸®åŠ©** | åœ¨é‡è¯•å‰è¿›è¡Œåæ€åè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼Œä¸å¦‚ç›´æ¥å°†é”™è¯¯ä¼ å› LLM |
| **GPT-4 ä¼˜äº Claude3** | Claude3 æœ‰å·¥å…·è°ƒç”¨é”™è¯¯é—®é¢˜ï¼ˆOpus æœ‰ 3 æ¬¡å¤±è´¥ï¼ŒHaiku æœ‰ 1 æ¬¡ï¼‰ |

---

## åã€è¿™ä¸ªè®¾è®¡çš„ç²¾å¦™ä¹‹å¤„

### 10.1 ç»“æ„åŒ–è¾“å‡º + åˆ†ç¦»æµ‹è¯•

å°†ä»£ç è¾“å‡ºåˆ†æˆä¸‰éƒ¨åˆ†ï¼ˆprefixã€importsã€codeï¼‰çš„è®¾è®¡éå¸¸å·§å¦™ï¼š

- **prefix**ï¼šè®© LLM å…ˆæè¿°æ–¹æ¡ˆï¼Œæœ‰åŠ©äºç†æ¸…æ€è·¯
- **imports**ï¼šå•ç‹¬åˆ†ç¦»ä¾¿äºç‹¬ç«‹æµ‹è¯•
- **code**ï¼šä¸»ä½“ä»£ç ï¼Œä¸å¯¼å…¥åˆ†å¼€æµ‹è¯•

### 10.2 è¿­ä»£è‡ªæˆ‘çº æ­£

```
ç”Ÿæˆä»£ç  â†’ æµ‹è¯•å¤±è´¥ â†’ é”™è¯¯ä¿¡æ¯åé¦ˆ â†’ é‡æ–°ç”Ÿæˆ â†’ å†æ¬¡æµ‹è¯•...
```

è¿™ä¸ªå¾ªç¯æ¨¡ä»¿äº†äººç±»ç¨‹åºå‘˜çš„å·¥ä½œæ–¹å¼ï¼š
1. å†™ä»£ç 
2. è¿è¡Œ/æµ‹è¯•
3. æŸ¥çœ‹é”™è¯¯
4. ä¿®å¤
5. é‡å¤ç›´åˆ°é€šè¿‡

### 10.3 æ¸è¿›å¼æ£€æŸ¥

å…ˆæ£€æŸ¥å¯¼å…¥ï¼Œå†æ£€æŸ¥æ‰§è¡Œã€‚è¿™æ ·å¯ä»¥æ›´ç²¾ç¡®åœ°å®šä½é—®é¢˜ï¼š

- å¯¼å…¥å¤±è´¥ â†’ åº“åé”™è¯¯æˆ–ä¸å­˜åœ¨
- æ‰§è¡Œå¤±è´¥ â†’ é€»è¾‘é”™è¯¯æˆ–è¯­æ³•é—®é¢˜

### 10.4 å¯é…ç½®çš„åæ€æœºåˆ¶

é€šè¿‡ `flag` å‚æ•°å¯ä»¥é€‰æ‹©æ˜¯å¦åœ¨é‡è¯•å‰è¿›è¡Œåæ€ã€‚å®éªŒè¡¨æ˜ï¼Œå¯¹äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œç›´æ¥é‡è¯•æ¯”åæ€æ›´æœ‰æ•ˆã€‚

---

## åä¸€ã€å®æˆ˜æ‰©å±•

### 11.1 å¢åŠ æµ‹è¯•ç”¨ä¾‹æ£€æŸ¥

```python
def code_check_with_tests(state: GraphState):
    """Check code with unit tests"""

    code_solution = state["generation"]
    imports = code_solution.imports
    code = code_solution.code

    # åŸºç¡€æ£€æŸ¥
    try:
        exec(imports)
    except Exception as e:
        return {**state, "error": "yes", "messages": state["messages"] + [
            ("user", f"Import error: {e}")
        ]}

    # æ·»åŠ å•å…ƒæµ‹è¯•
    test_cases = [
        "assert 'langchain' in dir()",  # æ£€æŸ¥æ˜¯å¦å¯¼å…¥äº† langchain
        "assert callable(chain.invoke)",  # æ£€æŸ¥ chain æ˜¯å¦æœ‰ invoke æ–¹æ³•
    ]

    for test in test_cases:
        try:
            exec(imports + "\n" + code + "\n" + test)
        except Exception as e:
            return {**state, "error": "yes", "messages": state["messages"] + [
                ("user", f"Test failed: {test}, Error: {e}")
            ]}

    return {**state, "error": "no"}
```

### 11.2 æ·»åŠ é™æ€ä»£ç åˆ†æ

```python
import ast

def static_analysis(code_str: str) -> list:
    """Perform static analysis on code"""
    warnings = []

    try:
        tree = ast.parse(code_str)

        # æ£€æŸ¥æœªä½¿ç”¨çš„å¯¼å…¥
        # æ£€æŸ¥æœªå®šä¹‰çš„å˜é‡
        # æ£€æŸ¥æ½œåœ¨çš„ç±»å‹é”™è¯¯

    except SyntaxError as e:
        warnings.append(f"Syntax error: {e}")

    return warnings
```

### 11.3 æ”¯æŒæ›´å¤šè¯­è¨€

```python
class MultiLangCode(BaseModel):
    """Schema for multi-language code solutions"""

    language: str = Field(description="Programming language")
    prefix: str = Field(description="Description of the solution")
    dependencies: str = Field(description="Package dependencies")
    code: str = Field(description="Main code block")
    test_code: str = Field(description="Test code to verify the solution")
```

---

## åäºŒã€æ€»ç»“

æœ¬æ¡ˆä¾‹å±•ç¤ºäº† LangGraph çš„ä¸€ä¸ªé‡è¦è®¾è®¡æ¨¡å¼ï¼š**å¸¦è‡ªæˆ‘çº æ­£çš„ä»£ç ç”Ÿæˆ**ã€‚

| è¦ç‚¹ | è¯´æ˜ |
|------|------|
| **ç»“æ„åŒ–è¾“å‡º** | å°†ä»£ç åˆ†æˆ prefix/imports/code ä¸‰éƒ¨åˆ† |
| **è¿­ä»£çº æ­£** | é€šè¿‡æµ‹è¯•-å¤±è´¥-é‡è¯•å¾ªç¯æ”¹è¿›ä»£ç  |
| **æ¸è¿›å¼æ£€æŸ¥** | å…ˆæ£€æŸ¥å¯¼å…¥ï¼Œå†æ£€æŸ¥æ‰§è¡Œ |
| **å¯é…ç½®åæ€** | å¯é€‰æ‹©æ˜¯å¦åœ¨é‡è¯•å‰è¿›è¡Œåæ€ |

è¿™ç§æ¨¡å¼é€‚ç”¨äºï¼š
- ä»£ç ç”Ÿæˆå’Œè‡ªåŠ¨ä¿®å¤
- SQL æŸ¥è¯¢ç”Ÿæˆ
- é…ç½®æ–‡ä»¶ç”Ÿæˆ
- ä»»ä½•éœ€è¦éªŒè¯è¾“å‡ºæ­£ç¡®æ€§çš„ç”Ÿæˆä»»åŠ¡

æ ¸å¿ƒæ€æƒ³ï¼š**ä¸è¦æœŸæœ›ä¸€æ¬¡ç”Ÿæˆæ­£ç¡®ç­”æ¡ˆï¼Œè€Œæ˜¯é€šè¿‡æµ‹è¯•å’Œè¿­ä»£é€æ­¥æ”¹è¿›ã€‚**

---

## å‚è€ƒèµ„æ–™

- [AlphaCodium è®ºæ–‡](https://github.com/Codium-ai/AlphaCodium)
- [Andrej Karpathy çš„æ¨æ–‡](https://x.com/karpathy/status/1748043513156272416?s=20)
- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)
- [åŸå§‹ Notebook](https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/code_assistant/langgraph_code_assistant.ipynb)
- [è¯„ä¼°ç»“æœ](https://smith.langchain.com/public/78a3d858-c811-4e46-91cb-0f10ef56260b/d)

