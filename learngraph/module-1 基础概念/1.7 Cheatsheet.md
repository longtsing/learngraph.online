# 1.7 LangGraph Cheatsheet

> **2é¡µæµ“ç¼©ç²¾å** Â· æ ¸å¿ƒæ¦‚å¿µ + ä»£ç æ¨¡æ¿ + æœ€ä½³å®è·µ

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„é€ŸæŸ¥

### åŸºç¡€ç»“æ„

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

# 1. å®šä¹‰çŠ¶æ€
from typing import TypedDict, Annotated
import operator

class State(TypedDict):
    messages: Annotated[list, operator.add]  # Reducer
    context: str

# 2. å®šä¹‰èŠ‚ç‚¹
def node_a(state: State) -> State:
    return {"messages": [{"role": "ai", "content": "å¤„ç†A"}]}

# 3. æ„å»ºå›¾
graph = StateGraph(State)
graph.add_node("a", node_a)
graph.add_edge(START, "a")
graph.add_edge("a", END)

# 4. ç¼–è¯‘
app = graph.compile(checkpointer=MemorySaver())

# 5. æ‰§è¡Œ
result = app.invoke({"messages": []}, config={"configurable": {"thread_id": "1"}})
```

### æ¡ä»¶è·¯ç”±

```python
def router(state: State) -> str:
    if state.get("needs_tool"):
        return "tools"
    return "end"

graph.add_conditional_edges("agent", router, {"tools": "tools", "end": END})
```

---

## ğŸ”§ å·¥å…·é›†æˆé€ŸæŸ¥

### Tool + ToolNode

```python
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode, tools_condition

# å®šä¹‰å·¥å…·
@tool
def search(query: str) -> str:
    """æœç´¢å·¥å…·,æŸ¥è¯¢äº’è”ç½‘ä¿¡æ¯"""
    return f"æœç´¢ç»“æœ: {query}"

# ç»‘å®šåˆ°LLM
tools = [search]
llm_with_tools = llm.bind_tools(tools)

# èŠ‚ç‚¹å®šä¹‰
def agent(state):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# æ·»åŠ åˆ°å›¾
graph.add_node("agent", agent)
graph.add_node("tools", ToolNode(tools))
graph.add_conditional_edges("agent", tools_condition)  # è‡ªåŠ¨è·¯ç”±
graph.add_edge("tools", "agent")  # å¾ªç¯å›agent
```

### ReAct Agent å®Œæ•´æ¨¡æ¿

```python
from langgraph.graph import MessagesState
from langgraph.prebuilt import create_react_agent

# ä¸€è¡Œåˆ›å»ºReAct Agent
agent = create_react_agent(llm, tools, checkpointer=MemorySaver())

# ä½¿ç”¨
for chunk in agent.stream(
    {"messages": [("user", "æœç´¢LangGraphæ•™ç¨‹")]},
    config={"configurable": {"thread_id": "user_123"}}
):
    print(chunk)
```

---

## ğŸ’¾ è®°å¿†ç³»ç»Ÿé€ŸæŸ¥

### Checkpointer(çŸ­æœŸè®°å¿†)

```python
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver

# å†…å­˜ç‰ˆ(å¼€å‘)
memory = MemorySaver()

# æŒä¹…åŒ–ç‰ˆ(ç”Ÿäº§)
memory = SqliteSaver.from_conn_string("checkpoints.db")

app = graph.compile(checkpointer=memory)

# æŒ‰thread_idåŒºåˆ†ä¼šè¯
config = {"configurable": {"thread_id": "user_123"}}
app.invoke(input, config)
```

### Store(é•¿æœŸè®°å¿†)

```python
from langgraph.store.memory import InMemoryStore

store = InMemoryStore()

# å­˜å‚¨
store.put(("users", "user_123"), "profile", {"name": "Alice", "age": 30})

# è¯»å–
profile = store.get(("users", "user_123"), "profile")

# æœç´¢
results = store.search(("users",), filter={"age": {"$gte": 25}})

# é›†æˆåˆ°å›¾
app = graph.compile(checkpointer=memory, store=store)
```

---

## ğŸ”„ çŠ¶æ€ç®¡ç†é€ŸæŸ¥

### å¸¸ç”¨ Reducers

```python
import operator
from typing import Annotated
from langgraph.graph import add_messages

# åŠ æ³•åˆå¹¶(æ•°å­—ã€åˆ—è¡¨)
count: Annotated[int, operator.add]

# æ¶ˆæ¯æ™ºèƒ½åˆå¹¶(æ¨è)
messages: Annotated[list, add_messages]

# è¦†ç›–(é»˜è®¤è¡Œä¸º)
status: str

# è‡ªå®šä¹‰Reducer
def merge_dict(left, right):
    return {**left, **right}

data: Annotated[dict, merge_dict]
```

### æ¶ˆæ¯æ“ä½œ

```python
from langgraph.graph import MessagesState, RemoveMessage
from langgraph.prebuilt import trim_messages, filter_messages

# è£å‰ªæ¶ˆæ¯(ä¿ç•™æœ€è¿‘Næ¡)
trimmed = trim_messages(state["messages"], max_messages=10)

# è¿‡æ»¤æ¶ˆæ¯
filtered = filter_messages(state["messages"], include_types=["human", "ai"])

# åˆ é™¤æ¶ˆæ¯
return {"messages": [RemoveMessage(id=msg.id)]}
```

---

## ğŸ›ï¸ äººæœºååŒé€ŸæŸ¥

### Breakpoints

```python
# ç¼–è¯‘æ—¶è®¾ç½®æ–­ç‚¹
app = graph.compile(
    checkpointer=memory,
    interrupt_before=["human_review"],  # èŠ‚ç‚¹å‰æš‚åœ
    interrupt_after=["agent"]           # èŠ‚ç‚¹åæš‚åœ
)

# æŸ¥çœ‹çŠ¶æ€
state = app.get_state(config)
print(state.values)  # å½“å‰çŠ¶æ€
print(state.next)    # ä¸‹ä¸€æ­¥èŠ‚ç‚¹

# æ›´æ–°çŠ¶æ€åç»§ç»­
app.update_state(config, {"approved": True})
app.invoke(None, config)  # ç»§ç»­æ‰§è¡Œ
```

### åŠ¨æ€ä¸­æ–­

```python
from langgraph.types import interrupt

def review_node(state):
    if needs_human_review(state):
        # è¯·æ±‚äººå·¥å®¡æ ¸,é™„å¸¦ä¿¡æ¯
        decision = interrupt({"data": state["draft"], "question": "æ˜¯å¦æ‰¹å‡†?"})
        return {"approved": decision}
    return {"approved": True}
```

---

## ğŸš€ é«˜çº§æ¨¡å¼é€ŸæŸ¥

### Parallelization(å¹¶è¡Œ)

```python
# æ–¹å¼1:å¤šä¸ªæ™®é€šè¾¹(è‡ªåŠ¨å¹¶è¡Œ)
graph.add_edge(START, "node_a")
graph.add_edge(START, "node_b")  # node_aå’Œnode_bå¹¶è¡Œæ‰§è¡Œ

# æ–¹å¼2:Send API(åŠ¨æ€å¹¶è¡Œ)
from langgraph.types import Send

def fan_out(state):
    return [Send("worker", {"task": t}) for t in state["tasks"]]

graph.add_conditional_edges(START, fan_out)
```

### Subgraph(å­å›¾)

```python
# å®šä¹‰å­å›¾
sub_graph = StateGraph(SubState)
sub_graph.add_node("step1", sub_node1)
sub_graph.add_edge("step1", END)
sub_app = sub_graph.compile()

# åµŒå…¥ä¸»å›¾
graph.add_node("subprocess", sub_app)
```

### Map-Reduce

```python
from langgraph.types import Send

def map_phase(state):
    # åˆ†å‘ä»»åŠ¡
    return [Send("worker", {"item": item}) for item in state["items"]]

def reduce_phase(state):
    # æ±‡æ€»ç»“æœ(è‡ªåŠ¨è°ƒç”¨,å› ä¸ºreducer)
    return {"summary": combine(state["results"])}

graph.add_conditional_edges("map", map_phase)
graph.add_node("worker", process_item)  # å¹¶è¡Œæ‰§è¡Œ
graph.add_edge("worker", "reduce")       # è‡ªåŠ¨æ”¶é›†
```

---

## ğŸ¨ æ¶ˆæ¯ç³»ç»Ÿé€ŸæŸ¥

### æ¶ˆæ¯ç±»å‹

```python
from langchain_core.messages import (
    HumanMessage,    # ç”¨æˆ·è¾“å…¥
    AIMessage,       # AIå›å¤(å«tool_calls)
    SystemMessage,   # ç³»ç»ŸæŒ‡ä»¤
    ToolMessage      # å·¥å…·ç»“æœ
)

messages = [
    SystemMessage(content="ä½ æ˜¯åŠ©æ‰‹"),
    HumanMessage(content="æŸ¥å¤©æ°”"),
    AIMessage(content="", tool_calls=[{
        "name": "search", "args": {"q": "å¤©æ°”"}
    }]),
    ToolMessage(content="æ™´å¤©", tool_call_id="call_123")
]
```

---

## ğŸ“¦ ç”Ÿäº§éƒ¨ç½²é€ŸæŸ¥

### LangGraph Platform

```python
# langgraph.json
{
  "dependencies": ["langchain-openai", "tavily-python"],
  "graphs": {
    "agent": "./agent.py:graph"
  },
  "env": ".env"
}

# agent.py
from langgraph.graph import StateGraph
graph = StateGraph(State)
# ... å®šä¹‰å›¾ ...
graph = graph.compile(checkpointer=...)  # å¿…é¡»èµ‹å€¼ç»™graphå˜é‡
```

### LangGraph SDK

```python
from langgraph_sdk import get_client

# è¿æ¥è¿œç¨‹
client = get_client(url="http://localhost:8123")

# è°ƒç”¨
thread = await client.threads.create()
async for chunk in client.runs.stream(
    thread["thread_id"],
    "agent",  # assistantåç§°
    input={"messages": [{"role": "user", "content": "ä½ å¥½"}]}
):
    print(chunk)
```

### Double Textingç­–ç•¥

```python
# é…ç½®multitask_strategy
app = graph.compile(
    checkpointer=memory,
    multitask_strategy="reject"  # reject/enqueue/interrupt/rollback
)
```

---

## ğŸ¯ æœ€ä½³å®è·µæ¸…å•

### âœ… å¼€å‘é˜¶æ®µ

- [ ] ä½¿ç”¨ `MemorySaver` å¿«é€ŸåŸå‹
- [ ] ç”¨ `LangGraph Studio` å¯è§†åŒ–è°ƒè¯•
- [ ] æ¯ä¸ªèŠ‚ç‚¹æ‰“å°æ—¥å¿—ä¾¿äºè¿½è¸ª
- [ ] å®šä¹‰æ¸…æ™°çš„ State Schema(TypedDict)
- [ ] å·¥å…· docstring è¦è¯¦ç»†(LLMä¾èµ–å®ƒ)

### âœ… ç”Ÿäº§éƒ¨ç½²

- [ ] åˆ‡æ¢åˆ° `SqliteSaver` æˆ– `PostgresSaver`
- [ ] è®¾ç½® `recursion_limit` é˜²æ­¢æ— é™å¾ªç¯
- [ ] å®ç°å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- [ ] ä½¿ç”¨ `interrupt()` å¤„ç†æ•æ„Ÿæ“ä½œ
- [ ] é…ç½® Double Texting ç­–ç•¥
- [ ] ç›‘æ§ token ä½¿ç”¨å’Œæˆæœ¬

### âœ… æ€§èƒ½ä¼˜åŒ–

- [ ] ä½¿ç”¨ Parallelization æå‡é€Ÿåº¦
- [ ] `trim_messages` æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
- [ ] å·¥å…·æ”¯æŒ `parallel_tool_calls`
- [ ] å¼‚æ­¥æ‰§è¡Œ(`astream`)æå‡åå
- [ ] Store ä½¿ç”¨ç´¢å¼•åŠ é€ŸæŸ¥è¯¢

---

## ğŸ“š æ ¸å¿ƒAPIé€ŸæŸ¥

| æ“ä½œ | æ–¹æ³• | è¯´æ˜ |
|------|------|------|
| åŒæ­¥æ‰§è¡Œ | `app.invoke(input, config)` | è¿è¡Œåˆ°ç»“æŸ |
| æµå¼æ‰§è¡Œ | `app.stream(input, config)` | é€æ­¥è¿”å› |
| å¼‚æ­¥æµå¼ | `await app.astream(input, config)` | å¼‚æ­¥ç‰ˆ |
| è·å–çŠ¶æ€ | `app.get_state(config)` | æŸ¥çœ‹å½“å‰çŠ¶æ€ |
| æ›´æ–°çŠ¶æ€ | `app.update_state(config, values)` | ç¼–è¾‘çŠ¶æ€ |
| å†å²è®°å½• | `app.get_state_history(config)` | Time Travel |

---

## ğŸ”— å…³é”®å¯¼å…¥é€ŸæŸ¥

```python
# æ ¸å¿ƒ
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.graph import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.store.memory import InMemoryStore

# å·¥å…·
from langgraph.prebuilt import ToolNode, tools_condition, create_react_agent

# äººæœºååŒ
from langgraph.types import interrupt, Command
from langgraph.errors import NodeInterrupt

# é«˜çº§
from langgraph.types import Send
from langgraph.prebuilt import trim_messages, filter_messages

# æ¶ˆæ¯
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
```

---

## ğŸ’¡ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool

# 1. å®šä¹‰å·¥å…·
@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼,å¦‚'2+2'"""
    return str(eval(expression))

# 2. åˆ›å»ºAgent(ä¸€è¡Œä»£ç )
llm = ChatOpenAI(model="gpt-4o-mini")
agent = create_react_agent(llm, [calculator], checkpointer=MemorySaver())

# 3. è¿è¡Œ
config = {"configurable": {"thread_id": "1"}}
for msg in agent.stream({"messages": [("user", "è®¡ç®—25*4")]}, config):
    print(msg)
```

---

**æŸ¥é˜…æç¤º**:
- ğŸ” æœç´¢å…³é”®è¯å¿«é€Ÿå®šä½
- ğŸ“‹ å¤åˆ¶ä»£ç æ¨¡æ¿ç›´æ¥ä½¿ç”¨
- ğŸ¯ å¯¹æ¯”"é€ŸæŸ¥"å’Œ"è¯¦ç»†æ•™ç¨‹"åŠ æ·±ç†è§£

**æœ¬ Cheatsheet è¦†ç›– LangGraph 90% çš„å¸¸ç”¨åœºæ™¯**
