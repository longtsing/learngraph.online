# LangChain å¿«é€Ÿå›é¡¾

>  ç½‘ç«™ä½¿ç”¨è¯´æ˜
> - æœ¬ç½‘ç«™å¯ä»¥å…ç™»é™†è¿è¡Œ Python ä»£ç 
> - Python ä»£ç å¯ä»¥ç¼–è¾‘å¹¶ä¸´æ—¶ä¿å­˜ï¼Œä½†ä¸ä¼šæ°¸ä¹…ä¿å­˜ï¼Œç½‘é¡µåˆ·æ–°åä¼šè‡ªåŠ¨è¿˜åŸ
> - å¯¹ç½‘ç«™çš„ä½¿ç”¨æœ‰ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥åˆ° [é—®é¢˜åé¦ˆ](http://localhost:5173/feedback.html) ï¼ˆæŒ‰é’®åœ¨æ¯ä¸ªé¡µé¢çš„å³ä¸‹è§’ï¼‰å…ç™»å½•è¿›è¡Œè¯„è®º
> - è¿è¡Œ `LangGraph/LangChain`ä»£ç ï¼Œéœ€è¦ç”¨æˆ·è¾“å…¥è‡ªå·±çš„ [API Key](http://localhost:5173/python-run.html)
> - é‡è¦å£°æ˜ï¼šæœ¬ç½‘ç«™ä¸ä¼šä¿å­˜ç”¨æˆ·çš„ API Key æ•°æ®ï¼Œè¯·æ”¾å¿ƒè¾“å…¥


---

## ğŸ“š æœ¯è¯­è¡¨

| æœ¯è¯­åç§° | LangGraph å®šä¹‰å’Œè§£è¯» | Python å®šä¹‰å’Œè¯´æ˜ | é‡è¦ç¨‹åº¦ |
|---------|---------------------|------------------|---------|
| **Chat Model** | èŠå¤©æ¨¡å‹ï¼ŒLangChain å¯¹ LLM API çš„å°è£…ï¼Œæ”¯æŒå¤šç§æä¾›å•† | Python ç±»(å¦‚ ChatOpenAI)ï¼Œå®ç° invoke/stream/batch ç­‰æ–¹æ³• | â­â­â­â­â­ |
| **Prompt Template** | æç¤ºè¯æ¨¡æ¿ï¼Œç”¨äºç»“æ„åŒ–ç®¡ç†æç¤ºè¯å’ŒåŠ¨æ€å˜é‡æ›¿æ¢ | ChatPromptTemplate ç±»ï¼Œé€šè¿‡ from_messages() åˆ›å»ºæ¨¡æ¿ | â­â­â­â­â­ |
| **LCEL** | LangChain Expression Languageï¼Œé“¾å¼ç»„åˆè¯­æ³•ï¼Œä½¿ç”¨ç®¡é“ç¬¦è¿æ¥ç»„ä»¶ | é€šè¿‡é‡è½½ | è¿ç®—ç¬¦å®ç°çš„ç»„ä»¶è¿æ¥è¯­æ³• | â­â­â­â­â­ |
| **Tool** | å·¥å…·ï¼Œé€šè¿‡ @tool è£…é¥°å™¨å®šä¹‰ï¼Œæ‰©å±• LLM çš„èƒ½åŠ› | Python å‡½æ•° + è£…é¥°å™¨ï¼ŒåŒ…å«æè¿°ã€å‚æ•° schema å’Œæ‰§è¡Œé€»è¾‘ | â­â­â­â­â­ |
| **Output Parser** | è¾“å‡ºè§£æå™¨ï¼Œå°† LLM çš„æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ® | StrOutputParserã€JsonOutputParserã€PydanticOutputParser ç­‰ç±» | â­â­â­â­ |
| **Message** | æ¶ˆæ¯å¯¹è±¡ï¼Œè¡¨ç¤ºå¯¹è¯ä¸­çš„ä¸åŒè§’è‰²å’Œå†…å®¹ | HumanMessageã€AIMessageã€SystemMessageã€ToolMessage ç­‰ç±» | â­â­â­â­â­ |
| **Runnable** | å¯è¿è¡Œæ¥å£ï¼Œæ‰€æœ‰ LangChain ç»„ä»¶å®ç°çš„ç»Ÿä¸€æ¥å£ | æŠ½è±¡æ¥å£ï¼Œå®šä¹‰ invoke/stream/batch/ainvoke ç­‰æ–¹æ³• | â­â­â­â­â­ |
| **Retriever** | æ£€ç´¢å™¨ï¼Œç”¨äºä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ | å®ç°æ£€ç´¢æ¥å£çš„ç±»ï¼Œé€šè¿‡ similarity_search() æŸ¥è¯¢ | â­â­â­â­ |
| **RAG** | Retrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯ | ç»“åˆæ£€ç´¢å™¨å’Œ LLM çš„å·¥ä½œæµæ¨¡å¼ï¼Œå…ˆæ£€ç´¢åç”Ÿæˆ | â­â­â­â­â­ |
| **Vector Store** | å‘é‡å­˜å‚¨ï¼Œä¿å­˜æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºå¹¶æ”¯æŒç›¸ä¼¼åº¦æœç´¢ | FAISSã€Chroma ç­‰ç±»ï¼Œæä¾› from_documents() å’Œæœç´¢æ–¹æ³• | â­â­â­â­ |
| **Embedding** | åµŒå…¥æ¨¡å‹ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º | OpenAIEmbeddings ç­‰ç±»ï¼Œå®ç° embed_query() å’Œ embed_documents() | â­â­â­â­ |
| **Pydantic** | Python æ•°æ®éªŒè¯åº“ï¼Œç”¨äºå®šä¹‰ç»“æ„åŒ–çš„æ•°æ®æ¨¡å‹ | BaseModel ç±»ï¼Œé€šè¿‡ç±»å‹æ³¨è§£å®šä¹‰å­—æ®µå’ŒéªŒè¯è§„åˆ™ | â­â­â­â­ |

---

## 1. LangChain æ˜¯ LangGraph çš„åŸºç¡€

å¦‚æœä½ æ­£åœ¨å­¦ä¹  LangGraphï¼Œä½ å¯èƒ½ä¼šå‘ç°å¾ˆå¤šä»£ç ç¤ºä¾‹ä¸­éƒ½é¢‘ç¹å‡ºç° LangChain çš„å„ç§ç»„ä»¶ã€‚å…³äºè¿™äº›ç»„é”®ï¼Œæˆ‘ä»¬åé¢ä¹Ÿä¼šè¿›è¡Œé€šä¿—è§£é‡Šï¼Œåå¤å­¦ä¹ ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.tools.tavily_search import TavilySearchResults
```

**è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿç®€å•æ¥è¯´ï¼š**

> **LangChain æ˜¯å·¥å…·ç®±ï¼ŒLangGraph æ˜¯æ–½å·¥å›¾**
>
> - **LangChain** æä¾›äº†ä¸ LLM äº¤äº’çš„å„ç§"é›¶ä»¶"ï¼ˆChat Modelsã€Toolsã€Prompts ç­‰ï¼‰
> - **LangGraph** æä¾›äº†ç¼–æ’è¿™äº›é›¶ä»¶çš„"è“å›¾"ï¼ˆStateGraphã€èŠ‚ç‚¹ã€æ¡ä»¶è¾¹ç­‰ï¼‰

### ç±»æ¯”ç†è§£

- **ç›–æˆ¿å­éœ€è¦ä»€ä¹ˆï¼Ÿ**
  - ç –å¤´ã€æ°´æ³¥ã€é’¢ç­‹ã€é—¨çª— â†’ è¿™æ˜¯ LangChain æä¾›çš„ç»„ä»¶
  - å»ºç­‘å›¾çº¸ã€æ–½å·¥æµç¨‹ã€è´¨é‡æ£€æŸ¥ â†’ è¿™æ˜¯ LangGraph æä¾›çš„**ç¼–æ’**èƒ½åŠ›

**æœ¬ç« ç›®æ ‡**ï¼šé€šè¿‡æœ¬ç« ï¼Œä½ å°†æ·±å…¥ç†è§£ LangChain çš„ 7 å¤§æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶æ˜ç™½å®ƒä»¬å¦‚ä½•åœ¨ LangGraph ä¸­å‘æŒ¥ä½œç”¨ã€‚

---

## ğŸ“Š LangChain ä¸ LangGraph çš„æŠ€æœ¯æ¶æ„

```

                    æ„å»º AI Agent

                          â†“

                   LangGraph å±‚

  StateGraphï¼ˆçŠ¶æ€å›¾ï¼‰   Agent ç¼–æ’
  èŠ‚ç‚¹     æ¡ä»¶è¾¹ï¼å¾ªç¯      å¯¹è¯æµ


                          â†“ è°ƒç”¨

                   LangChain å±‚

  Chat    Tools   Prompt  Parser  Memory
  Models  ï¼ˆå·¥å…·ï¼‰  ï¼ˆæç¤ºï¼‰  ï¼ˆè§£æï¼‰  ï¼ˆè®°å¿†ï¼‰


                          â†“ è°ƒç”¨

                    åº•å±‚ API
          OpenAI API / Anthropic API / æœ¬åœ°æ¨¡å‹

```

---

## ğŸ“š LangChain 7 å¤§æ ¸å¿ƒç»„ä»¶æ¦‚è§ˆ

### 2.1 Chat Modelsï¼ˆèŠå¤©æ¨¡å‹ï¼‰- LLM è°ƒç”¨çš„åŸºç¡€

Chat Models æ˜¯ LangChain æœ€æ ¸å¿ƒçš„ç»„ä»¶ï¼Œå®ƒå°è£…äº†ä¸å„ç§ LLMï¼ˆå¦‚ GPT-4ã€Claudeï¼‰çš„äº¤äº’æ¥å£ã€‚

#### åŸºç¡€è°ƒç”¨

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")

from langchain_openai import ChatOpenAI

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-5-nano",
    temperature=0.7,  # æ§åˆ¶éšæœºæ€§ï¼š0-2ï¼Œè¶Šé«˜è¶Šéšæœº
    max_tokens=100    # é™åˆ¶è¾“å‡ºé•¿åº¦
)

# è°ƒç”¨æ¨¡å‹
response = llm.invoke("è¯·ç”¨ä¸€å¥è¯è§£é‡Š LangChain")
print(response.content)
```

**è¿è¡Œç»“æœ**ï¼š
```
LangChain æ˜¯ä¸€ä¸ªå¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œç®€åŒ–äº† LLM é›†æˆå’Œå·¥ä½œæµæ„å»ºã€‚
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šå‡½æ•°å‚æ•°ä¸é»˜è®¤å€¼

```python
# temperature=0.7 æ˜¯"å…³é”®å­—å‚æ•°"ï¼ˆkeyword argumentï¼‰
# å¦‚æœä¸æŒ‡å®šï¼Œä¼šä½¿ç”¨é»˜è®¤å€¼
llm = ChatOpenAI(model="gpt-5-nano")  # temperature ä½¿ç”¨é»˜è®¤å€¼ 0.7
```

#### æ¶ˆæ¯å¯¹è±¡

LangChain ä½¿ç”¨ç»“æ„åŒ–çš„æ¶ˆæ¯å¯¹è±¡ï¼Œè€Œä¸æ˜¯ç®€å•å­—ç¬¦ä¸²ï¼š

```python
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½èµ„æ·± Python ä¸“å®¶"),
    HumanMessage(content="å¦‚ä½•åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Ÿ"),
]

response = llm.invoke(messages)
print(response.content)
```

**è¿è¡Œç»“æœ**ï¼š
```
åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒçš„æ­¥éª¤å¦‚ä¸‹ï¼š
1. è¿è¡Œå‘½ä»¤ `python -m venv myenv`
2. æ¿€æ´»ç¯å¢ƒï¼š
   - Windows: `myenv\Scripts\activate`
   - macOS/Linux: `source myenv/bin/activate`
3. ä½¿ç”¨ `pip install -r requirements.txt` å®‰è£…ä¾èµ–
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šç±»ä¸å¯¹è±¡

```python
# SystemMessage æ˜¯ä¸€ä¸ª"ç±»"ï¼ˆClassï¼‰ï¼Œcontent æ˜¯å®ƒçš„"å±æ€§"ï¼ˆAttributeï¼‰
msg = SystemMessage(content="ä½ å¥½")

# å¯ä»¥é€šè¿‡ msg.content è®¿é—®å†…å®¹
print(msg.content)  # è¾“å‡ºï¼šä½ å¥½
print(msg.type)     # è¾“å‡ºï¼šsystem
```

#### æµå¼è¾“å‡º

å½“å¤„ç†é•¿æ–‡æœ¬ç”Ÿæˆæ—¶ï¼Œæµå¼è¾“å‡ºèƒ½æä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒï¼š

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-5-nano", streaming=True)

print("æµå¼è¾“å‡ºï¼š", end="")
for chunk in llm.stream("è¯·å†™ä¸€é¦–å…³äº AI çš„å°è¯—ï¼ˆç®€çŸ­ï¼‰"):
    print(chunk.content, end="", flush=True)
print()
```

**è¿è¡Œç»“æœ**ï¼š
```
æµå¼è¾“å‡ºï¼šæ™ºèƒ½æ¶ŒåŠ¨å¦‚æ½®
ç®—æ³•ç¼–ç»‡å¥‡å¦™
æ•°æ®æµ·æ´‹æ¢ç´¢
æœªæ¥ç”±æ­¤å¼€åˆ›
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰

```python
# llm.stream() è¿”å›çš„æ˜¯ä¸€ä¸ª"ç”Ÿæˆå™¨"ï¼Œå¯ä»¥ç”¨ for å¾ªç¯é€ä¸ªè·å–ç»“æœ
# è¿™æ¯”ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰ç»“æœæ›´èŠ‚çœå†…å­˜

def my_generator():
    yield 1
    yield 2
    yield 3

for num in my_generator():
    print(num)  # è¾“å‡º 1, 2, 3ï¼ˆé€ä¸ªè¾“å‡ºï¼Œä¸æ˜¯ä¸€æ¬¡æ€§ï¼‰
```

#### å¤šæ¨¡å‹æ”¯æŒ

LangChain æ”¯æŒ 30+ ç§æ¨¡å‹æä¾›å•†ã€‚æœ¬æ•™ç¨‹é»˜è®¤ä½¿ç”¨ OpenAI æ¨¡å‹ï¼š

```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-5-nano")
```

> ğŸ’¡ å¦‚éœ€ä½¿ç”¨å…¶ä»–æ¨¡å‹æä¾›å•†ï¼ˆAnthropicã€DeepSeek ç­‰ï¼‰ï¼Œè¯·å‚è€ƒ [API Key é…ç½®è¯´æ˜](./0.-1-ç½‘ç«™ä½¿ç”¨è¯´æ˜.md#å¦‚ä½•è¿è¡Œä»£ç ç¤ºä¾‹)

#### âš¡ åœ¨ LangGraph ä¸­çš„åº”ç”¨ï¼ˆé‡è¦ï¼‰

```python
from langgraph.graph import StateGraph, MessagesState

def chatbot_node(state: MessagesState):
    # ç›´æ¥ä½¿ç”¨ LangChain çš„ Chat Model
    llm = ChatOpenAI(model="gpt-5-nano")
    return {"messages": [llm.invoke(state["messages"])]}
```

**å…³é”®ç‚¹**ï¼š
- LangGraph çš„èŠ‚ç‚¹å‡½æ•°ä¸­ï¼Œé€šå¸¸ä½¿ç”¨ LangChain çš„ Chat Model æ¥è°ƒç”¨ LLM
- `MessagesState` æ˜¯ LangGraph æä¾›çš„ä¾¿æ·çŠ¶æ€ç±»å‹ï¼Œå†…éƒ¨ä½¿ç”¨ LangChain çš„æ¶ˆæ¯æ ¼å¼

---

### 2.2 Promptsï¼ˆæç¤ºè¯æ¨¡æ¿ï¼‰- ç»“æ„åŒ–æç¤ºè¯ç®¡ç†

æç¤ºè¯æ¨¡æ¿å¸®åŠ©ä½ ä¼˜é›…åœ°ç»„ç»‡å¤æ‚çš„æç¤ºè¯ï¼Œé¿å…æ‰‹åŠ¨æ‹¼æ¥å­—ç¬¦ä¸²ã€‚

#### åŸºç¡€æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# åˆ›å»ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½èµ„æ·± {role} ä¸“å®¶"),
    ("human", "{question}")
])

# ç»„åˆæˆé“¾
llm = ChatOpenAI(model="gpt-5-nano")
chain = prompt | llm

response = chain.invoke({
    "role": "ç®—æ³•å·¥ç¨‹å¸ˆ",
    "question": "ä»€ä¹ˆæ˜¯äºŒå‰æ ‘ï¼Ÿ"
})
print(response.content)
```

**è¿è¡Œç»“æœ**ï¼š
```
äºŒå‰æ ‘æ˜¯ä¸€ç§åŸºç¡€æ•°æ®ç»“æ„ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šæœ‰ä¸¤ä¸ªå­èŠ‚ç‚¹ï¼šå·¦å­èŠ‚ç‚¹å’Œå³å­èŠ‚ç‚¹ã€‚
å¸¸è§åº”ç”¨åŒ…æ‹¬ï¼š
1. äºŒå‰æœç´¢æ ‘ï¼ˆBSTï¼‰ï¼šç”¨äºé«˜æ•ˆæŸ¥æ‰¾
2. å †ï¼šç”¨äºä¼˜å…ˆé˜Ÿåˆ—å®ç°
3. è¡¨è¾¾å¼æ ‘ï¼šç”¨äºç¼–è¯‘å™¨æ„å»ºæŠ½è±¡è¯­æ³•æ ‘
4. çº¢é»‘æ ‘ï¼šè‡ªå¹³è¡¡äºŒå‰æœç´¢æ ‘ï¼Œä¿è¯ O(log n) æ€§èƒ½
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šå­—å…¸ä¸æ ¼å¼åŒ–

```python
# {role} å’Œ {question} æ˜¯"å ä½ç¬¦"ï¼Œä¼šè¢«å­—å…¸ä¸­çš„å€¼æ›¿æ¢
template = "ä½ å¥½ï¼Œ{name}ï¼"
result = template.format(name="å¼ ä¸‰")  # è¾“å‡ºï¼šä½ å¥½ï¼Œå¼ ä¸‰ï¼

# ä¹Ÿå¯ä»¥ç”¨ f-stringï¼ˆPython 3.6+ï¼‰
name = "æå››"
result = f"ä½ å¥½ï¼Œ{name}ï¼"  # è¾“å‡ºï¼šä½ å¥½ï¼Œæå››ï¼
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šç®¡é“æ“ä½œç¬¦ï¼ˆ|ï¼‰

```python
# prompt | llm è¿™ç§å†™æ³•å«åš LCELï¼ˆLangChain Expression Languageï¼‰
# | æ˜¯ Python çš„"æŒ‰ä½æˆ–"è¿ç®—ç¬¦ï¼ŒLangChain é‡è½½äº†å®ƒï¼Œç”¨äºè¿æ¥ç»„ä»¶

# ç­‰ä»·äºï¼š
chain = prompt | llm
# å®é™…æ‰§è¡Œæ—¶ï¼šå…ˆè¿è¡Œ promptï¼Œå†å°†ç»“æœä¼ ç»™ llm

result = chain.invoke({"role": "ä¸“å®¶", "question": "é—®é¢˜"})
```

#### Few-Shot æç¤ºæ¨¡æ¿

Few-Shot Learning é€šè¿‡æä¾›ç¤ºä¾‹ï¼Œè®© LLM æ›´å¥½åœ°ç†è§£ä»»åŠ¡ï¼š

```python
from langchain_core.prompts import FewShotChatMessagePromptTemplate

# å®šä¹‰ç¤ºä¾‹
examples = [
    {"input": "å¼€å¿ƒ", "output": "ğŸ˜Š"},
    {"input": "éš¾è¿‡", "output": "ğŸ˜¢"},
    {"input": "æ„¤æ€’", "output": "ğŸ˜ "},
]

# åˆ›å»º Few-Shot æ¨¡æ¿
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),
    ("ai", "{output}"),
])

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

# å®Œæ•´æç¤ºè¯
final_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ éœ€è¦å°†æƒ…ç»ªè¯è½¬æ¢ä¸ºå¯¹åº”çš„ emoji"),
    few_shot_prompt,
    ("human", "{input}"),
])

chain = final_prompt | llm
response = chain.invoke({"input": "å…´å¥‹"})
print(response.content)
```

**è¿è¡Œç»“æœ**ï¼š
```
ğŸ¤©
```

#### æ¨¡æ¿å˜é‡å¤ç”¨

```python
from langchain_core.prompts import ChatPromptTemplate

# æ”¯æŒå¤šä¸ªå˜é‡
template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ {company} çš„å®¢æœä»£è¡¨ï¼Œè§’è‰²æ˜¯ {role}"),
    ("human", "è¯·ç”¨ {tone} çš„è¯­æ°”å›ç­”ï¼š{question}")
])

chain = template | llm
response = chain.invoke({
    "company": "OpenAI",
    "role": "æŠ€æœ¯æ”¯æŒ",
    "tone": "ä¸“ä¸šå‹å¥½",
    "question": "API è°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ"
})
print(response.content)
```

**è¿è¡Œç»“æœ**ï¼š
```
éå¸¸æŠ±æ­‰æ‚¨é‡åˆ°äº† API è°ƒç”¨å¤±è´¥çš„é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„å¸®åŠ©æ‚¨è§£å†³ã€‚

1. **æ£€æŸ¥ç½‘ç»œè¿æ¥**ï¼šè¯·ç¡®è®¤æ‚¨çš„ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®šï¼Œé˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†è¯·æ±‚
2. **æŸ¥çœ‹é”™è¯¯è¯¦æƒ…**ï¼šè¯·æä¾›å…·ä½“çš„é”™è¯¯ä»£ç ï¼ˆå¦‚ timeout æˆ–çŠ¶æ€ç ï¼‰
3. **æ£€æŸ¥è¯·æ±‚å‚æ•°**ï¼šç¡®è®¤æ‰€æœ‰å¿…éœ€å‚æ•°éƒ½å·²æ­£ç¡®ä¼ é€’ï¼Œæ•°æ®æ ¼å¼ç¬¦åˆè¦æ±‚
4. **æŸ¥çœ‹æœåŠ¡çŠ¶æ€**ï¼šè®¿é—® status.openai.com æŸ¥çœ‹ API æ˜¯å¦æ­£å¸¸è¿è¡Œ

å¦‚æœé—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œæ¬¢è¿æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œæˆ‘ä¼šè¿›ä¸€æ­¥ååŠ©æ‚¨ã€‚
```

#### âš¡ åœ¨ LangGraph ä¸­çš„åº”ç”¨ï¼ˆé‡è¦ï¼‰

```python
from langgraph.graph import StateGraph, MessagesState

def agent_node(state: MessagesState):
    # ä½¿ç”¨æç¤ºè¯æ¨¡æ¿ä¸º Agent è®¾è®¡ç³»ç»Ÿæç¤ºè¯
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿä½¿ç”¨å·¥å…·çš„ ReAct Agentï¼Œè¯·éµå¾ª Thought-Action-Observation æµç¨‹"),
        ("placeholder", "{messages}")  # å ä½ç¬¦ç”¨äºæ’å…¥å†å²æ¶ˆæ¯
    ])

    llm = ChatOpenAI(model="gpt-5-nano")
    chain = prompt | llm

    response = chain.invoke({"messages": state["messages"]})
    return {"messages": [response]}
```

**å…³é”®ç‚¹**ï¼š
- æç¤ºè¯æ¨¡æ¿åœ¨ LangGraph èŠ‚ç‚¹ä¸­ç”¨äºè®¾è®¡ Agent çš„"æ€§æ ¼"å’Œ"èƒ½åŠ›"
- `placeholder` ç±»å‹çš„æ¶ˆæ¯ç”¨äºåŠ¨æ€æ’å…¥å†å²å¯¹è¯

---

### 2.3 Output Parsersï¼ˆè¾“å‡ºè§£æå™¨ï¼‰- ç»“æ„åŒ– LLM è¾“å‡º

LLM çš„åŸå§‹è¾“å‡ºæ˜¯æ–‡æœ¬ï¼ŒOutput Parsers å¸®åŠ©æˆ‘ä»¬å°†æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®ï¼ˆå­—ç¬¦ä¸²ã€JSONã€Python å¯¹è±¡ï¼‰ã€‚

#### StrOutputParserï¼ˆå­—ç¬¦ä¸²è§£æï¼‰

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_messages([
    ("human", "è¯·å†™ä¸€é¦–å…³äº {topic} çš„è¯— ")
])

llm = ChatOpenAI(model="gpt-5-nano")
output_parser = StrOutputParser()

# ä½¿ç”¨ LCEL ä¸²è”ç»„ä»¶
chain = prompt | llm | output_parser

result = chain.invoke({"topic": "ç§‹å¤©"})
print(type(result))  # <class 'str'>
print(result)
```

**è¿è¡Œç»“æœ**ï¼š
```
<class 'str'>
é‡‘é£é€çˆ½å¶é£˜é»„
ç¨»è°·é£˜é¦™æ»¡ç”°åº„
è½æ—¥ä½™æ™–æŸ“å±±å²—
ç§‹æ„æµ“æµ“å…¥æ¢¦ä¹¡
```

#### JsonOutputParserï¼ˆJSON è§£æï¼‰

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate

# å®šä¹‰è¾“å‡ºç»“æ„
parser = JsonOutputParser()

prompt = ChatPromptTemplate.from_messages([
    ("system", "è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œå¿…é¡»åŒ…å« 'title' å’Œ 'summary' å­—æ®µ"),
    ("human", "æ€»ç»“è¿™æ®µæ–‡ç« ï¼š{article}")
])

llm = ChatOpenAI(model="gpt-5-nano")
chain = prompt | llm | parser

result = chain.invoke({
    "article": "LangChain æ˜¯ä¸€ä¸ªå¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†æ¨¡å‹è°ƒç”¨ã€æç¤ºè¯å·¥ç¨‹ã€å·¥å…·é›†æˆç­‰æ ¸å¿ƒåŠŸèƒ½"
})

print(type(result))  # <class 'dict'>
print(result)
```

**è¿è¡Œç»“æœ**ï¼š
```
<class 'dict'>
{
    'title': 'LangChain æ¡†æ¶ç®€ä»‹',
    'summary': 'LangChain æ˜¯ä¸“ä¸ºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›è®¾è®¡çš„æ¡†æ¶ï¼Œé›†æˆäº†æ¨¡å‹è°ƒç”¨ã€æç¤ºè¯ç®¡ç†ã€å·¥å…·æ¥å£ç­‰å…³é”®ç»„ä»¶'
}
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šå­—å…¸ï¼ˆDictionaryï¼‰

```python
# å­—å…¸æ˜¯ Python çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œä½¿ç”¨ {} æˆ– dict() åˆ›å»º
person = {
    "name": "å¼ ä¸‰",
    "age": 30,
    "city": "åŒ—äº¬"
}

# è®¿é—®å­—å…¸å…ƒç´ 
print(person["name"])  # è¾“å‡ºï¼šå¼ ä¸‰
print(person.get("age"))  # è¾“å‡ºï¼š30ï¼ˆæ¨èç”¨æ³•ï¼Œä¸ä¼šæŠ›å¼‚å¸¸ï¼‰

# æ·»åŠ æˆ–ä¿®æ”¹å…ƒç´ 
person["email"] = "zhangsan@example.com"
```

#### PydanticOutputParserï¼ˆç»“æ„åŒ–è§£æï¼‰

Pydantic æ˜¯ Python çš„æ•°æ®éªŒè¯åº“ï¼Œèƒ½ç¡®ä¿ LLM è¾“å‡ºç¬¦åˆé¢„æœŸæ ¼å¼ï¼š

```python
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

# å®šä¹‰æ•°æ®æ¨¡å‹
class Person(BaseModel):
    name: str = Field(description="äººç‰©å§“å")
    age: int = Field(description="å¹´é¾„")
    occupation: str = Field(description="èŒä¸š")
    skills: list[str] = Field(description="æŠ€èƒ½åˆ—è¡¨")

parser = PydanticOutputParser(pydantic_object=Person)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ä»æ–‡æœ¬ä¸­æå–äººç‰©ä¿¡æ¯å¹¶è¾“å‡º\n{format_instructions}"),
    ("human", "{text}")
])

llm = ChatOpenAI(model="gpt-5-nano")
chain = prompt | llm | parser

result = chain.invoke({
    "text": "å°æ˜æ˜¯ä¸€ä½ 35 å²çš„èµ„æ·±ç®—æ³•å·¥ç¨‹å¸ˆï¼Œç²¾é€š Pythonã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ",
    "format_instructions": parser.get_format_instructions()
})

print(type(result))  # <class '__main__.Person'>
print(f"å§“åï¼š{result.name}")
print(f"å¹´é¾„ï¼š{result.age}")
print(f"èŒä¸šï¼š{result.occupation}")
print(f"æŠ€èƒ½ï¼š{', '.join(result.skills)}")
```

**è¿è¡Œç»“æœ**ï¼š
```
<class '__main__.Person'>
å§“åï¼šå°æ˜
å¹´é¾„ï¼š35
èŒä¸šï¼šç®—æ³•å·¥ç¨‹å¸ˆ
æŠ€èƒ½ï¼šPython, æœºå™¨å­¦ä¹ , æ·±åº¦å­¦ä¹ 
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šç±»å‹æ³¨è§£ä¸ Pydantic

```python
from pydantic import BaseModel

# ç±»å‹æ³¨è§£ï¼ˆType Annotationï¼‰å‘Šè¯‰ Python å˜é‡çš„ç±»å‹
name: str = "å¼ ä¸‰"  # name åº”è¯¥æ˜¯å­—ç¬¦ä¸²
age: int = 30       # age åº”è¯¥æ˜¯æ•´æ•°

# Pydantic çš„ BaseModel ä¼šåœ¨è¿è¡Œæ—¶éªŒè¯ç±»å‹
class User(BaseModel):
    name: str
    age: int

# æ­£ç¡®ï¼šç±»å‹åŒ¹é…
user = User(name="æå››", age=25)

# é”™è¯¯ï¼šç±»å‹ä¸åŒ¹é…ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
# user = User(name="ç‹äº”", age="ä¸‰å")  # ValidationError
```

#### âš¡ åœ¨ LangGraph ä¸­çš„åº”ç”¨ï¼ˆé‡è¦ï¼‰

```python
from langgraph.graph import StateGraph
from typing import TypedDict
from pydantic import BaseModel

class ExtractedInfo(BaseModel):
    intent: str
    entities: list[str]

class State(TypedDict):
    user_input: str
    parsed_data: ExtractedInfo

def parse_node(state: State):
    parser = PydanticOutputParser(pydantic_object=ExtractedInfo)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä»ç”¨æˆ·è¾“å…¥ä¸­æå–æ„å›¾å’Œå®ä½“\n{format_instructions}"),
        ("human", "{input}")
    ])

    llm = ChatOpenAI(model="gpt-5-nano")
    chain = prompt | llm | parser

    parsed = chain.invoke({
        "input": state["user_input"],
        "format_instructions": parser.get_format_instructions()
    })

    return {"parsed_data": parsed}
```

**å…³é”®ç‚¹**ï¼š
- Output Parsers åœ¨ LangGraph ä¸­ç”¨äºå°† LLM çš„æ–‡æœ¬è¾“å‡ºè½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
- ç»“æ„åŒ–æ•°æ®å¯ä»¥å­˜å‚¨åœ¨ State ä¸­ï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨

---

### 2.4 Toolsï¼ˆå·¥å…·ï¼‰- æ‰©å±• LLM èƒ½åŠ›

Tools è®© LLM èƒ½å¤Ÿ"è°ƒç”¨å¤–éƒ¨å‡½æ•°"ï¼Œä»è€Œæ‰§è¡Œæœç´¢ã€è®¡ç®—ã€API è°ƒç”¨ç­‰æ“ä½œã€‚

#### ä½¿ç”¨ @tool è£…é¥°å™¨å®šä¹‰å·¥å…·

```python
from langchain_core.tools import tool

@tool
def calculate(expression: str) -> str:
    """æ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼ˆå¦‚ '2+3*4'ï¼‰
    """
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœï¼š{result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯ï¼š{str(e)}"

@tool
def get_weather(city: str) -> str:
    """æŸ¥è¯¢åŸå¸‚å¤©æ°”ã€‚

    Args:
        city: åŸå¸‚åç§°ï¼ˆå¦‚ 'åŒ—äº¬'ï¼‰
    """
    # æ¨¡æ‹Ÿå¤©æ°”æŸ¥è¯¢
    weather_db = {
        "åŒ—äº¬": "æ™´å¤©ï¼Œ15Â°Cï¼Œç©ºæ°”è´¨é‡ä¼˜",
        "ä¸Šæµ·": "é˜´å¤©ï¼Œ18Â°Cï¼Œæ¹¿åº¦ 70%",
        "æ·±åœ³": "é›¨å¤©ï¼Œ22Â°Cï¼Œæœ‰é›·æš´"
    }
    return weather_db.get(city, f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ° {city} çš„å¤©æ°”ä¿¡æ¯")

# æŸ¥çœ‹å·¥å…·ä¿¡æ¯
print(f"å·¥å…·åï¼š{calculate.name}")
print(f"å·¥å…·æè¿°ï¼š{calculate.description}")
print(f"å‚æ•°ç»“æ„ï¼š{calculate.args}")
```

**è¿è¡Œç»“æœ**ï¼š
```
å·¥å…·åï¼šcalculate
å·¥å…·æè¿°ï¼šæ‰§è¡Œæ•°å­¦è®¡ç®—ã€‚

    Args:
        expression: æ•°å­¦è¡¨è¾¾å¼ï¼ˆå¦‚ '2+3*4'ï¼‰

å‚æ•°ç»“æ„ï¼š{'expression': {'title': 'Expression', 'type': 'string'}}
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šè£…é¥°å™¨ï¼ˆDecoratorï¼‰

```python
# è£…é¥°å™¨æ˜¯ Python çš„"è¯­æ³•ç³–"ï¼Œç”¨äºä¿®æ”¹å‡½æ•°è¡Œä¸º
# @tool ä¼šå°†æ™®é€šå‡½æ•°è½¬æ¢ä¸º LangChain å·¥å…·

# æ²¡æœ‰è£…é¥°å™¨çš„ç‰ˆæœ¬
def my_function():
    return "Hello"

# ä½¿ç”¨è£…é¥°å™¨
@tool
def my_function():
    """è¿™æ˜¯å·¥å…·æè¿°"""
    return "Hello"

# ç­‰ä»·äºï¼š
def my_function():
    """è¿™æ˜¯å·¥å…·æè¿°"""
    return "Hello"
my_function = tool(my_function)
```

#### å·¥å…·ç»‘å®šåˆ° LLM

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

tools = [calculate, get_weather]
llm = ChatOpenAI(model="gpt-5-nano")
llm_with_tools = llm.bind_tools(tools)

# LLM ä¼šè‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
response = llm_with_tools.invoke([
    HumanMessage(content="åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿå¦å¤–ï¼Œå¸®æˆ‘ç®—ä¸€ä¸‹  25 * 4 + 10")
])

print("LLM çš„å†³ç­–ï¼š")
if response.tool_calls:
    for tool_call in response.tool_calls:
        print(f"  - è°ƒç”¨å·¥å…·ï¼š{tool_call['name']}")
        print(f"    å‚æ•°ï¼š{tool_call['args']}")
else:
    print("  - ç›´æ¥æ–‡æœ¬å›ç­”ï¼ˆä¸è°ƒç”¨å·¥å…·ï¼‰")
```

**è¿è¡Œç»“æœ**ï¼š
```
LLM çš„å†³ç­–ï¼š
  - è°ƒç”¨å·¥å…·ï¼šget_weather
    å‚æ•°ï¼š{'city': 'åŒ—äº¬'}
  - è°ƒç”¨å·¥å…·ï¼šcalculate
    å‚æ•°ï¼š{'expression': '25 * 4 + 10'}
```

#### æ‰§è¡Œå·¥å…·è°ƒç”¨

```python
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage

# 1. ç”¨æˆ·æé—®
messages = [HumanMessage(content="æ·±åœ³å¤©æ°”å¦‚ä½•ï¼Ÿè®¡ç®— 100 / 5")]

# 2. LLM å†³ç­–è°ƒç”¨å·¥å…·
llm_with_tools = ChatOpenAI(model="gpt-5-nano").bind_tools(tools)
ai_message = llm_with_tools.invoke(messages)
messages.append(ai_message)

# 3. æ‰§è¡Œå·¥å…·è°ƒç”¨
for tool_call in ai_message.tool_calls:
    # æ‰¾åˆ°å¯¹åº”å·¥å…·
    tool_map = {tool.name: tool for tool in tools}
    selected_tool = tool_map[tool_call["name"]]

    # æ‰§è¡Œå·¥å…·
    tool_output = selected_tool.invoke(tool_call["args"])

    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
    messages.append(ToolMessage(
        content=tool_output,
        tool_call_id=tool_call["id"]
    ))

    print(f"å·¥å…· {tool_call['name']} è¾“å‡ºï¼š{tool_output}")

# 4. LLM æ ¹æ®å·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
final_response = llm_with_tools.invoke(messages)
print(f"\næœ€ç»ˆå›ç­”ï¼š{final_response.content}")
```

**è¿è¡Œç»“æœ**ï¼š
```
å·¥å…· get_weather è¾“å‡ºï¼šé›¨å¤©ï¼Œ22Â°Cï¼Œæœ‰é›·æš´
å·¥å…· calculate è¾“å‡ºï¼šè®¡ç®—ç»“æœï¼š20.0

æœ€ç»ˆå›ç­”ï¼šæ·±åœ³ä»Šå¤©çš„å¤©æ°”æ˜¯é›¨å¤©ï¼Œæ¸©åº¦ 22Â°Cï¼Œæœ‰é›·æš´ã€‚è®¡ç®—ç»“æœï¼š100 é™¤ä»¥ 5 ç­‰äº 20ã€‚
```

#### ä½¿ç”¨ StructuredTool å®šä¹‰å¤æ‚å·¥å…·

```python
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field

# å®šä¹‰è¾“å…¥å‚æ•°æ¨¡å‹
class SearchInput(BaseModel):
    query: str = Field(description="æœç´¢å…³é”®è¯")
    max_results: int = Field(default=5, description="æœ€å¤šè¿”å›ç»“æœæ•°")
    language: str = Field(default="zh", description="æœç´¢è¯­è¨€ä»£ç ")

def search_web(query: str, max_results: int = 5, language: str = "zh") -> str:
    """æ¨¡æ‹Ÿç½‘ç»œæœç´¢"""
    return f"æœç´¢ '{query}' æ‰¾åˆ° {max_results} æ¡ç»“æœï¼ˆè¯­è¨€ï¼š{language}ï¼‰"

# åˆ›å»ºç»“æ„åŒ–å·¥å…·
search_tool = StructuredTool.from_function(
    func=search_web,
    name="web_search",
    description="åœ¨äº’è”ç½‘ä¸Šæœç´¢ä¿¡æ¯",
    args_schema=SearchInput
)

# æµ‹è¯•å·¥å…·
result = search_tool.invoke({
    "query": "LangGraph æ•™ç¨‹",
    "max_results": 10,
    "language": "zh"
})
print(result)
```

**è¿è¡Œç»“æœ**ï¼š
```
æœç´¢ 'LangGraph æ•™ç¨‹' æ‰¾åˆ° 10 æ¡ç»“æœï¼ˆè¯­è¨€ï¼šzhï¼‰
```

#### âš¡ åœ¨ LangGraph ä¸­çš„åº”ç”¨ï¼ˆé‡è¦ï¼‰

```python
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, MessagesState, START, END

def agent(state: MessagesState):
    llm = ChatOpenAI(model="gpt-5-nano").bind_tools(tools)
    return {"messages": [llm.invoke(state["messages"])]}

def should_continue(state: MessagesState):
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END

# æ„å»ºå›¾
graph = StateGraph(MessagesState)
graph.add_node("agent", agent)
graph.add_node("tools", ToolNode(tools))  # LangGraph çš„ ToolNode ä¼šè‡ªåŠ¨æ‰§è¡Œå·¥å…·

graph.add_edge(START, "agent")
graph.add_conditional_edges("agent", should_continue, ["tools", END])
graph.add_edge("tools", "agent")

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))
```

**å…³é”®ç‚¹**ï¼š
- `ToolNode` æ˜¯ LangGraph æä¾›çš„ä¾¿æ·èŠ‚ç‚¹ï¼Œè‡ªåŠ¨å¤„ç†å·¥å…·è°ƒç”¨
- Agent èŠ‚ç‚¹è´Ÿè´£å†³ç­–ï¼ŒToolNode è´Ÿè´£æ‰§è¡Œ
- é€šè¿‡æ¡ä»¶è¾¹å®ç°"å†³ç­– â†’ å·¥å…·è°ƒç”¨ â†’ å†³ç­–"çš„å¾ªç¯

---

### 2.5 Messagesï¼ˆæ¶ˆæ¯å¯¹è±¡ï¼‰- å¯¹è¯å†å²ç®¡ç†

æ¶ˆæ¯å¯¹è±¡æ˜¯ LangChain æœ€åŸºç¡€çš„æ•°æ®ç»“æ„ï¼Œç”¨äºè¡¨ç¤º LLM çš„å¯¹è¯å†å²ã€‚

#### æ¶ˆæ¯ç±»å‹æ¦‚è§ˆ

```python
from langchain_core.messages import (
    SystemMessage,
    HumanMessage,
    AIMessage,
    ToolMessage,
    FunctionMessage
)

# 1. SystemMessageï¼ˆç³»ç»Ÿæ¶ˆæ¯ï¼‰ï¼šè®¾å®šè§’è‰²å’Œæç¤ºè¯
system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä½èµ„æ·± Python ä¸“å®¶")
print(f"System: {system_msg.content}")

# 2. HumanMessageï¼ˆç”¨æˆ·æ¶ˆæ¯ï¼‰ï¼šç”¨æˆ·çš„è¾“å…¥
human_msg = HumanMessage(content="å¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ")
print(f"Human: {human_msg.content}")

# 3. AIMessageï¼ˆAI æ¶ˆæ¯ï¼‰ï¼šAI çš„å›å¤
ai_msg = AIMessage(content="ä½¿ç”¨ open() å‡½æ•°å¯ä»¥è¯»å–æ–‡ä»¶")
print(f"AI: {ai_msg.content}")

# 4. ToolMessageï¼ˆå·¥å…·æ¶ˆæ¯ï¼‰ï¼šå·¥å…·çš„è¿”å›ç»“æœ
tool_msg = ToolMessage(
    content="æ–‡ä»¶å†…å®¹ï¼šHello World",
    tool_call_id="call_123"
)
print(f"Tool: {tool_msg.content}")
```

**è¿è¡Œç»“æœ**ï¼š
```
System: ä½ æ˜¯ä¸€ä½èµ„æ·± Python ä¸“å®¶
Human: å¦‚ä½•è¯»å–æ–‡ä»¶ï¼Ÿ
AI: ä½¿ç”¨ open() å‡½æ•°å¯ä»¥è¯»å–æ–‡ä»¶
Tool: æ–‡ä»¶å†…å®¹ï¼šHello World
```

#### æ¶ˆæ¯çš„ç‰¹æ®Šå±æ€§

```python
from langchain_core.messages import AIMessage

# åˆ›å»ºåŒ…å«å·¥å…·è°ƒç”¨çš„æ¶ˆæ¯
ai_message = AIMessage(
    content="æˆ‘éœ€è¦æŸ¥è¯¢å¤©æ°”",
    tool_calls=[{
        "name": "get_weather",
        "args": {"city": "åŒ—äº¬"},
        "id": "call_abc123"
    }]
)

print(f"å†…å®¹ï¼š{ai_message.content}")
print(f"å·¥å…·è°ƒç”¨ï¼š{ai_message.tool_calls}")
print(f"ç±»å‹ï¼š{ai_message.type}")
```

**è¿è¡Œç»“æœ**ï¼š
```
å†…å®¹ï¼šæˆ‘éœ€è¦æŸ¥è¯¢å¤©æ°”
å·¥å…·è°ƒç”¨ï¼š[{'name': 'get_weather', 'args': {'city': 'åŒ—äº¬'}, 'id': 'call_abc123'}]
ç±»å‹ï¼šai
```

#### æ¶ˆæ¯å†å²ç®¡ç†

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

llm = ChatOpenAI(model="gpt-5-nano")

# æ¨¡æ‹Ÿå¤šè½®å¯¹è¯
conversation = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å¹½é»˜çš„æ•°å­¦è€å¸ˆ"),
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="ä½ å¥½ï¼ä»Šå¤©æƒ³å­¦ç‚¹ä»€ä¹ˆæ•°å­¦çŸ¥è¯†å‘¢ï¼Ÿ"),
    HumanMessage(content="æˆ‘æƒ³å­¦å¾®ç§¯åˆ†")
]

response = llm.invoke(conversation)
print(response.content)
```

**è¿è¡Œç»“æœ**ï¼š
```
å¤ªå¥½äº†ï¼å¾®ç§¯åˆ†æ˜¯æ•°å­¦çš„ç²¾åã€‚è®©æˆ‘ä»¬ä»æœ€åŸºç¡€çš„æé™æ¦‚å¿µå¼€å§‹å§ã€‚
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šåˆ—è¡¨ï¼ˆListï¼‰

```python
# åˆ—è¡¨æ˜¯ Python ä¸­æœ€å¸¸ç”¨çš„æ•°æ®ç»“æ„ï¼Œç”¨ [] åˆ›å»º
messages = [
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="ä½ å¥½ï¼")
]

# æ·»åŠ å…ƒç´ 
messages.append(HumanMessage(content="å†è§"))

# è®¿é—®å…ƒç´ 
print(messages[0])   # ç¬¬ä¸€ä¸ªå…ƒç´ 
print(messages[-1])  # æœ€åä¸€ä¸ªå…ƒç´ 

# åˆ‡ç‰‡
print(messages[:2])  # å‰ä¸¤ä¸ªå…ƒç´ 
```

#### æ¶ˆæ¯åºåˆ—åŒ–

```python
from langchain_core.messages import HumanMessage, AIMessage
import json

# æ¶ˆæ¯å¯ä»¥è½¬æ¢ä¸ºå­—å…¸
messages = [
    HumanMessage(content="ä½ å¥½"),
    AIMessage(content="ä½ å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©çš„ï¼Ÿ")
]

# è½¬æ¢ä¸ºå­—å…¸
messages_dict = [msg.dict() for msg in messages]
print(json.dumps(messages_dict, ensure_ascii=False, indent=2))
```

**è¿è¡Œç»“æœ**ï¼š
```json
[
  {
    "content": "ä½ å¥½",
    "type": "human",
    "additional_kwargs": {},
    "example": false
  },
  {
    "content": "ä½ å¥½ï¼è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©çš„ï¼Ÿ",
    "type": "ai",
    "additional_kwargs": {},
    "example": false
  }
]
```

#### âš¡ åœ¨ LangGraph ä¸­çš„åº”ç”¨ï¼ˆé‡è¦ï¼‰

```python
from langgraph.graph import MessagesState

# MessagesState æ˜¯ LangGraph å†…ç½®çš„çŠ¶æ€ç±»å‹ï¼Œä¼šè‡ªåŠ¨ç®¡ç†æ¶ˆæ¯å†å²
class CustomState(MessagesState):
    user_name: str

def chatbot(state: CustomState):
    # è®¿é—®æ¶ˆæ¯å†å²
    messages = state["messages"]

    # å¯ä»¥æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿæ¶ˆæ¯
    if state.get("user_name"):
        system_msg = SystemMessage(content=f"ç”¨æˆ·åæ˜¯ {state['user_name']}")
        messages = [system_msg] + messages

    llm = ChatOpenAI(model="gpt-5-nano")
    response = llm.invoke(messages)

    return {"messages": [response]}
```

**å…³é”®ç‚¹**ï¼š
- `MessagesState` è‡ªåŠ¨ç´¯ç§¯æ¶ˆæ¯å†å²
- åœ¨ LangGraph èŠ‚ç‚¹ä¸­ï¼Œå¯ä»¥éšæ—¶è®¿é—®å’Œä¿®æ”¹æ¶ˆæ¯åˆ—è¡¨
- æ¶ˆæ¯ç±»å‹ï¼ˆSystem/Human/AI/Toolï¼‰å¸®åŠ© LLM ç†è§£å¯¹è¯ä¸Šä¸‹æ–‡

---

### 2.6 Retrievers & RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰- è®© LLM è®¿é—®å¤–éƒ¨çŸ¥è¯†

Retriever æ˜¯ RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè®© LLM èƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†åº“ã€‚

#### å‘é‡å­˜å‚¨åŸºç¡€

```python
import getpass
import os

def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")

_set_env("OPENAI_API_KEY")

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document

# å‡†å¤‡æ–‡æ¡£
documents = [
    Document(page_content="LangChain æ˜¯ä¸€ä¸ªå¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶", metadata={"source": "doc1"}),
    Document(page_content="LangGraph ç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„ Agent ç³»ç»Ÿ", metadata={"source": "doc2"}),
    Document(page_content="RAG æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œè®© LLM èƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†åº“", metadata={"source": "doc3"}),
    Document(page_content="å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤º", metadata={"source": "doc4"}),
]

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(documents, embeddings)

# ç›¸ä¼¼åº¦æœç´¢
query = "å¦‚ä½•æ„å»º Agent ç³»ç»Ÿ"
results = vectorstore.similarity_search(query, k=2)

print(f"æŸ¥è¯¢ï¼š{query}\n")
for i, doc in enumerate(results, 1):
    print(f"ç»“æœ {i}ï¼š{doc.page_content}")
    print(f"æ¥æºï¼š{doc.metadata['source']}\n")
```

**è¿è¡Œç»“æœ**ï¼š
```
æŸ¥è¯¢ï¼šå¦‚ä½•æ„å»º Agent ç³»ç»Ÿ

ç»“æœ 1ï¼šLangGraph ç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„ Agent ç³»ç»Ÿ
æ¥æºï¼šdoc2

ç»“æœ 2ï¼šLangChain æ˜¯ä¸€ä¸ªå¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶
æ¥æºï¼šdoc1
```

#### ğŸ” Python çŸ¥è¯†ç‚¹ï¼šEnumerate

```python
# enumerate() åœ¨éå†åˆ—è¡¨æ—¶åŒæ—¶è·å–ç´¢å¼•å’Œå…ƒç´ 
fruits = ["è‹¹æœ", "é¦™è•‰", "æ©™å­"]

for i, fruit in enumerate(fruits, 1):  # 1 è¡¨ç¤ºç´¢å¼•ä» 1 å¼€å§‹
    print(f"ç¬¬ {i} ä¸ªæ°´æœæ˜¯ {fruit}")

# è¾“å‡ºï¼š
# ç¬¬ 1 ä¸ªæ°´æœæ˜¯ è‹¹æœ
# ç¬¬ 2 ä¸ªæ°´æœæ˜¯ é¦™è•‰
# ç¬¬ 3 ä¸ªæ°´æœæ˜¯ æ©™å­
```

#### å¸¦åˆ†æ•°çš„æ£€ç´¢

```python
# ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦åˆ†æ•°ï¼‰
results_with_scores = vectorstore.similarity_search_with_score(query, k=3)

print(f"æŸ¥è¯¢ï¼š{query}\n")
for doc, score in results_with_scores:
    print(f"ç›¸ä¼¼åº¦ï¼š{score:.4f}")
    print(f"å†…å®¹ï¼š{doc.page_content}\n")
```

**è¿è¡Œç»“æœ**ï¼š
```
æŸ¥è¯¢ï¼šå¦‚ä½•æ„å»º Agent ç³»ç»Ÿ

ç›¸ä¼¼åº¦ï¼š0.2145
å†…å®¹ï¼šLangGraph ç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„ Agent ç³»ç»Ÿ

ç›¸ä¼¼åº¦ï¼š0.3267
å†…å®¹ï¼šLangChain æ˜¯ä¸€ä¸ªå¼ºåŒ–å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶

ç›¸ä¼¼åº¦ï¼š0.4521
å†…å®¹ï¼šRAG æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œè®© LLM èƒ½å¤Ÿè®¿é—®å¤–éƒ¨çŸ¥è¯†åº“
```

#### å®Œæ•´ RAG æµç¨‹

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. å‡†å¤‡çŸ¥è¯†åº“
documents = [
    Document(page_content="LangGraph æ”¯æŒå¾ªç¯å’Œæ¡ä»¶åˆ†æ”¯"),
    Document(page_content="ä½¿ç”¨ StateGraph å¯ä»¥å®šä¹‰çŠ¶æ€å›¾"),
    Document(page_content="ToolNode ç”¨äºè‡ªåŠ¨æ‰§è¡Œå·¥å…·è°ƒç”¨"),
    Document(page_content="MemorySaver å¯ä»¥ä¿å­˜å¯¹è¯å†å²"),
]

vectorstore = FAISS.from_documents(documents, OpenAIEmbeddings())
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 2. åˆ›å»º RAG é“¾
template = """è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)
llm = ChatOpenAI(model="gpt-5-nano")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# LCEL æ„å»ºé“¾
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 3. æŸ¥è¯¢
question = "å¦‚ä½•åœ¨ LangGraph ä¸­ä¿å­˜å¯¹è¯å†å²ï¼Ÿ"
answer = rag_chain.invoke(question)

print(f"é—®é¢˜ï¼š{question}\n")
print(f"å›ç­”ï¼š{answer}")
```

**è¿è¡Œç»“æœ**ï¼š
```
é—®é¢˜ï¼šå¦‚ä½•åœ¨ LangGraph ä¸­ä¿å­˜å¯¹è¯å†å²ï¼Ÿ

å›ç­”ï¼šåœ¨ LangGraph ä¸­å¯ä»¥ä½¿ç”¨ MemorySaver æ¥ä¿å­˜å¯¹è¯å†å²ã€‚MemorySaver æ˜¯ä¸€ä¸ªå†…å­˜æ£€æŸ¥ç‚¹ä¿å­˜å™¨ï¼Œ
èƒ½å¤Ÿä¿å­˜çŠ¶æ€å¿«ç…§ã€‚ä½¿ç”¨æ–¹æ³•æ˜¯åœ¨ç¼–è¯‘å›¾æ—¶ä¼ å…¥ checkpointer å‚æ•°ï¼š

app = graph.compile(checkpointer=MemorySaver())

åœ¨è°ƒç”¨æ—¶éœ€è¦æŒ‡å®š thread_id æ¥åŒºåˆ†ä¸åŒä¼šè¯ã€‚
```

#### âš¡ åœ¨ LangGraph ä¸­çš„åº”ç”¨ï¼ˆé‡è¦ï¼‰

```python
from langgraph.graph import StateGraph, MessagesState
from langchain_community.vectorstores import FAISS

class RAGState(MessagesState):
    context: str

def retrieve_node(state: RAGState):
    """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
    last_message = state["messages"][-1].content
    docs = retriever.invoke(last_message)
    context = "\n".join([doc.page_content for doc in docs])
    return {"context": context}

def generate_node(state: RAGState):
    """ç”Ÿæˆå›ç­”"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", "è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”\n{context}"),
        ("placeholder", "{messages}")
    ])

    llm = ChatOpenAI(model="gpt-5-nano")
    chain = prompt | llm

    response = chain.invoke({
        "context": state["context"],
        "messages": state["messages"]
    })

    return {"messages": [response]}

# æ„å»º RAG å›¾
graph = StateGraph(RAGState)
graph.add_node("retrieve", retrieve_node)
graph.add_node("generate", generate_node)
graph.add_edge("retrieve", "generate")
```

**å…³é”®ç‚¹**ï¼š
- RAG åœ¨ LangGraph ä¸­é€šå¸¸åˆ†ä¸ºä¸¤ä¸ªèŠ‚ç‚¹ï¼šæ£€ç´¢èŠ‚ç‚¹ + ç”ŸæˆèŠ‚ç‚¹
- æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å­˜å‚¨åœ¨ State ä¸­
- è¿™ç§æ¨¡å¼å¯ä»¥æ”¯æŒæ›´å¤æ‚çš„æ£€ç´¢ç­–ç•¥ï¼ˆå¦‚é‡æ’åºã€è¿‡æ»¤ï¼‰

---

### 2.7 LCELï¼ˆLangChain Expression Languageï¼‰- ç»„ä»¶ç¼–æ’è¯­è¨€

LCEL æ˜¯ LangChain çš„é“¾å¼ç»„åˆè¯­æ³•ï¼Œä½¿ç”¨ `|` æ“ä½œç¬¦è¿æ¥ç»„ä»¶ã€‚

#### åŸºç¡€é“¾ç»„åˆ

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# å®šä¹‰ç»„ä»¶
prompt = ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº {topic} çš„ç¬‘è¯")
llm = ChatOpenAI(model="gpt-5-nano")
output_parser = StrOutputParser()

# ä½¿ç”¨ | æ“ä½œç¬¦ä¸²è”
chain = prompt | llm | output_parser

# è°ƒç”¨é“¾
result = chain.invoke({"topic": "ç¨‹åºå‘˜"})
print(result)
```

**è¿è¡Œç»“æœ**ï¼š
```
ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»å–œæ¬¢æ·±å¤œå·¥ä½œï¼Ÿ
ç­”æ¡ˆï¼šå› ä¸º Oct 31 == Dec 25
1031ï¼ˆå…«è¿›åˆ¶ï¼‰ = 1225ï¼ˆå…«è¿›åˆ¶ï¼‰ï¼šä¸‡åœ£èŠ‚å°±æ˜¯åœ£è¯èŠ‚ï¼
```

#### RunnablePassthroughï¼ˆæ•°æ®ä¼ é€’ï¼‰

```python
from langchain_core.runnables import RunnablePassthrough

chain = (
    {"original": RunnablePassthrough(), "uppercase": lambda x: x.upper()}
    | (lambda x: f"åŸæ–‡ï¼š{x['original']}\nå¤§å†™ï¼š{x['uppercase']}")
)

result = chain.invoke("hello world")
print(result)
```

**è¿è¡Œç»“æœ**ï¼š
```
åŸæ–‡ï¼šhello world
å¤§å†™ï¼šHELLO WORLD
```

#### RunnableLambdaï¼ˆè‡ªå®šä¹‰å‡½æ•°ï¼‰

```python
from langchain_core.runnables import RunnableLambda

def add_prefix(text: str) -> str:
    return f"AI å›ç­”ï¼š{text}"

def add_suffix(text: str) -> str:
    return f"{text}\n\n---\nç”Ÿæˆæ—¶é—´ï¼š2024-01-15"

chain = (
    ChatPromptTemplate.from_template("è§£é‡Šä»€ä¹ˆæ˜¯ {term}")
    | ChatOpenAI(model="gpt-5-nano")
    | StrOutputParser()
    | RunnableLambda(add_prefix)
    | RunnableLambda(add_suffix)
)

result = chain.invoke({"term": "LCEL"})
print(result)
```

**è¿è¡Œç»“æœ**ï¼š
```
AI å›ç­”ï¼šLCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ LangChain æä¾›çš„é“¾å¼ç»„åˆè¯­æ³•ï¼Œ
é€šè¿‡ç®¡é“æ“ä½œç¬¦è¿æ¥ç»„ä»¶ï¼Œåˆ›å»ºæ•°æ®å¤„ç†æµç¨‹ã€‚

---
ç”Ÿæˆæ—¶é—´ï¼š2024-01-15
```

#### å¹¶è¡Œæ‰§è¡Œ

```python
from langchain_core.runnables import RunnableParallel

# å¹¶è¡Œè°ƒç”¨å¤šä¸ªé“¾
chain = RunnableParallel({
    "joke": ChatPromptTemplate.from_template("è®²ä¸€ä¸ªå…³äº {topic} çš„ç¬‘è¯") | llm | StrOutputParser(),
    "poem": ChatPromptTemplate.from_template("å†™ä¸€é¦–å…³äº {topic} çš„è¯—") | llm | StrOutputParser(),
    "fact": ChatPromptTemplate.from_template("è¯´ä¸€ä¸ªå…³äº {topic} çš„äº‹å®") | llm | StrOutputParser(),
})

results = chain.invoke({"topic": "Python"})

print("=== ç¬‘è¯ ===")
print(results["joke"])
print("\n=== è¯—æ­Œ ===")
print(results["poem"])
print("\n=== äº‹å® ===")
print(results["fact"])
```

**è¿è¡Œç»“æœ**ï¼š
```
=== ç¬‘è¯ ===
ä¸ºä»€ä¹ˆ Python ç¨‹åºå‘˜å–œæ¬¢å¤§è‡ªç„¶ï¼Ÿ
ç­”æ¡ˆï¼šå› ä¸ºå®ƒæœ‰å¾ˆå¤š "ç±»"ï¼ˆclassï¼‰ï¼

=== è¯—æ­Œ ===
ä»£ç è¯—è¡Œå¦‚æµäº‘
ç¼©è¿›ä¼˜é›…èƒœç¹æ–‡

=== äº‹å® ===
Python ç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºï¼Œå‘½åçµæ„Ÿæ¥è‡ªè‹±å›½å–œå‰§å›¢ä½“ Monty Pythonã€‚
å®ƒé‡‡ç”¨çš„æ˜¯ç®€æ´æ˜äº†çš„è¯­æ³•ï¼Œå¼ºè°ƒä»£ç å¯è¯»æ€§ã€‚
```

#### æ¡ä»¶åˆ†æ”¯

```python
from langchain_core.runnables import RunnableBranch

# æ ¹æ®è¾“å…¥é•¿åº¦åˆ†é…ä¸åŒçš„å¤„ç†é€»è¾‘
def route_by_length(x: dict) -> str:
    if len(x["text"]) < 10:
        return "short"
    elif len(x["text"]) < 50:
        return "medium"
    else:
        return "long"

branch = RunnableBranch(
    (lambda x: route_by_length(x) == "short", lambda x: f"çŸ­æ–‡æœ¬ï¼š{x['text']}"),
    (lambda x: route_by_length(x) == "medium", lambda x: f"ä¸­ç­‰é•¿åº¦ï¼š{x['text'][:20]}..."),
    lambda x: f"é•¿æ–‡æœ¬ï¼š{x['text'][:30]}..."
)

print(branch.invoke({"text": "Hi"}))
print(branch.invoke({"text": "This is a medium length text"}))
print(branch.invoke({"text": "This is a very long text that exceeds the threshold for long texts"}))
```

**è¿è¡Œç»“æœ**ï¼š
```
çŸ­æ–‡æœ¬ï¼šHi
ä¸­ç­‰é•¿åº¦ï¼šThis is a medium leng...
é•¿æ–‡æœ¬ï¼šThis is a very long text that...
```

#### LCEL vs LangGraph å¯¹æ¯”

| ç»´åº¦ | LCEL | LangGraph |
|------|------|-----------|
| **é€‚ç”¨åœºæ™¯** | ç®€å•çš„å•å‘æµç¨‹ | å¤æ‚çš„å¤šåˆ†æ”¯æµç¨‹ |
| **è¯­æ³•** | ç®¡é“æ“ä½œç¬¦ `|` | å›¾ç»“æ„ï¼ˆèŠ‚ç‚¹+è¾¹ï¼‰ |
| **çŠ¶æ€ç®¡ç†** | æ—   | æ˜¾å¼ State ç®¡ç† |
| **å¾ªç¯æ§åˆ¶** | ä¸æ”¯æŒ | æ”¯æŒ |
| **æ¡ä»¶åˆ†æ”¯** | RunnableBranch | add_conditional_edges |
| **å¯è§†åŒ–** | å›°éš¾ | æ”¯æŒå›¾å¯è§†åŒ– |
| **è°ƒè¯•** | æœ‰é™ | æ”¯æŒæ–­ç‚¹ã€æ—¶é—´æ—…è¡Œ |

#### ğŸ“Š é€‚ç”¨åœºæ™¯å¯¹æ¯”

**ä½•æ—¶ä½¿ç”¨ LCEL**ï¼š
- ç®€å•çš„"æç¤ºè¯ â†’ LLM â†’ è§£æ"æµç¨‹
- RAG å•æ¬¡æ£€ç´¢åœºæ™¯
- æ— éœ€çŠ¶æ€ç®¡ç†çš„ä»»åŠ¡

**ä½•æ—¶ä½¿ç”¨ LangGraph**ï¼š
- éœ€è¦å¾ªç¯æ‰§è¡Œï¼ˆå¦‚ ReAct Agentï¼‰
- å¤æ‚æ¡ä»¶åˆ†æ”¯ï¼ˆå¦‚ Multi-Agent è·¯ç”±ï¼‰
- éœ€è¦çŠ¶æ€æŒä¹…åŒ–ï¼ˆå¯¹è¯å†å²ï¼‰
- äººæœºåä½œï¼ˆæ–­ç‚¹ã€ç¼–è¾‘çŠ¶æ€ï¼‰

#### ä»£ç å¯¹æ¯”ç¤ºä¾‹

```python
# LCELï¼šç®€å•çš„å•å‘æµç¨‹
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# LangGraphï¼šå¤æ‚çš„å¾ªç¯æµç¨‹
graph = StateGraph(State)
graph.add_node("retrieve", retrieve_node)
graph.add_node("generate", generate_node)
graph.add_conditional_edges("generate", should_retry, {
    "retry": "retrieve",
    "end": END
})
```

---

## ç¬¬ 3 éƒ¨åˆ†ï¼šLangChain ä¸ LangGraph æ•´åˆå®æˆ˜

### 3.1 Chat Models æ•´åˆ

```python
from langgraph.graph import StateGraph, MessagesState
from langchain_openai import ChatOpenAI

def chatbot_node(state: MessagesState):
    # ç›´æ¥ä½¿ç”¨ LangChain çš„ ChatOpenAI
    llm = ChatOpenAI(model="gpt-5-nano", temperature=0.7)
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

graph = StateGraph(MessagesState)
graph.add_node("chatbot", chatbot_node)
```

### 3.2 Tools æ•´åˆ

```python
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode

@tool
def search_tool(query: str) -> str:
    """æœç´¢å·¥å…·"""
    return f"æœç´¢ç»“æœï¼š{query}"

tools = [search_tool]

# LangGraph çš„ ToolNode ä¼šè‡ªåŠ¨æ‰§è¡Œ LangChain å·¥å…·
tool_node = ToolNode(tools)
graph.add_node("tools", tool_node)
```

### 3.3 Prompts æ•´åˆ

```python
from langchain_core.prompts import ChatPromptTemplate

def agent_node(state: MessagesState):
    # ä½¿ç”¨ LangChain æç¤ºè¯æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ {role}"),
        ("placeholder", "{messages}")
    ])

    llm = ChatOpenAI(model="gpt-5-nano")
    chain = prompt | llm

    response = chain.invoke({
        "role": "Python ä¸“å®¶",
        "messages": state["messages"]
    })

    return {"messages": [response]}
```

### 3.4 å®Œæ•´ LangChain + LangGraph ç¤ºä¾‹

```python
from langgraph.graph import StateGraph, MessagesState, START, END
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate
from langgraph.prebuilt import ToolNode

# 1. å®šä¹‰å·¥å…·ï¼ˆLangChainï¼‰
@tool
def calculate(expression: str) -> str:
    """æ‰§è¡Œæ•°å­¦è®¡ç®—"""
    return str(eval(expression))

tools = [calculate]

# 2. å®šä¹‰æç¤ºè¯ï¼ˆLangChainï¼‰
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ•°å­¦åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨è®¡ç®—å·¥å…·"),
    ("placeholder", "{messages}")
])

# 3. å®šä¹‰ Agent èŠ‚ç‚¹ï¼ˆLangChain + LangGraphï¼‰
def agent(state: MessagesState):
    llm = ChatOpenAI(model="gpt-5-nano").bind_tools(tools)
    chain = prompt | llm
    response = chain.invoke({"messages": state["messages"]})
    return {"messages": [response]}

# 4. æ¡ä»¶è·¯ç”±ï¼ˆLangGraphï¼‰
def should_continue(state: MessagesState):
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return END

# 5. æ„å»ºå›¾ï¼ˆLangGraphï¼‰
graph = StateGraph(MessagesState)
graph.add_node("agent", agent)
graph.add_node("tools", ToolNode(tools))

graph.add_edge(START, "agent")
graph.add_conditional_edges("agent", should_continue)
graph.add_edge("tools", "agent")

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# 6. æµ‹è¯•
response = app.invoke({
    "messages": [("user", "è®¡ç®— (25 + 75) * 2 / 10")]
})
print(response["messages"][-1].content)
```

---

## ç¬¬ 4 éƒ¨åˆ†ï¼šæœ€ä½³å®è·µä¸å¸¸è§é—®é¢˜

### 4.1 é”™è¯¯å¤„ç†

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

llm = ChatOpenAI(model="gpt-5-nano")

try:
    response = llm.invoke([HumanMessage(content="ä½ å¥½")])
    print(response.content)
except Exception as e:
    print(f"é”™è¯¯ï¼š{type(e).__name__}: {str(e)}")
```

### 4.2 é‡è¯•ç­–ç•¥

```python
from langchain_openai import ChatOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def call_llm_with_retry(message: str):
    llm = ChatOpenAI(model="gpt-5-nano")
    return llm.invoke([HumanMessage(content=message)])

response = call_llm_with_retry("è§£é‡Šä»€ä¹ˆæ˜¯é“¾å¼æ³•åˆ™")
print(response.content)
```

### 4.3 Token è®¡æ•°

```python
from langchain_openai import ChatOpenAI
from langchain.callbacks import get_openai_callback

llm = ChatOpenAI(model="gpt-5-nano")

with get_openai_callback() as cb:
    response = llm.invoke([HumanMessage(content="å†™ä¸€é¦–è¯—")])

    print(f"æç¤ºè¯ Tokens: {cb.prompt_tokens}")
    print(f"å›å¤ Tokens: {cb.completion_tokens}")
    print(f"æ€»è®¡ Tokens: {cb.total_tokens}")
    print(f"æ€»è®¡æˆæœ¬: ${cb.total_cost:.6f}")
```

**è¿è¡Œç»“æœ**ï¼š
```
æç¤ºè¯ Tokens: 12
å›å¤ Tokens: 48
æ€»è®¡ Tokens: 60
æ€»è®¡æˆæœ¬: $0.000090
```

### 4.4 æˆæœ¬ä¼˜åŒ–

```python
# 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹
llm_cheap = ChatOpenAI(model="gpt-5-nano")      # ä¾¿å®œ
llm_expensive = ChatOpenAI(model="gpt-5-nano")        # è´µä½†æ€§èƒ½å¥½

# 2. é™åˆ¶è¾“å‡ºé•¿åº¦
llm = ChatOpenAI(model="gpt-5-nano", max_tokens=100)

# 3. ä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼ˆç›¸åŒè¾“å…¥ä¸é‡å¤è°ƒç”¨ï¼‰
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆçœŸå®è°ƒç”¨ï¼‰
response1 = llm.invoke([HumanMessage(content="ä»€ä¹ˆæ˜¯ AI")])

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆä»ç¼“å­˜è¯»å–ï¼Œä¸æ¶ˆè€— tokensï¼‰
response2 = llm.invoke([HumanMessage(content="ä»€ä¹ˆæ˜¯ AI")])
```

---

## æ€»ç»“

### 7 å¤§æ ¸å¿ƒç»„ä»¶å›é¡¾

| ç»„ä»¶ | ä½œç”¨ | åœ¨ LangGraph ä¸­çš„åº”ç”¨ |
|------|------|---------------------|
| **Chat Models** | è°ƒç”¨ LLM æ¥å£ | èŠ‚ç‚¹å‡½æ•°ä¸­è°ƒç”¨ LLM |
| **Prompts** | ç»“æ„åŒ–æç¤ºè¯ | ä¸º Agent è®¾è®¡ç³»ç»Ÿæç¤ºè¯ |
| **Output Parsers** | è§£æ LLM è¾“å‡º | ä»æ–‡æœ¬æå–ç»“æ„åŒ–æ•°æ® |
| **Tools** | æ‰©å±• LLM èƒ½åŠ› | ToolNode è‡ªåŠ¨æ‰§è¡Œ |
| **Messages** | å¯¹è¯æ¶ˆæ¯ç±»å‹ | MessagesState ç®¡ç† |
| **Retrievers** | RAG æ£€ç´¢ | çŸ¥è¯†åº“å¢å¼º |
| **LCEL** | é“¾å¼ç¼–æ’ | èŠ‚ç‚¹å†…éƒ¨æ•°æ®æµ |

### LangChain vs LangGraph é€‚ç”¨åœºæ™¯

```
ç®€å•åœºæ™¯ï¼ˆLCELï¼‰:
  æç¤ºè¯ â†’ LLM â†’ è§£æ â†’ è¾“å‡º

å¤æ‚åœºæ™¯ï¼ˆLangGraphï¼‰:
  ç”¨æˆ·è¾“å…¥ â†’ Agentå†³ç­– â†’ å·¥å…·è°ƒç”¨ â†’ ç»“æœæ±‡æ€» â†’ å¾ªç¯å†³ç­–
```

### å­¦ä¹ å»ºè®®

1. **å…ˆæŒæ¡ LangChain åŸºç¡€**ï¼šChat Modelsã€Promptsã€Tools
2. **ç†è§£æ¶ˆæ¯å¯¹è±¡**ï¼šè¿™æ˜¯ LangGraph æœ€åŸºç¡€çš„æ•°æ®ç»“æ„
3. **å­¦ä¹  LCEL**ï¼šè™½ç„¶å¤§éƒ¨åˆ†æ—¶å€™ç”¨ LangGraphï¼Œä½†èŠ‚ç‚¹å†…éƒ¨ä»ç„¶ä¼šç”¨ LCEL æ¥ç»„åˆç»„ä»¶
4. **å®è·µ RAG**ï¼šæ£€ç´¢å¢å¼ºæ˜¯ç”Ÿäº§ç¯å¢ƒæœ€å¸¸ç”¨çš„æ¨¡å¼
5. **æ•´åˆå®æˆ˜**ï¼šå°† LangChain ç»„ä»¶é›†æˆåˆ° LangGraph å·¥ä½œæµ

### ä¸‹ä¸€æ­¥

- **ç¬¬ 1 ç« **ï¼šæ·±å…¥ LangGraph æ ¸å¿ƒæ¦‚å¿µï¼ˆChainã€Routerã€Agentï¼‰
- **ç¬¬ 2 ç« **ï¼šState Schema é«˜çº§è®¾è®¡
- **ç¬¬ 3 ç« **ï¼šæ–­ç‚¹è°ƒè¯•ä¸äººæœºåä½œ
- **ç¬¬ 4 ç« **ï¼šé«˜çº§æ¨¡å¼ï¼ˆå¹¶è¡Œã€å­å›¾ã€Map-Reduceï¼‰

---

**è®°ä½**ï¼šLangChain æ˜¯"å·¥å…·ç®±"ï¼ŒLangGraph æ˜¯"æ–½å·¥å›¾çº¸"ã€‚æŒæ¡ LangChainï¼Œä½ å°±èƒ½çµæ´»ä½¿ç”¨å„ç§ AI ç»„ä»¶ï¼›æŒæ¡ LangGraphï¼Œä½ å°±èƒ½ç¼–æ’å‡ºå¼ºå¤§çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼
