# 1.5 å°ç»“å’Œå¤ä¹ : LangGraph æ¡†æ¶åŸºç¡€

> **å¤ä¹ å¯„è¯­**
>
> æ­å–œä½ å®Œæˆäº† Module 1 çš„å­¦ä¹ !ä½œä¸ºä¸€ä½æ•™è‚²è€…,æˆ‘æ·±çŸ¥**ä¸»åŠ¨å›é¡¾**æ˜¯å·©å›ºçŸ¥è¯†çš„æœ€æœ‰æ•ˆæ–¹æ³•ã€‚æœ¬ç« å°†é€šè¿‡ 15 ä¸ªç²¾å¿ƒè®¾è®¡çš„é—®é¢˜,å¸®åŠ©ä½ æ£€éªŒç†è§£æ·±åº¦,å‘ç°çŸ¥è¯†ç›²ç‚¹,å¹¶å»ºç«‹ç³»ç»Ÿæ€§çš„è®¤çŸ¥æ¡†æ¶ã€‚

---

## ğŸ“š æœ¯è¯­è¡¨

| æœ¯è¯­åç§° | LangGraph å®šä¹‰å’Œè§£è¯» | Python å®šä¹‰å’Œè¯´æ˜ | é‡è¦ç¨‹åº¦ |
|---------|---------------------|------------------|---------|
| **Graph State Machine** | å›¾çŠ¶æ€æœºï¼ŒLangGraph çš„æ ¸å¿ƒæ¶æ„ï¼Œå°† Agent å»ºæ¨¡ä¸ºæœ‰å‘å›¾ | è®¾è®¡æ¨¡å¼ï¼Œé€šè¿‡èŠ‚ç‚¹å’Œè¾¹å®šä¹‰çŠ¶æ€è½¬æ¢å’Œæ‰§è¡Œæµç¨‹ | â­â­â­â­â­ |
| **StateGraph** | çŠ¶æ€å›¾ç±»ï¼Œç”¨äºæ„å»ºå’Œç®¡ç† LangGraph å·¥ä½œæµ | Python ç±»ï¼Œæä¾› add_node/add_edge/compile ç­‰æ–¹æ³• | â­â­â­â­â­ |
| **State Schema** | çŠ¶æ€æ¨¡å¼ï¼Œå®šä¹‰å›¾ä¸­æµè½¬çš„æ•°æ®ç»“æ„ | TypedDict æˆ– Pydantic BaseModelï¼Œå®šä¹‰å­—æ®µå’Œç±»å‹ | â­â­â­â­â­ |
| **Node** | èŠ‚ç‚¹å‡½æ•°ï¼Œæ¥æ”¶çŠ¶æ€å¹¶è¿”å›çŠ¶æ€æ›´æ–°çš„æ‰§è¡Œå•å…ƒ | Python å‡½æ•°ï¼Œç­¾åä¸º def node(state: State) -> dict | â­â­â­â­â­ |
| **Edge** | è¾¹ï¼Œå®šä¹‰èŠ‚ç‚¹é—´çš„è¿æ¥å’Œæ‰§è¡Œé¡ºåº | add_edge(from_node, to_node) æ–¹æ³•å®šä¹‰çš„è·¯å¾„ | â­â­â­â­â­ |
| **Conditional Edge** | æ¡ä»¶è¾¹ï¼Œæ ¹æ®çŠ¶æ€åŠ¨æ€è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹ | add_conditional_edges() é…åˆè·¯ç”±å‡½æ•°å®ç°åˆ†æ”¯é€»è¾‘ | â­â­â­â­â­ |
| **Checkpointer** | æ£€æŸ¥ç‚¹ä¿å­˜å™¨ï¼ŒæŒä¹…åŒ–å›¾çš„æ‰§è¡ŒçŠ¶æ€ | MemorySaverã€SqliteSaver ç­‰ç±»ï¼Œæ”¯æŒçŠ¶æ€æ¢å¤ | â­â­â­â­ |
| **Reducer** | å½’çº¦å™¨ï¼Œå®šä¹‰å¦‚ä½•åˆå¹¶çŠ¶æ€æ›´æ–° | add_messages ç­‰å‡½æ•°ï¼Œæ§åˆ¶çŠ¶æ€å­—æ®µçš„æ›´æ–°é€»è¾‘ | â­â­â­â­ |
| **Breakpoint** | æ–­ç‚¹ï¼Œåœ¨ç‰¹å®šèŠ‚ç‚¹æš‚åœæ‰§è¡Œç­‰å¾…äººå·¥å¹²é¢„ | é€šè¿‡é…ç½®æˆ– interrupt æ–¹æ³•å®ç°äººæœºåä½œ | â­â­â­â­ |
| **Time Travel** | æ—¶é—´æ—…è¡Œï¼Œå›æº¯å’Œä¿®æ”¹å†å²æ‰§è¡ŒçŠ¶æ€ | get_state_history() å’Œ update_state() æ–¹æ³•å®ç° | â­â­â­â­ |
| **Cycles** | å¾ªç¯ï¼Œå›¾ä¸­èŠ‚ç‚¹å¯ä»¥å›åˆ°ä¹‹å‰çš„èŠ‚ç‚¹å½¢æˆå¾ªç¯ | é€šè¿‡è¾¹çš„å®šä¹‰å®ç°ï¼Œæ”¯æŒ Agent çš„è¿­ä»£æ‰§è¡Œ | â­â­â­â­â­ |
| **Controllability** | å¯æ§æ€§ï¼Œå¼€å‘è€…å¯¹æ‰§è¡Œæµç¨‹çš„ç²¾ç¡®æ§åˆ¶èƒ½åŠ› | é€šè¿‡æ˜¾å¼å®šä¹‰èŠ‚ç‚¹ã€è¾¹å’Œè·¯ç”±å‡½æ•°å®ç° | â­â­â­â­â­ |

---

## ğŸ“š æœ¬ç« æ ¸å¿ƒçŸ¥è¯†å›é¡¾

åœ¨å¼€å§‹é—®ç­”ä¹‹å‰,è®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ Module 1 çš„çŸ¥è¯†åœ°å›¾:

```
Module 1: LangGraph æ¡†æ¶åŸºç¡€
â”œâ”€ 1.1 æ¡†æ¶å¯¹æ¯” â†’ ç†è§£"ä¸ºä»€ä¹ˆé€‰æ‹© LangGraph"
â”œâ”€ 1.2 ä¸Šæ‰‹æ¡ˆä¾‹ â†’ å»ºç«‹"å›¾æ€ç»´"
â”œâ”€ 1.3 LangChain å›é¡¾ â†’ æŒæ¡åŸºç¡€ç»„ä»¶
â””â”€ 1.4 åŸºç¡€å…¥é—¨ â†’ ç³»ç»ŸåŒ–ç†è§£

æ ¸å¿ƒæ¦‚å¿µ:
  â€¢ Graph (å›¾) â†’ æ‰§è¡Œå¼•æ“
  â€¢ State (çŠ¶æ€) â†’ æ•°æ®å®¹å™¨
  â€¢ Nodes (èŠ‚ç‚¹) â†’ çŠ¶æ€è½¬æ¢å‡½æ•°
  â€¢ Edges (è¾¹) â†’ æµç¨‹æ§åˆ¶
```

---

## ğŸ¯ å¤ä¹ é—®ç­” (15 é¢˜)

### ç¬¬ä¸€éƒ¨åˆ†: æ¡†æ¶ç†è§£ä¸é€‰æ‹© (5 é¢˜)

---

#### **é—®é¢˜ 1: LangGraph çš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯ä»€ä¹ˆ?å®ƒè§£å†³äº†ä¼ ç»Ÿ Agent æ¡†æ¶çš„å“ªäº›é—®é¢˜?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

LangGraph çš„æ ¸å¿ƒè®¾è®¡ç†å¿µæ˜¯**å›¾çŠ¶æ€æœº (Graph State Machine)**,å°† Agent çš„æ‰§è¡Œè¿‡ç¨‹å»ºæ¨¡ä¸ºæœ‰å‘å›¾ã€‚

**è§£å†³çš„æ ¸å¿ƒé—®é¢˜:**

1. **å¯æ§æ€§ä¸è¶³**
   - **ä¼ ç»Ÿæ¡†æ¶**: Agent åƒ"é»‘ç›’",æ‰§è¡Œæµç¨‹éš¾ä»¥é¢„æµ‹
   - **LangGraph**: æ˜¾å¼å®šä¹‰æ¯ä¸ªèŠ‚ç‚¹å’Œè¾¹,æµç¨‹å®Œå…¨å¯æ§

2. **éš¾ä»¥å®ç°å¤æ‚é€»è¾‘**
   - **ä¼ ç»Ÿæ¡†æ¶**: çº¿æ€§æ‰§è¡Œé“¾,éš¾ä»¥å¤„ç†å¾ªç¯å’Œå¤æ‚åˆ†æ”¯
   - **LangGraph**: æ”¯æŒä»»æ„å›¾ç»“æ„,åŒ…æ‹¬å¾ªç¯

3. **ç”Ÿäº§ç¯å¢ƒä¸å¯é **
   - **ä¼ ç»Ÿæ¡†æ¶**: ç¼ºä¹æ–­ç‚¹ã€å›æº¯ã€çŠ¶æ€ç®¡ç†
   - **LangGraph**: æä¾› Breakpointsã€Time Travelã€æŒä¹…åŒ–

4. **è°ƒè¯•å›°éš¾**
   - **ä¼ ç»Ÿæ¡†æ¶**: æ— æ³•é€æ­¥æ‰§è¡Œ,éš¾ä»¥å®šä½é—®é¢˜
   - **LangGraph**: å¯è§†åŒ–å›¾ç»“æ„,é€èŠ‚ç‚¹è°ƒè¯•

**å…³é”®æ´å¯Ÿ:**
```
LangGraph = å¯æ§æ€§ + çµæ´»æ€§ + ç”Ÿäº§çº§ç‰¹æ€§
```

**ä»£ç ä½“ç°:**
```python
# ä¼ ç»Ÿé“¾å¼è°ƒç”¨ (ä¸å¯æ§)
result = chain.invoke(input)  # é»‘ç›’æ‰§è¡Œ

# LangGraph (å®Œå…¨å¯æ§)
graph.add_conditional_edges(
    "decision_node",
    router,  # ä½ å®šä¹‰è·¯ç”±é€»è¾‘
    {"path_a": "node_a", "path_b": "node_b"}
)
```

</details>

---

#### **é—®é¢˜ 2: åœ¨ä»¥ä¸‹åœºæ™¯ä¸­,åº”è¯¥é€‰æ‹©å“ªä¸ªæ¡†æ¶?è¯·è¯´æ˜ç†ç”±ã€‚**

**åœºæ™¯ A**: å¿«é€Ÿæ„å»ºä¸€ä¸ªå†…å®¹åˆ›ä½œå›¢é˜Ÿ,åŒ…æ‹¬ç ”ç©¶å‘˜ã€ä½œå®¶ã€ç¼–è¾‘ä¸‰ä¸ªè§’è‰²
**åœºæ™¯ B**: æ„å»ºä¸€ä¸ªéœ€è¦å¤šæ¬¡è¿­ä»£çš„ä»£ç ç”Ÿæˆ Agent,è¦æ±‚å¯ä»¥è‡ªåŠ¨ä¿®å¤é”™è¯¯
**åœºæ™¯ C**: æ„å»ºä¸€ä¸ªç”Ÿäº§çº§çš„å®¢æœç³»ç»Ÿ,éœ€è¦ç²¾ç¡®æ§åˆ¶æµç¨‹å’Œäººå·¥å®¡æ‰¹
**åœºæ™¯ D**: å¿«é€Ÿå®éªŒä¸€ä¸ªç®€å•çš„ Agent äº¤äº’æ¨¡å¼

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**åœºæ™¯ A: CrewAI** âœ…
- **ç†ç”±**: è§’è‰²åˆ†å·¥æ˜ç¡®,ä»»åŠ¡çº¿æ€§æµè½¬,CrewAI çš„ `Agent + Task + Crew` æ¨¡å¼æœ€é€‚åˆ
- **ä»£ç ç¤ºä¾‹**:
  ```python
  researcher = Agent(role="ç ”ç©¶å‘˜", goal="æ”¶é›†ä¿¡æ¯")
  writer = Agent(role="ä½œå®¶", goal="æ’°å†™æ–‡ç« ")
  crew = Crew(agents=[researcher, writer])
  ```

**åœºæ™¯ B: AutoGen** âœ…
- **ç†ç”±**: ä»£ç ç”Ÿæˆå’Œè‡ªåŠ¨ä¿®å¤æ˜¯ AutoGen çš„å¼ºé¡¹,æ”¯æŒä»£ç æ‰§è¡Œå’Œè‡ªåŠ¨è¿­ä»£
- **ç‰¹ç‚¹**: UserProxy å¯ä»¥æ‰§è¡Œä»£ç ,Assistant å¯ä»¥æ ¹æ®æ‰§è¡Œç»“æœè‡ªåŠ¨ä¿®å¤

**åœºæ™¯ C: LangGraph** âœ…
- **ç†ç”±**:
  - ç”Ÿäº§çº§è¦æ±‚ â†’ éœ€è¦å¯é æ€§å’Œå¯è§‚æµ‹æ€§
  - ç²¾ç¡®æ§åˆ¶æµç¨‹ â†’ å›¾çŠ¶æ€æœºç²¾ç¡®å»ºæ¨¡
  - äººå·¥å®¡æ‰¹ â†’ Breakpoints æœºåˆ¶
- **å…³é”®ç‰¹æ€§**: `interrupt_before=["approval_node"]`

**åœºæ™¯ D: OpenAI Swarm** âœ…
- **ç†ç”±**: å®éªŒæ€§é¡¹ç›®,æœ€ç®€å•çš„å®ç°,å¿«é€ŸéªŒè¯æƒ³æ³•
- **æ³¨æ„**: ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒ

**å†³ç­–åŸåˆ™:**
```
å¿«é€ŸåŸå‹ â†’ CrewAI / Swarm
ä»£ç ç”Ÿæˆ â†’ AutoGen
ç”Ÿäº§ç³»ç»Ÿ â†’ LangGraph
```

</details>

---

#### **é—®é¢˜ 3: LangGraph ç›¸æ¯” CrewAI çš„æœ€å¤§ä¼˜åŠ¿æ˜¯ä»€ä¹ˆ?ä»€ä¹ˆæƒ…å†µä¸‹è¿™ä¸ªä¼˜åŠ¿è‡³å…³é‡è¦?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**æœ€å¤§ä¼˜åŠ¿: ç²¾ç¡®çš„æµç¨‹æ§åˆ¶å’ŒçŠ¶æ€ç®¡ç†**

**å…·ä½“ä½“ç°:**

1. **æ¡ä»¶åˆ†æ”¯**
   - CrewAI: çº¿æ€§ä»»åŠ¡æµ,åˆ†æ”¯èƒ½åŠ›æœ‰é™
   - LangGraph: ä»»æ„å¤æ‚çš„æ¡ä»¶è·¯ç”±
   ```python
   graph.add_conditional_edges(
       "analyzer",
       lambda state: "path_a" if state["score"] > 0.8 else "path_b",
       {"path_a": "high_score_handler", "path_b": "low_score_handler"}
   )
   ```

2. **å¾ªç¯æ‰§è¡Œ**
   - CrewAI: ä¸æ”¯æŒå¾ªç¯
   - LangGraph: å¤©ç„¶æ”¯æŒå¾ªç¯(å¦‚ Agent é‡è¯•)
   ```python
   # å¯ä»¥æ·»åŠ ä» "tools" å›åˆ° "assistant" çš„è¾¹
   graph.add_edge("tools", "assistant")  # å½¢æˆå¾ªç¯
   ```

3. **çŠ¶æ€ç´¯ç§¯**
   - CrewAI: ä¸»è¦ä¾èµ–æ¶ˆæ¯ä¼ é€’
   - LangGraph: å®Œæ•´çš„çŠ¶æ€ç®¡ç†,æ”¯æŒä»»æ„å¤æ‚çš„çŠ¶æ€
   ```python
   class AgentState(TypedDict):
       messages: list
       retry_count: int
       error_log: list
       context: dict
   ```

**ä½•æ—¶è‡³å…³é‡è¦:**

âœ… **é‡‘èäº¤æ˜“ç³»ç»Ÿ**
- éœ€è¦å¤šé‡å®¡æ‰¹
- å¼‚å¸¸æƒ…å†µéœ€è¦å›é€€
- æ¯æ­¥å†³ç­–éƒ½è¦è®°å½•

âœ… **åŒ»ç–—è¯Šæ–­ Agent**
- è¯Šæ–­æµç¨‹å¯èƒ½éœ€è¦å¤šæ¬¡è¿­ä»£
- æ¯ä¸ªå†³ç­–èŠ‚ç‚¹éœ€è¦å¯å®¡æŸ¥
- ä¸åŒç—…æƒ…èµ°ä¸åŒè·¯å¾„

âœ… **è‡ªåŠ¨åŒ–è¿ç»´ç³»ç»Ÿ**
- å¤æ‚çš„å†³ç­–æ ‘
- å¤±è´¥éœ€è¦å›æ»š
- éœ€è¦ç²¾ç¡®çš„çŠ¶æ€è¿½è¸ª

**å…³é”®æ´å¯Ÿ:**
> CrewAI åƒ"æµæ°´çº¿",LangGraph åƒ"å¯ç¼–ç¨‹çš„æ‰§è¡Œå¼•æ“"ã€‚å½“ä½ éœ€è¦çš„ä¸ä»…æ˜¯"åä½œ",è¿˜æœ‰"ç²¾ç¡®æ§åˆ¶"æ—¶,LangGraph æ˜¯å”¯ä¸€é€‰æ‹©ã€‚

</details>

---

#### **é—®é¢˜ 4: ä»€ä¹ˆæ˜¯"å›¾æ€ç»´"?å®ƒä¸ä¼ ç»Ÿçš„"é“¾å¼æ€ç»´"æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**å›¾æ€ç»´ vs é“¾å¼æ€ç»´:**

| ç»´åº¦ | é“¾å¼æ€ç»´ | å›¾æ€ç»´ |
|------|---------|--------|
| **æ‰§è¡Œæ¨¡å¼** | çº¿æ€§: Aâ†’Bâ†’Câ†’D | ç½‘ç»œ: æ ¹æ®çŠ¶æ€åŠ¨æ€è·¯ç”± |
| **å†³ç­–ç‚¹** | é¢„å®šä¹‰çš„å›ºå®šæµç¨‹ | è¿è¡Œæ—¶åŠ¨æ€å†³ç­– |
| **å¾ªç¯** | ä¸æ”¯æŒæˆ–éš¾ä»¥å®ç° | å¤©ç„¶æ”¯æŒ |
| **çŠ¶æ€** | éšå¼,éš¾ä»¥è¿½è¸ª | æ˜¾å¼,å®Œå…¨å¯è§ |
| **é”™è¯¯å¤„ç†** | ä¸­æ–­æ•´ä¸ªé“¾ | å¯ä»¥è·³è½¬åˆ°é”™è¯¯å¤„ç†èŠ‚ç‚¹ |

**ç¤ºä¾‹å¯¹æ¯”:**

**é“¾å¼æ€ç»´ (ä¼ ç»Ÿ Chain):**
```python
# å›ºå®šæµç¨‹,æ— æ³•æ ¹æ®ç»“æœæ”¹å˜è·¯å¾„
chain = prompt | llm | output_parser | tool_executor
result = chain.invoke(input)
```

**å›¾æ€ç»´ (LangGraph):**
```python
# æ ¹æ® LLM è¾“å‡ºåŠ¨æ€å†³å®šä¸‹ä¸€æ­¥
def router(state):
    last_message = state["messages"][-1]
    if has_tool_calls(last_message):
        return "tools"  # éœ€è¦è°ƒç”¨å·¥å…·
    elif needs_more_info(last_message):
        return "clarify"  # éœ€è¦æ¾„æ¸…
    else:
        return END  # ç›´æ¥ç»“æŸ

graph.add_conditional_edges("llm", router, {
    "tools": "tool_node",
    "clarify": "clarification_node"
})
```

**å›¾æ€ç»´çš„æœ¬è´¨:**

1. **çŠ¶æ€é©±åŠ¨**:
   ```
   å½“å‰çŠ¶æ€ + èŠ‚ç‚¹å¤„ç† â†’ æ–°çŠ¶æ€ + è·¯ç”±å†³ç­–
   ```

2. **éçº¿æ€§æµç¨‹**:
   ```
   START â†’ A â†’ [å†³ç­–] â†’ B æˆ– C â†’ [å†³ç­–] â†’ D æˆ–å›åˆ° A â†’ END
   ```

3. **æ˜¾å¼å»ºæ¨¡**:
   ```python
   # æ¯ä¸ªå†³ç­–ç‚¹éƒ½æ˜¯æ˜¾å¼çš„å‡½æ•°
   def decide_next_step(state):
       # ä½ å®Œå…¨æ§åˆ¶è·¯ç”±é€»è¾‘
       if state["confidence"] < 0.7:
           return "retry"
       return "continue"
   ```

**å®é™…åº”ç”¨ä»·å€¼:**

**åœºæ™¯: æ™ºèƒ½å®¢æœ**
```
é“¾å¼æ€ç»´:
  ç”¨æˆ·è¾“å…¥ â†’ LLM â†’ è¾“å‡º (æ— æ³•å¤„ç†å¤æ‚æƒ…å†µ)

å›¾æ€ç»´:
  ç”¨æˆ·è¾“å…¥ â†’ æ„å›¾åˆ†ç±» â†’ [è·¯ç”±]
                            â”œâ†’ FAQ: ç›´æ¥å›ç­”
                            â”œâ†’ æŠ€æœ¯é—®é¢˜: è°ƒç”¨çŸ¥è¯†åº“
                            â”œâ†’ æŠ•è¯‰: è½¬äººå·¥
                            â””â†’ å…¶ä»–: æ¾„æ¸…æ„å›¾ (å¾ªç¯å›åˆ°åˆ†ç±»)
```

**å…³é”®æ´å¯Ÿ:**
> å›¾æ€ç»´ä¸ä»…æ˜¯å·¥å…·çš„æ”¹å˜,æ›´æ˜¯**è®¤çŸ¥æ¨¡å¼çš„å‡çº§**ã€‚å®ƒè®©æˆ‘ä»¬ä»"å¸Œæœ› AI æŒ‰é¢„æœŸå·¥ä½œ"åˆ°"è®¾è®¡ AI æŒ‰é¢„æœŸå·¥ä½œ"ã€‚

</details>

---

#### **é—®é¢˜ 5: LangGraph å¦‚ä½•å®ç° Human-in-the-Loop?ä¸¾ä¾‹è¯´æ˜å…¶åº”ç”¨åœºæ™¯ã€‚**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**å®ç°æœºåˆ¶: Breakpoints (æ–­ç‚¹)**

LangGraph æä¾›ä¸‰ç§ Human-in-the-Loop æ–¹å¼:

**1. interrupt_before (æ‰§è¡Œå‰ä¸­æ–­)**
```python
# åœ¨æ‰§è¡Œ tools èŠ‚ç‚¹å‰æš‚åœ,ç­‰å¾…äººå·¥æ‰¹å‡†
app = graph.compile(
    checkpointer=memory,
    interrupt_before=["tools"]  # åœ¨å·¥å…·è°ƒç”¨å‰æš‚åœ
)

# æ‰§è¡Œ
config = {"configurable": {"thread_id": "1"}}
result = app.invoke(input, config)  # æš‚åœåœ¨å·¥å…·è°ƒç”¨å‰

# äººå·¥å®¡æŸ¥åç»§ç»­
result = app.invoke(None, config)  # ç»§ç»­æ‰§è¡Œ
```

**2. interrupt_after (æ‰§è¡Œåä¸­æ–­)**
```python
# åœ¨æ‰§è¡Œ tools èŠ‚ç‚¹åæš‚åœ,æŸ¥çœ‹ç»“æœ
app = graph.compile(
    checkpointer=memory,
    interrupt_after=["tools"]  # å·¥å…·è°ƒç”¨åæš‚åœ
)
```

**3. åŠ¨æ€ä¸­æ–­ (NodeInterrupt)**
```python
from langgraph.types import interrupt

def approval_node(state):
    # æ ¹æ®æ¡ä»¶å†³å®šæ˜¯å¦éœ€è¦äººå·¥å®¡æ‰¹
    if state["amount"] > 10000:
        decision = interrupt({
            "message": f"éœ€è¦æ‰¹å‡†: ${state['amount']} çš„æ”¯ä»˜",
            "data": state["payment_details"]
        })
        if decision != "approved":
            return {"status": "rejected"}
    return {"status": "approved"}
```

**åº”ç”¨åœºæ™¯:**

**åœºæ™¯ 1: é‡‘èäº¤æ˜“å®¡æ‰¹** ğŸ’°
```python
def payment_agent():
    # æ„å»ºå›¾
    graph = StateGraph(PaymentState)
    graph.add_node("analyze", analyze_transaction)
    graph.add_node("execute", execute_payment)

    # é«˜é£é™©äº¤æ˜“éœ€è¦äººå·¥å®¡æ‰¹
    def risk_router(state):
        if state["risk_score"] > 0.7:
            return "approval"  # è·³è½¬åˆ°å®¡æ‰¹èŠ‚ç‚¹
        return "execute"  # ç›´æ¥æ‰§è¡Œ

    graph.add_conditional_edges("analyze", risk_router)

    # åœ¨å®¡æ‰¹èŠ‚ç‚¹å‰ä¸­æ–­
    app = graph.compile(interrupt_before=["approval"])
```

**åœºæ™¯ 2: å†…å®¹å®¡æ ¸** ğŸ“
```python
# ç”Ÿæˆå†…å®¹å,å‘å¸ƒå‰éœ€è¦äººå·¥å®¡æŸ¥
app = graph.compile(
    interrupt_after=["content_generation"],
    interrupt_before=["publish"]
)

# å·¥ä½œæµ
# 1. ç”Ÿæˆå†…å®¹ (è‡ªåŠ¨)
# 2. æš‚åœ â†’ äººå·¥å®¡æŸ¥
# 3. æ‰¹å‡†å â†’ å‘å¸ƒ
```

**åœºæ™¯ 3: åŒ»ç–—è¯Šæ–­** ğŸ¥
```python
class DiagnosisState(TypedDict):
    symptoms: list
    diagnosis: str
    confidence: float

def diagnosis_node(state):
    diagnosis, confidence = ai_diagnose(state["symptoms"])

    # ä½ç½®ä¿¡åº¦éœ€è¦åŒ»ç”Ÿç¡®è®¤
    if confidence < 0.9:
        doctor_input = interrupt({
            "ai_diagnosis": diagnosis,
            "confidence": confidence,
            "request": "è¯·åŒ»ç”Ÿç¡®è®¤æˆ–ä¿®æ­£è¯Šæ–­"
        })
        diagnosis = doctor_input["final_diagnosis"]

    return {"diagnosis": diagnosis}
```

**åœºæ™¯ 4: ä»£ç éƒ¨ç½²** ğŸš€
```python
# CI/CD æµç¨‹
graph.add_node("test", run_tests)
graph.add_node("deploy", deploy_to_production)

# éƒ¨ç½²åˆ°ç”Ÿäº§å‰éœ€è¦æ‰‹åŠ¨æ‰¹å‡†
app = graph.compile(interrupt_before=["deploy"])

# å·¥ä½œæµ:
# 1. è¿è¡Œæµ‹è¯• (è‡ªåŠ¨)
# 2. æµ‹è¯•é€šè¿‡ â†’ æš‚åœ
# 3. DevOps å®¡æŸ¥ â†’ æ‰¹å‡†
# 4. éƒ¨ç½²åˆ°ç”Ÿäº§ (è‡ªåŠ¨)
```

**å…³é”®ä¼˜åŠ¿:**

1. **çµæ´»æ€§**: å¯ä»¥åœ¨ä»»æ„èŠ‚ç‚¹ä¸­æ–­
2. **å¯æ¢å¤**: æ‰¹å‡†åä»ä¸­æ–­ç‚¹ç»§ç»­,ä¿æŒçŠ¶æ€
3. **å¯è¿½æº¯**: æ‰€æœ‰äººå·¥å†³ç­–éƒ½è¢«è®°å½•
4. **ç»†ç²’åº¦æ§åˆ¶**: å¯ä»¥ç¼–è¾‘çŠ¶æ€åå†ç»§ç»­

**å®ç°ç»†èŠ‚:**

```python
# å®Œæ•´ç¤ºä¾‹
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = graph.compile(
    checkpointer=memory,  # å¿…éœ€: ä¿å­˜çŠ¶æ€
    interrupt_before=["critical_node"]
)

config = {"configurable": {"thread_id": "user123"}}

# ç¬¬ä¸€æ¬¡è°ƒç”¨: æ‰§è¡Œåˆ°æ–­ç‚¹
result = app.invoke(input, config)

# æ£€æŸ¥çŠ¶æ€
state = app.get_state(config)
print(state.values)  # å½“å‰çŠ¶æ€
print(state.next)    # ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹

# äººå·¥å†³ç­–åç»§ç»­
result = app.invoke(None, config)  # None è¡¨ç¤ºç»§ç»­æ‰§è¡Œ
```

**å…³é”®æ´å¯Ÿ:**
> Human-in-the-Loop ä¸æ˜¯"åŠŸèƒ½",è€Œæ˜¯**ç”Ÿäº§çº§ AI ç³»ç»Ÿçš„å¿…éœ€å“**ã€‚å®ƒè®© AI ä»"æ›¿ä»£äºº"å˜ä¸º"è¾…åŠ©äºº",æ—¢æé«˜æ•ˆç‡,åˆä¿è¯å®‰å…¨ã€‚

</details>

---

### ç¬¬äºŒéƒ¨åˆ†: æ ¸å¿ƒæ¦‚å¿µ (5 é¢˜)

---

#### **é—®é¢˜ 6: è§£é‡Š LangGraph ä¸­ Stateã€Nodeã€Edge çš„å…³ç³»ã€‚ç”¨ä¸€ä¸ªç±»æ¯”è¯´æ˜å®ƒä»¬å¦‚ä½•ååŒå·¥ä½œã€‚**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**æ ¸å¿ƒå…³ç³»:**

```
State (çŠ¶æ€)  = å…±äº«çš„æ•°æ®å®¹å™¨
Node (èŠ‚ç‚¹)   = çŠ¶æ€è½¬æ¢å‡½æ•°
Edge (è¾¹)     = æµç¨‹æ§åˆ¶è§„åˆ™

æ•°å­¦è¡¨è¾¾:
  Node: State_old â†’ State_new
  Edge: å†³å®šä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„ Node
  Graph: æ‰€æœ‰ Nodes å’Œ Edges çš„ç»„åˆ
```

**ç±»æ¯” 1: æµæ°´çº¿å·¥å‚** ğŸ­

```
State (çŠ¶æ€)    = äº§å“åŠå…¶åŠ å·¥è¿›åº¦
Node (èŠ‚ç‚¹)     = åŠ å·¥ç«™ (æ¯ä¸ªç«™æ‰§è¡Œç‰¹å®šæ“ä½œ)
Edge (è¾¹)       = ä¼ é€å¸¦è·¯å¾„

å·¥ä½œæµç¨‹:
1. åŸææ–™ (åˆå§‹ State) è¿›å…¥æµæ°´çº¿
2. ç¬¬ä¸€ä¸ªåŠ å·¥ç«™ (Node) å¤„ç† â†’ äº§å“çŠ¶æ€æ”¹å˜
3. ä¼ é€å¸¦ (Edge) å†³å®šé€åˆ°å“ªä¸ªåŠ å·¥ç«™
4. é‡å¤ç›´åˆ°äº§å“å®Œæˆ (åˆ°è¾¾ END)
```

**ç±»æ¯” 2: RPG æ¸¸æˆ** ğŸ®

```
State (çŠ¶æ€)    = è§’è‰²å±æ€§ (HP, MP, è£…å¤‡, ä½ç½®...)
Node (èŠ‚ç‚¹)     = æ¸¸æˆäº‹ä»¶ (æˆ˜æ–—, å¯¹è¯, å•†åº—...)
Edge (è¾¹)       = è§¦å‘æ¡ä»¶ (HP<50 â†’ é€ƒè·‘, HP>50 â†’ ç»§ç»­æˆ˜æ–—)

æ¸¸æˆæµç¨‹:
1. ç©å®¶çŠ¶æ€: HP=100, ä½ç½®=æ£®æ—
2. é‡æ•Œäº‹ä»¶ (Node) â†’ æˆ˜æ–— â†’ HP=60
3. æ£€æŸ¥ HP (Edge) â†’ HP>50 â†’ ç»§ç»­æ¢ç´¢ (Next Node)
4. å¦‚æœ HP<20 â†’ è‡ªåŠ¨å›åŸ (Different Node)
```

**ä»£ç ä½“ç°:**

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

# 1. State: æ•°æ®å®¹å™¨
class GameState(TypedDict):
    player_hp: int
    enemy_hp: int
    location: str

# 2. Node: çŠ¶æ€è½¬æ¢å‡½æ•°
def battle_node(state: GameState) -> dict:
    # è¯»å–çŠ¶æ€
    player_hp = state["player_hp"]
    enemy_hp = state["enemy_hp"]

    # å¤„ç†é€»è¾‘ (æˆ˜æ–—)
    player_hp -= 20  # ç©å®¶å—ä¼¤
    enemy_hp -= 30   # æ•Œäººå—ä¼¤

    # è¿”å›æ›´æ–° (éƒ¨åˆ†æ›´æ–°!)
    return {
        "player_hp": player_hp,
        "enemy_hp": enemy_hp
    }

def heal_node(state: GameState) -> dict:
    return {"player_hp": min(state["player_hp"] + 50, 100)}

# 3. Edge: æµç¨‹æ§åˆ¶
def decide_next_action(state: GameState) -> str:
    if state["player_hp"] < 30:
        return "heal"  # HP ä½ â†’ æ²»ç–—
    elif state["enemy_hp"] <= 0:
        return "end"   # æ•Œäººæ­»äº¡ â†’ ç»“æŸ
    else:
        return "battle"  # ç»§ç»­æˆ˜æ–—

# 4. æ„å»º Graph
graph = StateGraph(GameState)
graph.add_node("battle", battle_node)
graph.add_node("heal", heal_node)

graph.add_edge(START, "battle")
graph.add_conditional_edges(
    "battle",
    decide_next_action,
    {
        "battle": "battle",  # å¾ªç¯
        "heal": "heal",
        "end": END
    }
)
graph.add_edge("heal", "battle")  # æ²»ç–—åç»§ç»­æˆ˜æ–—

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))
```

**ååŒå·¥ä½œæµç¨‹:**

```
1. åˆå§‹çŠ¶æ€è¿›å…¥å›¾
   State = {player_hp: 100, enemy_hp: 100}

2. START edge â†’ battle node
   battle_node æ‰§è¡Œ â†’ æ›´æ–° State
   State = {player_hp: 80, enemy_hp: 70}

3. Conditional edge æ£€æŸ¥çŠ¶æ€
   decide_next_action(State) â†’ "battle" (HP è¿˜å¤Ÿ)

4. battle edge â†’ battle node (å¾ªç¯)
   battle_node æ‰§è¡Œ â†’ æ›´æ–° State
   State = {player_hp: 60, enemy_hp: 40}

5. ç»§ç»­å¾ªç¯...ç›´åˆ°æŸä¸ªæ¡ä»¶æ»¡è¶³

6. æœ€ç»ˆ edge â†’ END
```

**å…³é”®æ´å¯Ÿ:**

1. **State æ˜¯"å…¨å±€å…±äº«å†…å­˜"**
   - æ‰€æœ‰ Node éƒ½èƒ½è¯»å–
   - æ¯ä¸ª Node åªè¿”å›éœ€è¦æ›´æ–°çš„éƒ¨åˆ†
   - LangGraph è‡ªåŠ¨åˆå¹¶æ›´æ–°

2. **Node æ˜¯"çº¯å‡½æ•°"**
   ```python
   # ç†æƒ³çš„ Node å‡½æ•°
   def my_node(state: State) -> dict:
       # 1. è¯»å–çŠ¶æ€
       # 2. æ‰§è¡Œé€»è¾‘
       # 3. è¿”å›æ›´æ–°
       return {"field": new_value}
   ```

3. **Edge æ˜¯"æ™ºèƒ½è·¯ç”±å™¨"**
   - Normal Edge: å›ºå®šè·¯å¾„
   - Conditional Edge: æ ¹æ®çŠ¶æ€åŠ¨æ€é€‰æ‹©è·¯å¾„

**æœ€ä½³å®è·µ:**

```python
# âœ… å¥½çš„è®¾è®¡
class State(TypedDict):
    # æ¸…æ™°çš„å­—æ®µå®šä¹‰
    messages: list[BaseMessage]
    user_info: dict
    next_action: str

def node(state: State) -> dict:
    # åªæ›´æ–°éœ€è¦çš„å­—æ®µ
    return {"next_action": "call_tool"}

# âŒ é¿å…çš„æ¨¡å¼
def bad_node(state: State) -> State:
    # ä¸è¦è¿”å›å®Œæ•´çš„ State
    state["field"] = "value"  # ä¸è¦ç›´æ¥ä¿®æ”¹
    return state  # ä¸æ¨è
```

</details>

---

#### **é—®é¢˜ 7: ä»€ä¹ˆæ˜¯ Channels?å®ƒåœ¨çŠ¶æ€ç®¡ç†ä¸­æ‰®æ¼”ä»€ä¹ˆè§’è‰²?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**Channels (é€šé“) æ˜¯ LangGraph çŠ¶æ€ç®¡ç†çš„æ ¸å¿ƒæœºåˆ¶ã€‚**

**å®šä¹‰:**
- State ä¸­çš„æ¯ä¸ªé”® (key) å°±æ˜¯ä¸€ä¸ª Channel
- Channel æ˜¯ç‹¬ç«‹çš„æ•°æ®é€šé“,æœ‰è‡ªå·±çš„æ›´æ–°è§„åˆ™

**ç¤ºä¾‹:**

```python
class State(TypedDict):
    name: str          # name é€šé“
    messages: list     # messages é€šé“
    count: int         # count é€šé“
```

è¿™ä¸ª State æœ‰ 3 ä¸ª Channels: `name`, `messages`, `count`

**Channel çš„ç‰¹æ€§:**

**1. ç‹¬ç«‹æ›´æ–°**
```python
# Node 1 åªæ›´æ–° name
def node_1(state):
    return {"name": "Alice"}  # å…¶ä»–é€šé“ä¸å˜

# Node 2 åªæ›´æ–° messages
def node_2(state):
    return {"messages": [new_message]}  # å…¶ä»–é€šé“ä¸å˜
```

**2. é»˜è®¤è¦†ç›–è¡Œä¸º**
```python
state = {"name": "Alice", "count": 1}

# Node è¿”å›
return {"name": "Bob", "count": 2}

# ç»“æœ: å®Œå…¨è¦†ç›–
state = {"name": "Bob", "count": 2}
```

**3. Reducer å‡½æ•° (å…³é”®!)**

å¯¹äºåˆ—è¡¨ç±»å‹,é€šå¸¸éœ€è¦**è¿½åŠ **è€Œä¸æ˜¯è¦†ç›–:

```python
from typing import Annotated
from operator import add

class State(TypedDict):
    messages: Annotated[list, add]  # ä½¿ç”¨ add reducer
    #         ^^^^^^^^^^^^^^^^^^^^
    #         è¿™ä¸ªé€šé“ä½¿ç”¨ add å‡½æ•°åˆå¹¶æ›´æ–°

# ç°åœ¨çš„è¡Œä¸º:
state = {"messages": [msg1, msg2]}

# Node è¿”å›
return {"messages": [msg3]}

# ç»“æœ: è¿½åŠ  (ä¸æ˜¯è¦†ç›–!)
state = {"messages": [msg1, msg2, msg3]}
```

**å¸¸ç”¨ Reducers:**

```python
from operator import add
from typing import Annotated

# 1. åˆ—è¡¨è¿½åŠ 
messages: Annotated[list, add]

# 2. å­—å…¸åˆå¹¶
context: Annotated[dict, lambda x, y: {**x, **y}]

# 3. è‡ªå®šä¹‰ reducer
def custom_reducer(current, update):
    # current: å½“å‰å€¼
    # update: èŠ‚ç‚¹è¿”å›çš„å€¼
    return current + update  # ä½ çš„åˆå¹¶é€»è¾‘

field: Annotated[type, custom_reducer]
```

**å®é™…åº”ç”¨:**

**åœºæ™¯: å¤šè½®å¯¹è¯ Agent**

```python
from langchain_core.messages import BaseMessage
from typing import Annotated
from operator import add

class ChatState(TypedDict):
    # messages é€šé“: è¿½åŠ æ¶ˆæ¯,ä¸è¦†ç›–
    messages: Annotated[list[BaseMessage], add]

    # user_info é€šé“: è¦†ç›–æ›´æ–°
    user_info: dict

    # turn_count é€šé“: é€’å¢
    turn_count: Annotated[int, lambda curr, upd: curr + upd]

# Node 1: æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
def user_input_node(state):
    return {
        "messages": [HumanMessage(content="Hello")],
        "turn_count": 1  # é€’å¢ 1
    }
    # messages ä¼šè¿½åŠ ,ä¸ä¼šè¦†ç›–!
    # turn_count ä¼šç›¸åŠ ,ä¸ä¼šè¦†ç›–!

# Node 2: æ·»åŠ  AI æ¶ˆæ¯
def ai_response_node(state):
    return {
        "messages": [AIMessage(content="Hi there!")],
        "turn_count": 1
    }
    # åˆè¿½åŠ äº†ä¸€æ¡æ¶ˆæ¯
    # turn_count åˆåŠ  1

# æ‰§è¡Œå:
# state = {
#     "messages": [HumanMessage("Hello"), AIMessage("Hi there!")],
#     "user_info": {...},  # æœªæ›´æ–°,ä¿æŒåŸå€¼
#     "turn_count": 2      # 1 + 1 = 2
# }
```

**Channels çš„é«˜çº§ç”¨æ³•:**

**1. æ¡ä»¶æ›´æ–°**
```python
def node(state):
    updates = {}

    if state["score"] > 0.8:
        updates["status"] = "passed"  # åªæ›´æ–° status é€šé“

    if len(state["messages"]) > 10:
        updates["messages"] = state["messages"][-5:]  # åªä¿ç•™æœ€å 5 æ¡

    return updates  # å¯ä»¥è¿”å›ä»»æ„é€šé“çš„ç»„åˆ
```

**2. ä¾èµ–å…³ç³»**
```python
class State(TypedDict):
    raw_data: str
    processed_data: str  # ä¾èµ– raw_data

def process_node(state):
    # åŸºäº raw_data é€šé“è®¡ç®— processed_data é€šé“
    processed = transform(state["raw_data"])
    return {"processed_data": processed}
```

**3. é€šé“éš”ç¦»**
```python
# ä¸åŒ Node å¯ä»¥å…³æ³¨ä¸åŒçš„ Channels
def node_a(state):
    # åªå…³å¿ƒ channel_a
    return {"channel_a": value_a}

def node_b(state):
    # åªå…³å¿ƒ channel_b
    return {"channel_b": value_b}

# å®ƒä»¬ä¸ä¼šäº’ç›¸å¹²æ‰°
```

**å¯è§†åŒ–ç†è§£:**

```
State (çŠ¶æ€å®¹å™¨)
â”œâ”€ Channel: name (è¦†ç›–æ¨¡å¼)
â”‚   current: "Alice"
â”‚   update:  "Bob"
â”‚   result:  "Bob" â† ç›´æ¥è¦†ç›–
â”‚
â”œâ”€ Channel: messages (è¿½åŠ æ¨¡å¼)
â”‚   current: [msg1, msg2]
â”‚   update:  [msg3]
â”‚   result:  [msg1, msg2, msg3] â† ä½¿ç”¨ add reducer
â”‚
â””â”€ Channel: count (ç´¯åŠ æ¨¡å¼)
    current: 5
    update:  3
    result:  8 â† ä½¿ç”¨ lambda curr, upd: curr + upd
```

**æœ€ä½³å®è·µ:**

```python
# âœ… æ˜ç¡®æŒ‡å®š Reducer
from typing import Annotated
from operator import add

class State(TypedDict):
    # åˆ—è¡¨: ç”¨ add
    messages: Annotated[list, add]

    # å­—å…¸: ç”¨åˆå¹¶å‡½æ•°
    context: Annotated[dict, lambda x, y: {**x, **y}]

    # æ•°å­—: ç”¨ç´¯åŠ æˆ–è¦†ç›– (æ ¹æ®éœ€æ±‚)
    count: Annotated[int, lambda curr, upd: curr + upd]  # ç´¯åŠ 
    total: int  # è¦†ç›– (é»˜è®¤è¡Œä¸º)

# âŒ å¿˜è®° Reducer å¯¼è‡´çš„é—®é¢˜
class BadState(TypedDict):
    messages: list  # æ²¡æœ‰ add â†’ ä¼šè¢«è¦†ç›–!

def node(state):
    return {"messages": [new_msg]}
    # ç»“æœ: state["messages"] = [new_msg]  â† æ—§æ¶ˆæ¯ä¸¢å¤±!
```

**å…³é”®æ´å¯Ÿ:**

1. **Channels å®ç°æ¨¡å—åŒ–çŠ¶æ€ç®¡ç†**
   - æ¯ä¸ª Channel ç‹¬ç«‹æ›´æ–°
   - é¿å…äº†"å…¨å±€çŠ¶æ€æ±¡æŸ“"

2. **Reducer å†³å®šåˆå¹¶ç­–ç•¥**
   - é»˜è®¤: è¦†ç›–
   - åˆ—è¡¨: é€šå¸¸ç”¨ add
   - è‡ªå®šä¹‰: ä»»æ„é€»è¾‘

3. **è®¾è®¡åŸåˆ™: æœ€å°æ›´æ–°**
   ```python
   # åªè¿”å›æ”¹å˜çš„ Channels
   return {"field_a": new_value}
   # è€Œä¸æ˜¯è¿”å›æ•´ä¸ª State
   ```

</details>

---

#### **é—®é¢˜ 8: å†™å‡ºä¸€ä¸ªå®Œæ•´çš„ LangGraph åº”ç”¨çš„æœ€å°ä»£ç æ¨¡æ¿,å¹¶è§£é‡Šæ¯ä¸ªéƒ¨åˆ†çš„ä½œç”¨ã€‚**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**å®Œæ•´çš„æœ€å°æ¨¡æ¿:**

```python
# ========== 1. å¯¼å…¥å¿…éœ€çš„åŒ… ==========
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

# ========== 2. å®šä¹‰çŠ¶æ€ Schema ==========
class State(TypedDict):
    """
    çŠ¶æ€å®šä¹‰: å›¾ä¸­æµåŠ¨çš„æ•°æ®ç»“æ„
    - æ‰€æœ‰èŠ‚ç‚¹å…±äº«è¿™ä¸ªçŠ¶æ€
    - èŠ‚ç‚¹é€šè¿‡è¿”å›å­—å…¸æ¥æ›´æ–°çŠ¶æ€
    """
    input: str    # è¾“å…¥æ•°æ®
    output: str   # è¾“å‡ºæ•°æ®

# ========== 3. å®šä¹‰èŠ‚ç‚¹å‡½æ•° ==========
def processing_node(state: State) -> dict:
    """
    èŠ‚ç‚¹å‡½æ•°: State â†’ dict
    - æ¥æ”¶å®Œæ•´çš„ State
    - è¿”å›è¦æ›´æ–°çš„å­—æ®µ (éƒ¨åˆ†æ›´æ–°)
    """
    # è¯»å–çŠ¶æ€
    user_input = state["input"]

    # æ‰§è¡Œé€»è¾‘
    processed = f"Processed: {user_input}"

    # è¿”å›æ›´æ–° (åªè¿”å›æ”¹å˜çš„å­—æ®µ)
    return {"output": processed}

# ========== 4. æ„å»ºå›¾ ==========
# 4.1 åˆ›å»ºå›¾å®ä¾‹
graph = StateGraph(State)

# 4.2 æ·»åŠ èŠ‚ç‚¹
graph.add_node("process", processing_node)

# 4.3 æ·»åŠ è¾¹ (å®šä¹‰æµç¨‹)
graph.add_edge(START, "process")  # START â†’ process
graph.add_edge("process", END)    # process â†’ END

# ========== 5. ç¼–è¯‘å›¾ ==========
app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# ========== 6. è¿è¡Œå›¾ ==========
result = app.invoke({"input": "Hello, World!"})

# ========== 7. æŸ¥çœ‹ç»“æœ ==========
print(result)
# è¾“å‡º: {'input': 'Hello, World!', 'output': 'Processed: Hello, World!'}
```

**å„éƒ¨åˆ†è¯¦ç»†è§£é‡Š:**

---

**1. å¯¼å…¥** ğŸ“¦
```python
from typing_extensions import TypedDict  # å®šä¹‰ç±»å‹åŒ–å­—å…¸
from langgraph.graph import StateGraph, START, END  # æ ¸å¿ƒç±»å’Œå¸¸é‡
```
- `TypedDict`: Python ç±»å‹æç¤º,å®šä¹‰çŠ¶æ€ç»“æ„
- `StateGraph`: å›¾æ„å»ºå™¨
- `START/END`: ç‰¹æ®ŠèŠ‚ç‚¹æ ‡è®°

---

**2. çŠ¶æ€å®šä¹‰** ğŸ“
```python
class State(TypedDict):
    input: str
    output: str
```

**ä½œç”¨:**
- å®šä¹‰å›¾ä¸­æµåŠ¨çš„æ•°æ®ç»“æ„
- ç±»å‹æç¤ºå¸®åŠ© IDE è‡ªåŠ¨è¡¥å…¨å’Œç±»å‹æ£€æŸ¥
- æ‰€æœ‰èŠ‚ç‚¹å…±äº«è¿™ä¸ªçŠ¶æ€

**ç±»æ¯”:** State æ˜¯"å…±äº«å†…å­˜",æ‰€æœ‰èŠ‚ç‚¹éƒ½èƒ½è¯»å†™

---

**3. èŠ‚ç‚¹å‡½æ•°** âš™ï¸
```python
def processing_node(state: State) -> dict:
    user_input = state["input"]         # è¯»å–
    processed = f"Processed: {user_input}"  # å¤„ç†
    return {"output": processed}        # æ›´æ–°
```

**ä½œç”¨:**
- æ‰§è¡Œå…·ä½“çš„ä¸šåŠ¡é€»è¾‘
- **ç­¾å**: `(State) -> dict`
- **è¿”å›**: åªè¿”å›éœ€è¦æ›´æ–°çš„å­—æ®µ

**å…³é”®ç‚¹:**
- âœ… è¿”å› `dict`,ä¸æ˜¯ `State`
- âœ… éƒ¨åˆ†æ›´æ–°,ä¸æ˜¯å…¨é‡æ›¿æ¢
- âœ… çº¯å‡½æ•°,æ— å‰¯ä½œç”¨

---

**4. æ„å»ºå›¾** ğŸ”§
```python
graph = StateGraph(State)              # åˆ›å»ºå›¾
graph.add_node("process", processing_node)  # æ·»åŠ èŠ‚ç‚¹
graph.add_edge(START, "process")       # å®šä¹‰æµç¨‹
graph.add_edge("process", END)
```

**ä½œç”¨:**
- `StateGraph(State)`: åˆ›å»ºå›¾æ„å»ºå™¨,æŒ‡å®šçŠ¶æ€ç±»å‹
- `add_node`: æ³¨å†ŒèŠ‚ç‚¹å‡½æ•°
- `add_edge`: å®šä¹‰æ‰§è¡Œé¡ºåº

**æ‰§è¡Œæµç¨‹:**
```
START â†’ "process" â†’ END
```

---

**5. ç¼–è¯‘** ğŸ”¨
```python
app = graph.compile()
```

**ä½œç”¨:**
- å°†å›¾å®šä¹‰è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„åº”ç”¨
- éªŒè¯å›¾çš„æ­£ç¡®æ€§ (æ— æ‚¬ç©ºèŠ‚ç‚¹ã€å¾ªç¯æ£€æµ‹ç­‰)
- è¿”å› `CompiledGraph` å¯¹è±¡

**ç±»æ¯”:** ç¼–è¯‘æºä»£ç ä¸ºå¯æ‰§è¡Œæ–‡ä»¶

---

**6. è¿è¡Œ** â–¶ï¸
```python
result = app.invoke({"input": "Hello, World!"})
```

**ä½œç”¨:**
- æ‰§è¡Œå›¾,ä¼ å…¥åˆå§‹çŠ¶æ€
- è¿”å›æœ€ç»ˆçŠ¶æ€

**æ‰§è¡Œè¿‡ç¨‹:**
```
1. åˆå§‹çŠ¶æ€: {"input": "Hello, World!"}
2. START â†’ process èŠ‚ç‚¹
3. process_node æ‰§è¡Œ â†’ {"output": "Processed: ..."}
4. çŠ¶æ€åˆå¹¶: {"input": "...", "output": "..."}
5. process â†’ END
6. è¿”å›æœ€ç»ˆçŠ¶æ€
```

---

**å¸¦æ¡ä»¶è·¯ç”±çš„å®Œæ•´æ¨¡æ¿:**

```python
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START, END

# çŠ¶æ€å®šä¹‰
class State(TypedDict):
    input: str
    score: float
    output: str

# èŠ‚ç‚¹å‡½æ•°
def analyze_node(state: State) -> dict:
    """åˆ†æè¾“å…¥,æ‰“åˆ†"""
    text = state["input"]
    score = len(text) / 100.0  # ç®€å•æ‰“åˆ†é€»è¾‘
    return {"score": score}

def high_score_node(state: State) -> dict:
    return {"output": f"High quality: {state['input']}"}

def low_score_node(state: State) -> dict:
    return {"output": f"Needs improvement: {state['input']}"}

# è·¯ç”±å‡½æ•°
def router(state: State) -> Literal["high", "low"]:
    """æ ¹æ®åˆ†æ•°è·¯ç”±"""
    return "high" if state["score"] > 0.5 else "low"

# æ„å»ºå›¾
graph = StateGraph(State)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("analyze", analyze_node)
graph.add_node("high_quality", high_score_node)
graph.add_node("low_quality", low_score_node)

# æ·»åŠ è¾¹
graph.add_edge(START, "analyze")

# æ¡ä»¶è¾¹
graph.add_conditional_edges(
    "analyze",           # æºèŠ‚ç‚¹
    router,              # è·¯ç”±å‡½æ•°
    {                    # è·¯å¾„æ˜ å°„
        "high": "high_quality",
        "low": "low_quality"
    }
)

graph.add_edge("high_quality", END)
graph.add_edge("low_quality", END)

# ç¼–è¯‘å’Œè¿è¡Œ
app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

result = app.invoke({"input": "This is a test message"})
print(result)
```

**å…³é”®æ´å¯Ÿ:**

1. **æ¨¡æ¿åŒ–æ€ç»´**
   ```
   å®šä¹‰ State â†’ å®šä¹‰ Nodes â†’ è¿æ¥ Edges â†’ ç¼–è¯‘ â†’ è¿è¡Œ
   ```

2. **çŠ¶æ€æ˜¯æ ¸å¿ƒ**
   - æ‰€æœ‰é€»è¾‘å›´ç»•çŠ¶æ€å±•å¼€
   - èŠ‚ç‚¹é€šè¿‡çŠ¶æ€é€šä¿¡

3. **å£°æ˜å¼æ„å»º**
   - å…ˆå£°æ˜å›¾ç»“æ„
   - åæ‰§è¡Œå…·ä½“é€»è¾‘

</details>

---

#### **é—®é¢˜ 9: æ¡ä»¶è¾¹ (Conditional Edge) çš„è·¯ç”±å‡½æ•°æœ‰ä»€ä¹ˆè¦æ±‚?å¦‚ä½•å¤„ç†å¤šåˆ†æ”¯è·¯ç”±?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**è·¯ç”±å‡½æ•°çš„è¦æ±‚:**

**1. å‡½æ•°ç­¾å**
```python
from typing import Literal

def router(state: State) -> Literal["path_a", "path_b", "path_c"]:
    #          ^^^^^ è¾“å…¥: State
    #                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^ è¾“å‡º: è·¯å¾„å
    pass
```

**è¦æ±‚:**
- âœ… è¾“å…¥: æ¥æ”¶ State
- âœ… è¾“å‡º: è¿”å›**å­—ç¬¦ä¸²**,è¡¨ç¤ºä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åç§°
- âœ… ç±»å‹æç¤º: ä½¿ç”¨ `Literal` æ˜ç¡®æ‰€æœ‰å¯èƒ½çš„è·¯å¾„ (æ¨èä½†éå¿…éœ€)

**2. è¿”å›å€¼**
```python
# âœ… æ­£ç¡®: è¿”å›èŠ‚ç‚¹å
return "node_a"

# âœ… æ­£ç¡®: è¿”å› END
return END

# âŒ é”™è¯¯: è¿”å›å…¶ä»–ç±»å‹
return True  # ä¸è¡Œ!
return 123   # ä¸è¡Œ!
```

**3. çº¯å‡½æ•°**
```python
# âœ… å¥½çš„è·¯ç”±å‡½æ•°: çº¯å‡½æ•°,æ— å‰¯ä½œç”¨
def good_router(state):
    if state["score"] > 0.8:
        return "high"
    return "low"

# âŒ é¿å…: æœ‰å‰¯ä½œç”¨
def bad_router(state):
    print("Routing...")  # å‰¯ä½œç”¨: æ‰“å°
    state["routed"] = True  # å‰¯ä½œç”¨: ä¿®æ”¹çŠ¶æ€
    return "next"
```

---

**å¤šåˆ†æ”¯è·¯ç”±çš„å®ç°:**

**æ–¹å¼ 1: if-elif-else (åŸºç¡€)**
```python
def multi_branch_router(state: State) -> Literal["path_a", "path_b", "path_c", "end"]:
    score = state["score"]

    if score > 0.9:
        return "path_a"  # ä¼˜ç§€
    elif score > 0.7:
        return "path_b"  # è‰¯å¥½
    elif score > 0.5:
        return "path_c"  # åŠæ ¼
    else:
        return "end"     # ä¸åŠæ ¼,ç›´æ¥ç»“æŸ

# ä½¿ç”¨
graph.add_conditional_edges(
    "classifier",
    multi_branch_router,
    {
        "path_a": "excellent_handler",
        "path_b": "good_handler",
        "path_c": "pass_handler",
        "end": END
    }
)
```

**æ–¹å¼ 2: å­—å…¸æ˜ å°„ (ä¼˜é›…)**
```python
def intent_router(state: State) -> str:
    intent = state["intent"]

    route_map = {
        "weather": "weather_tool",
        "booking": "booking_tool",
        "faq": "faq_handler",
        "complaint": "human_agent",
    }

    # æä¾›é»˜è®¤è·¯å¾„
    return route_map.get(intent, "fallback")

# ä½¿ç”¨
graph.add_conditional_edges(
    "intent_classifier",
    intent_router,
    {
        "weather_tool": "weather_node",
        "booking_tool": "booking_node",
        "faq_handler": "faq_node",
        "human_agent": "handoff_node",
        "fallback": "general_response_node"
    }
)
```

**æ–¹å¼ 3: åŸºäºæ¨¡å¼åŒ¹é… (Python 3.10+)**
```python
def pattern_router(state: State) -> str:
    match state["category"]:
        case "urgent":
            return "priority_handler"
        case "normal":
            return "standard_handler"
        case "low":
            return "queue_handler"
        case _:
            return "default_handler"
```

**æ–¹å¼ 4: å¤æ‚é€»è¾‘ (ç»„åˆæ¡ä»¶)**
```python
def complex_router(state: State) -> str:
    user_type = state["user_type"]
    request_type = state["request_type"]
    priority = state["priority"]

    # VIP ç”¨æˆ·
    if user_type == "vip":
        return "vip_handler"

    # ç´§æ€¥è¯·æ±‚
    if priority == "urgent":
        if request_type == "technical":
            return "tech_urgent"
        else:
            return "general_urgent"

    # æ™®é€šè¯·æ±‚
    if request_type == "technical":
        return "tech_support"
    elif request_type == "billing":
        return "billing_support"
    else:
        return "general_support"
```

---

**å®é™…æ¡ˆä¾‹: æ™ºèƒ½å®¢æœç³»ç»Ÿ**

```python
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START, END

class CustomerServiceState(TypedDict):
    user_message: str
    intent: str
    sentiment: str  # positive/negative/neutral
    user_tier: str  # vip/premium/regular
    requires_human: bool

def classify_node(state):
    """æ„å›¾åˆ†ç±»å’Œæƒ…æ„Ÿåˆ†æ"""
    message = state["user_message"]

    # ç®€åŒ–çš„åˆ†ç±»é€»è¾‘
    intent = "complaint" if "problem" in message.lower() else "inquiry"
    sentiment = "negative" if "bad" in message.lower() else "neutral"

    return {
        "intent": intent,
        "sentiment": sentiment
    }

def route_request(state) -> Literal["auto", "human", "vip", "end"]:
    """æ™ºèƒ½è·¯ç”±é€»è¾‘"""

    # VIP ç”¨æˆ·ç›´æ¥è½¬äººå·¥
    if state["user_tier"] == "vip":
        return "vip"

    # æŠ•è¯‰ + è´Ÿé¢æƒ…ç»ª â†’ äººå·¥
    if state["intent"] == "complaint" and state["sentiment"] == "negative":
        return "human"

    # æ ‡è®°éœ€è¦äººå·¥çš„æƒ…å†µ
    if state.get("requires_human", False):
        return "human"

    # ä¸€èˆ¬è¯¢é—® â†’ è‡ªåŠ¨å›å¤
    if state["intent"] == "inquiry":
        return "auto"

    # å…¶ä»–æƒ…å†µç»“æŸ
    return "end"

# æ„å»ºå›¾
graph = StateGraph(CustomerServiceState)
graph.add_node("classify", classify_node)
graph.add_node("auto_response", lambda s: {"output": "è‡ªåŠ¨å›å¤"})
graph.add_node("human_handoff", lambda s: {"output": "è½¬äººå·¥"})
graph.add_node("vip_service", lambda s: {"output": "VIP æœåŠ¡"})

graph.add_edge(START, "classify")
graph.add_conditional_edges(
    "classify",
    route_request,
    {
        "auto": "auto_response",
        "human": "human_handoff",
        "vip": "vip_service",
        "end": END
    }
)
graph.add_edge("auto_response", END)
graph.add_edge("human_handoff", END)
graph.add_edge("vip_service", END)

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# æµ‹è¯•
result = app.invoke({
    "user_message": "I have a problem with my order",
    "user_tier": "regular"
})
```

---

**æœ€ä½³å®è·µ:**

**1. ä½¿ç”¨ Literal ç±»å‹æç¤º**
```python
# âœ… æ¨è: æ˜ç¡®æ‰€æœ‰è·¯å¾„
def router(state) -> Literal["path_a", "path_b", "path_c"]:
    ...

# âš ï¸ å¯è¡Œä½†ä¸æ¨è: æ²¡æœ‰ç±»å‹æç¤º
def router(state) -> str:
    ...
```

**2. æä¾›é»˜è®¤è·¯å¾„**
```python
def safe_router(state):
    route_map = {...}
    return route_map.get(state["intent"], "fallback")  # é»˜è®¤è·¯å¾„
```

**3. è®°å½•è·¯ç”±å†³ç­– (è°ƒè¯•)**
```python
def logged_router(state):
    decision = make_routing_decision(state)
    print(f"Routing to: {decision}")  # ç”Ÿäº§ç¯å¢ƒç”¨ logging
    return decision
```

**4. éªŒè¯è·¯å¾„å­˜åœ¨**
```python
# åœ¨å›¾ä¸­å®šä¹‰æ‰€æœ‰è·¯å¾„
graph.add_conditional_edges(
    "source",
    router,
    {
        "path_a": "node_a",
        "path_b": "node_b",
        # ç¡®ä¿ router è¿”å›çš„æ‰€æœ‰å€¼éƒ½åœ¨è¿™é‡Œå®šä¹‰
    }
)
```

**å…³é”®æ´å¯Ÿ:**

1. **è·¯ç”±å‡½æ•°æ˜¯"æµç¨‹æ§åˆ¶çš„å¤§è„‘"**
   - å†³å®š Agent çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨
   - æ˜¯ LangGraph åŠ¨æ€æ€§çš„æ ¸å¿ƒ

2. **çŠ¶æ€é©±åŠ¨è·¯ç”±**
   ```
   State â†’ Router â†’ Next Node
   ```

3. **å¯æµ‹è¯•æ€§**
   ```python
   # è·¯ç”±å‡½æ•°æ˜¯çº¯å‡½æ•°,å®¹æ˜“æµ‹è¯•
   assert router({"score": 0.9}) == "high"
   assert router({"score": 0.3}) == "low"
   ```

</details>

---

#### **é—®é¢˜ 10: åœ¨ LangGraph ä¸­,å¦‚ä½•å®ç°å¾ªç¯?ä¸¾ä¾‹è¯´æ˜ä¸€ä¸ªéœ€è¦å¾ªç¯çš„çœŸå®åœºæ™¯ã€‚**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**å®ç°å¾ªç¯çš„æ–¹æ³•:**

LangGraph ä¸­çš„å¾ªç¯é€šè¿‡**è¾¹è¿æ¥**å®ç°:

```python
# å¾ªç¯çš„æœ¬è´¨: ä»åé¢çš„èŠ‚ç‚¹è¿å›å‰é¢çš„èŠ‚ç‚¹
graph.add_edge("node_b", "node_a")  # node_b â†’ node_a (å½¢æˆå¾ªç¯)
```

**åŸºç¡€å¾ªç¯ç»“æ„:**

```python
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    count: int
    max_iterations: int

def increment_node(state):
    return {"count": state["count"] + 1}

def check_node(state) -> Literal["continue", "end"]:
    if state["count"] < state["max_iterations"]:
        return "continue"  # ç»§ç»­å¾ªç¯
    return "end"           # è·³å‡ºå¾ªç¯

graph = StateGraph(State)
graph.add_node("increment", increment_node)

graph.add_edge(START, "increment")
graph.add_conditional_edges(
    "increment",
    check_node,
    {
        "continue": "increment",  # å›åˆ°è‡ªå·±,å½¢æˆå¾ªç¯
        "end": END
    }
)

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

result = app.invoke({"count": 0, "max_iterations": 5})
# æ‰§è¡Œæµç¨‹: START â†’ increment â†’ increment â†’ ... â†’ END
# count: 0 â†’ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5
```

---

**çœŸå®åœºæ™¯ 1: ReAct Agent (Reasoning + Acting)**

è¿™æ˜¯æœ€ç»å…¸çš„å¾ªç¯åœºæ™¯:

```python
from langchain_core.messages import BaseMessage, AIMessage, ToolMessage
from langchain_openai import ChatOpenAI
from typing import Annotated
from operator import add

class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add]

# LLM with tools
llm = ChatOpenAI(model="gpt-4")
tools = [search_tool, calculator_tool]
llm_with_tools = llm.bind_tools(tools)

def llm_node(state):
    """LLM å†³ç­–: è°ƒç”¨å·¥å…·æˆ–ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ"""
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

def tool_node(state):
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    last_message = state["messages"][-1]
    tool_calls = last_message.tool_calls

    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    results = []
    for tool_call in tool_calls:
        tool = get_tool(tool_call["name"])
        result = tool.invoke(tool_call["args"])
        results.append(ToolMessage(
            content=str(result),
            tool_call_id=tool_call["id"]
        ))

    return {"messages": results}

def should_continue(state) -> Literal["tools", "end"]:
    """å†³å®šæ˜¯ç»§ç»­è°ƒç”¨å·¥å…·,è¿˜æ˜¯ç»“æŸ"""
    last_message = state["messages"][-1]

    # å¦‚æœ LLM å†³å®šè°ƒç”¨å·¥å…·,ç»§ç»­å¾ªç¯
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"

    # å¦åˆ™ç»“æŸ
    return "end"

# æ„å»ºå›¾
graph = StateGraph(AgentState)
graph.add_node("llm", llm_node)
graph.add_node("tools", tool_node)

graph.add_edge(START, "llm")
graph.add_conditional_edges(
    "llm",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)
graph.add_edge("tools", "llm")  # ğŸ” å…³é”®: å½¢æˆå¾ªç¯

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# è¿è¡Œ
result = app.invoke({
    "messages": [HumanMessage(content="What's 25 * 4, and search for the weather?")]
})

# æ‰§è¡Œæµç¨‹:
# 1. llm â†’ å†³å®šè°ƒç”¨ calculator å’Œ search
# 2. tools â†’ æ‰§è¡Œä¸¤ä¸ªå·¥å…·
# 3. llm â†’ çœ‹åˆ°å·¥å…·ç»“æœ,å†³å®šè°ƒç”¨æ›´å¤šå·¥å…·æˆ–ç»™å‡ºç­”æ¡ˆ
# 4. ... (å¯èƒ½å¤šæ¬¡å¾ªç¯)
# 5. llm â†’ ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ â†’ END
```

**æ‰§è¡Œæµç¨‹å¯è§†åŒ–:**
```
START
  â†“
[llm] "æˆ‘éœ€è¦è®¡ç®— 25*4 å’ŒæŸ¥è¯¢å¤©æ°”"
  â†“ (should_continue â†’ "tools")
[tools] æ‰§è¡Œ calculator(25, 4) å’Œ search("weather")
  â†“
[llm] "ç»“æœæ˜¯ 100,å¤©æ°”æ˜¯æ™´å¤©ã€‚è®©æˆ‘æ€»ç»“ç­”æ¡ˆ"
  â†“ (should_continue â†’ "end")
END
```

---

**çœŸå®åœºæ™¯ 2: é”™è¯¯é‡è¯•æœºåˆ¶**

```python
class RetryState(TypedDict):
    task: str
    result: Optional[str]
    error: Optional[str]
    retry_count: int
    max_retries: int

def execute_task_node(state):
    """æ‰§è¡Œä»»åŠ¡ (å¯èƒ½å¤±è´¥)"""
    try:
        result = risky_operation(state["task"])
        return {"result": result, "error": None}
    except Exception as e:
        return {
            "error": str(e),
            "retry_count": state["retry_count"] + 1
        }

def check_retry(state) -> Literal["retry", "failed", "success"]:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•"""
    # æˆåŠŸ
    if state["result"] is not None:
        return "success"

    # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
    if state["retry_count"] >= state["max_retries"]:
        return "failed"

    # ç»§ç»­é‡è¯•
    return "retry"

graph = StateGraph(RetryState)
graph.add_node("execute", execute_task_node)
graph.add_node("success_handler", lambda s: {"result": "Task completed"})
graph.add_node("failure_handler", lambda s: {"result": "Task failed"})

graph.add_edge(START, "execute")
graph.add_conditional_edges(
    "execute",
    check_retry,
    {
        "retry": "execute",       # ğŸ” é‡è¯•: å›åˆ° execute
        "success": "success_handler",
        "failed": "failure_handler"
    }
)
graph.add_edge("success_handler", END)
graph.add_edge("failure_handler", END)
```

---

**çœŸå®åœºæ™¯ 3: å¯¹è¯æ¾„æ¸…å¾ªç¯**

```python
class ClarificationState(TypedDict):
    user_messages: list[str]
    bot_messages: list[str]
    intent_clear: bool
    clarification_attempts: int

def understand_intent_node(state):
    """å°è¯•ç†è§£ç”¨æˆ·æ„å›¾"""
    latest_message = state["user_messages"][-1]
    intent_clear = is_intent_clear(latest_message)
    return {"intent_clear": intent_clear}

def ask_clarification_node(state):
    """è¯¢é—®æ¾„æ¸…é—®é¢˜"""
    question = generate_clarification_question(state)
    return {
        "bot_messages": [question],
        "clarification_attempts": state["clarification_attempts"] + 1
    }

def final_response_node(state):
    """ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ"""
    response = generate_response(state)
    return {"bot_messages": [response]}

def should_clarify(state) -> Literal["clarify", "respond"]:
    """å†³å®šæ˜¯å¦éœ€è¦æ¾„æ¸…"""
    if state["intent_clear"]:
        return "respond"  # æ„å›¾æ¸…æ¥š,ç›´æ¥å›ç­”

    if state["clarification_attempts"] >= 2:
        return "respond"  # å°è¯•å¤ªå¤šæ¬¡,ç›´æ¥å›ç­”

    return "clarify"  # ç»§ç»­æ¾„æ¸…

graph = StateGraph(ClarificationState)
graph.add_node("understand", understand_intent_node)
graph.add_node("clarify", ask_clarification_node)
graph.add_node("respond", final_response_node)

graph.add_edge(START, "understand")
graph.add_conditional_edges(
    "understand",
    should_clarify,
    {
        "clarify": "clarify",
        "respond": "respond"
    }
)
graph.add_edge("clarify", "understand")  # ğŸ” æ¾„æ¸…åé‡æ–°ç†è§£
graph.add_edge("respond", END)

# å¯¹è¯æµç¨‹:
# User: "æˆ‘æƒ³è®¢ä¸ªä¸œè¥¿"
# Bot: "è¯·é—®æ‚¨æƒ³è®¢ä»€ä¹ˆ?" (clarify)
# User: "æœºç¥¨"
# Bot: "å¥½çš„,è¯·é—®æ˜¯å“ªä¸ªåŸå¸‚?" (clarify)
# User: "åŒ—äº¬åˆ°ä¸Šæµ·"
# Bot: "ä¸ºæ‚¨æŸ¥è¯¢åŒ—äº¬åˆ°ä¸Šæµ·çš„æœºç¥¨..." (respond)
```

---

**å¾ªç¯æ§åˆ¶çš„æœ€ä½³å®è·µ:**

**1. é˜²æ­¢æ— é™å¾ªç¯**
```python
class State(TypedDict):
    iteration_count: int
    max_iterations: int  # å¼ºåˆ¶ä¸Šé™

def router(state):
    if state["iteration_count"] >= state["max_iterations"]:
        return "end"  # å¼ºåˆ¶é€€å‡º
    if should_continue(state):
        return "loop"
    return "end"
```

**2. è®°å½•å¾ªç¯è·¯å¾„ (è°ƒè¯•)**
```python
def debug_node(state):
    print(f"Iteration {state['iteration_count']}: {state['current_step']}")
    return {}
```

**3. æ¸è¿›å¼é€€å‡ºæ¡ä»¶**
```python
def should_continue(state):
    # å¤šä¸ªé€€å‡ºæ¡ä»¶
    if state["found_answer"]:
        return "end"
    if state["retry_count"] > 3:
        return "end"
    if state["time_elapsed"] > 30:
        return "end"
    return "continue"
```

---

**å…³é”®æ´å¯Ÿ:**

1. **å¾ªç¯æ˜¯ Agent æ™ºèƒ½çš„ä½“ç°**
   - ReAct: æ€è€ƒ â†’ è¡ŒåŠ¨ â†’ è§‚å¯Ÿ â†’ æ€è€ƒ (å¾ªç¯)
   - è‡ªæˆ‘ä¿®æ­£: å°è¯• â†’ æ£€æŸ¥ â†’ é‡è¯• (å¾ªç¯)

2. **LangGraph çš„å¾ªç¯æ˜¯"å¯æ§å¾ªç¯"**
   - ä¸åŒäºé€’å½’æˆ– while å¾ªç¯
   - æ¯ä¸€æ­¥éƒ½æ˜¯å¯è§‚æµ‹çš„
   - å¯ä»¥éšæ—¶ä¸­æ–­å’Œæ¢å¤

3. **å¾ªç¯ + æ¡ä»¶è¾¹ = å¼ºå¤§çš„ Agent æ¨¡å¼**
   ```
   å†³ç­–èŠ‚ç‚¹ â‡„ æ‰§è¡ŒèŠ‚ç‚¹
       â†“ (æ¡ä»¶æ»¡è¶³)
     ç»“æŸ
   ```

</details>

---

### ç¬¬ä¸‰éƒ¨åˆ†: LangChain åŸºç¡€ (3 é¢˜)

---

#### **é—®é¢˜ 11: è§£é‡Š LangChain ä¸­çš„ Messages ç±»å‹ã€‚ä¸ºä»€ä¹ˆ LangGraph ä¸­ç»å¸¸ä½¿ç”¨ `messages: list[BaseMessage]` ä½œä¸ºçŠ¶æ€å­—æ®µ?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**LangChain Messages ç±»å‹ä½“ç³»:**

```python
from langchain_core.messages import (
    BaseMessage,      # åŸºç±»
    HumanMessage,     # ç”¨æˆ·æ¶ˆæ¯
    AIMessage,        # AI å›å¤
    SystemMessage,    # ç³»ç»Ÿæç¤º
    ToolMessage,      # å·¥å…·æ‰§è¡Œç»“æœ
    FunctionMessage   # å‡½æ•°è°ƒç”¨ç»“æœ (å·²åºŸå¼ƒ,ç”¨ ToolMessage)
)
```

**æ¯ç§æ¶ˆæ¯çš„ä½œç”¨:**

**1. SystemMessage** ğŸ¤–
```python
SystemMessage(content="You are a helpful assistant")
```
- **ç”¨é€”**: å®šä¹‰ AI çš„è§’è‰²å’Œè¡Œä¸º
- **ä½ç½®**: é€šå¸¸åœ¨å¯¹è¯å¼€å§‹
- **ç‰¹ç‚¹**: ä¸æ˜¾ç¤ºç»™ç”¨æˆ·,åªå½±å“ AI è¡Œä¸º

**2. HumanMessage** ğŸ‘¤
```python
HumanMessage(content="What's the weather today?")
```
- **ç”¨é€”**: ç”¨æˆ·çš„è¾“å…¥
- **æ¥æº**: ç”¨æˆ·ç•Œé¢
- **ç‰¹ç‚¹**: é©±åŠ¨å¯¹è¯çš„è¿›å±•

**3. AIMessage** ğŸ§ 
```python
AIMessage(content="Let me check the weather for you.")

# å¸¦å·¥å…·è°ƒç”¨
AIMessage(
    content="",
    tool_calls=[{
        "id": "call_123",
        "name": "get_weather",
        "args": {"city": "Beijing"}
    }]
)
```
- **ç”¨é€”**: AI çš„å›å¤
- **ç‰¹æ®Š**: å¯ä»¥åŒ…å« `tool_calls` (å·¥å…·è°ƒç”¨è¯·æ±‚)

**4. ToolMessage** ğŸ”§
```python
ToolMessage(
    content='{"temp": 22, "condition": "sunny"}',
    tool_call_id="call_123"
)
```
- **ç”¨é€”**: å·¥å…·æ‰§è¡Œçš„ç»“æœ
- **å…³è”**: é€šè¿‡ `tool_call_id` å…³è”åˆ° AIMessage çš„ tool_call

**æ¶ˆæ¯æµç¤ºä¾‹:**

```python
messages = [
    SystemMessage(content="You are a helpful assistant"),
    HumanMessage(content="What's 25 * 4?"),
    AIMessage(
        content="",
        tool_calls=[{"id": "1", "name": "calculator", "args": {"a": 25, "b": 4}}]
    ),
    ToolMessage(content="100", tool_call_id="1"),
    AIMessage(content="The answer is 100")
]
```

**å¯¹è¯æµç¨‹:**
```
System â†’ è®¾å®šè§’è‰²
Human â†’ æé—®
AI â†’ å†³å®šè°ƒç”¨å·¥å…·
Tool â†’ è¿”å›ç»“æœ
AI â†’ åŸºäºç»“æœå›ç­”
```

---

**ä¸ºä»€ä¹ˆ LangGraph ä½¿ç”¨ `messages: list[BaseMessage]`?**

**åŸå›  1: ç»Ÿä¸€çš„å¯¹è¯å†å²æ ¼å¼** ğŸ“š

```python
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add]  # æ‰€æœ‰æ¶ˆæ¯ç±»å‹çš„åˆ—è¡¨
```

**ä¼˜åŠ¿:**
- **é€šç”¨æ€§**: æ”¯æŒæ‰€æœ‰ç±»å‹çš„æ¶ˆæ¯
- **æ‰©å±•æ€§**: æ–°å¢æ¶ˆæ¯ç±»å‹æ— éœ€æ”¹å˜çŠ¶æ€ç»“æ„
- **å…¼å®¹æ€§**: ç›´æ¥ä¼ ç»™ LLM API

**åŸå›  2: ä¸ LLM API æ— ç¼é›†æˆ** ğŸ”—

```python
def llm_node(state):
    # LangChain çš„ LLM ç›´æ¥æ¥å— messages åˆ—è¡¨
    response = llm.invoke(state["messages"])
    return {"messages": [response]}
```

**LLM API çš„è¾“å…¥æ ¼å¼:**
```json
{
  "messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```

LangChain çš„ Messages è‡ªåŠ¨è½¬æ¢ä¸ºè¿™ç§æ ¼å¼!

**åŸå›  3: æ”¯æŒ Tool Calling å·¥ä½œæµ** ğŸ› ï¸

```python
# å®Œæ•´çš„ ReAct å¾ªç¯
def agent_node(state):
    # 1. LLM å†³ç­– (å¯èƒ½è°ƒç”¨å·¥å…·)
    response = llm.invoke(state["messages"])
    # response å¯èƒ½åŒ…å« tool_calls
    return {"messages": [response]}

def tool_node(state):
    # 2. æ‰§è¡Œå·¥å…·
    last_message = state["messages"][-1]
    results = []
    for tool_call in last_message.tool_calls:
        result = execute_tool(tool_call)
        results.append(ToolMessage(
            content=result,
            tool_call_id=tool_call["id"]
        ))
    return {"messages": results}

# å¾ªç¯: agent â†’ tool â†’ agent â†’ tool â†’ ... â†’ agent (æœ€ç»ˆç­”æ¡ˆ)
```

**Messages åˆ—è¡¨çš„æ¼”å˜:**
```python
[
    HumanMessage("What's 25*4 and search weather?"),
]
â†“ LLM å†³ç­–
[
    HumanMessage(...),
    AIMessage(tool_calls=[calculator, search])
]
â†“ å·¥å…·æ‰§è¡Œ
[
    HumanMessage(...),
    AIMessage(tool_calls=[...]),
    ToolMessage(content="100", tool_call_id="1"),
    ToolMessage(content="Sunny", tool_call_id="2")
]
â†“ LLM æ€»ç»“
[
    ...,
    AIMessage("The answer is 100, and the weather is sunny")
]
```

**åŸå›  4: Reducer æœºåˆ¶çš„å®Œç¾åº”ç”¨** â•

```python
from operator import add

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add]
    #         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ä½¿ç”¨ add reducer
```

**ä¸ºä»€ä¹ˆç”¨ add:**
```python
# ä¸ç”¨ add (é»˜è®¤è¦†ç›–)
state = {"messages": [msg1, msg2]}
node_return = {"messages": [msg3]}
# ç»“æœ: {"messages": [msg3]}  â† æ—§æ¶ˆæ¯ä¸¢å¤±!

# ä½¿ç”¨ add (è¿½åŠ )
state = {"messages": [msg1, msg2]}
node_return = {"messages": [msg3]}
# ç»“æœ: {"messages": [msg1, msg2, msg3]}  â† ä¿ç•™å†å²!
```

---

**å®é™…åº”ç”¨æ¨¡å¼:**

**æ¨¡å¼ 1: ç®€å•å¯¹è¯**
```python
class State(TypedDict):
    messages: Annotated[list[BaseMessage], add]

def chatbot(state):
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

# ä½¿ç”¨
result = app.invoke({
    "messages": [HumanMessage("Hello")]
})
# ç»“æœ: messages = [HumanMessage("Hello"), AIMessage("Hi there!")]
```

**æ¨¡å¼ 2: å¸¦ç³»ç»Ÿæç¤º**
```python
def add_system_message(state):
    if not state["messages"] or state["messages"][0].type != "system":
        return {"messages": [SystemMessage("You are a helpful assistant")]}
    return {}

graph.add_edge(START, "add_system")
graph.add_edge("add_system", "chatbot")
```

**æ¨¡å¼ 3: æ¶ˆæ¯ä¿®å‰ª (é¿å…è¶…é•¿)**
```python
def trim_messages_node(state):
    messages = state["messages"]
    if len(messages) > 10:
        # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ + æœ€å 9 æ¡
        system_msg = [m for m in messages if m.type == "system"]
        recent_msgs = messages[-9:]
        return {"messages": system_msg + recent_msgs}
    return {}
```

---

**æœ€ä½³å®è·µ:**

```python
# âœ… æ¨è: ä½¿ç”¨ Annotated + add
from operator import add
from typing import Annotated

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add]

# âœ… èŠ‚ç‚¹è¿”å›æ–°æ¶ˆæ¯
def node(state):
    new_msg = AIMessage(content="...")
    return {"messages": [new_msg]}  # ä¼šè‡ªåŠ¨è¿½åŠ 

# âŒ é¿å…: ç›´æ¥ä¿®æ”¹ messages
def bad_node(state):
    state["messages"].append(new_msg)  # ä¸æ¨è
    return {}

# âŒ é¿å…: è¿”å›æ‰€æœ‰æ¶ˆæ¯
def bad_node2(state):
    all_messages = state["messages"] + [new_msg]
    return {"messages": all_messages}  # å†—ä½™
```

**å…³é”®æ´å¯Ÿ:**

1. **Messages æ˜¯å¯¹è¯çš„"DNA"**
   - å®Œæ•´è®°å½•å¯¹è¯å†å²
   - åŒ…å«æ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯

2. **list[BaseMessage] æ˜¯"é€šç”¨æ¥å£"**
   - ä¸ LLM API æ— ç¼å¯¹æ¥
   - æ”¯æŒæ‰€æœ‰å¯¹è¯æ¨¡å¼

3. **add Reducer æ˜¯"å†å²ç´¯ç§¯å™¨"**
   - è‡ªåŠ¨ç»´æŠ¤å¯¹è¯å†å²
   - é¿å…æ‰‹åŠ¨ç®¡ç†åˆ—è¡¨

</details>

---

#### **é—®é¢˜ 12: ä»€ä¹ˆæ˜¯ Tools?å¦‚ä½•åœ¨ LangGraph ä¸­ä½¿ç”¨å·¥å…·?è§£é‡Š `bind_tools` çš„ä½œç”¨ã€‚**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**Tools (å·¥å…·) çš„å®šä¹‰:**

Tools æ˜¯ Agent ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’çš„"èƒ½åŠ›",ä½¿ AI ä¸å†å±€é™äºæ–‡æœ¬ç”Ÿæˆ,å¯ä»¥æ‰§è¡Œå®é™…æ“ä½œã€‚

**å·¥å…·çš„ç±»å‹:**
- ğŸ” **æœç´¢å·¥å…·**: Google æœç´¢ã€Wikipedia æŸ¥è¯¢
- ğŸ§® **è®¡ç®—å·¥å…·**: æ•°å­¦è®¡ç®—ã€æ•°æ®åˆ†æ
- ğŸ“Š **æ•°æ®åº“å·¥å…·**: SQL æŸ¥è¯¢ã€æ•°æ®æå–
- ğŸŒ **API å·¥å…·**: å¤©æ°” APIã€æ”¯ä»˜ API
- ğŸ’» **ç³»ç»Ÿå·¥å…·**: æ–‡ä»¶æ“ä½œã€å‘½ä»¤æ‰§è¡Œ

---

**å®šä¹‰å·¥å…·çš„æ–¹æ³•:**

**æ–¹æ³• 1: ä½¿ç”¨ `@tool` è£…é¥°å™¨** (æ¨è)

```python
from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers.

    Args:
        a: First number
        b: Second number
    """
    return a * b

@tool
def search(query: str) -> str:
    """Search for information.

    Args:
        query: The search query
    """
    return f"Search results for: {query}"

# tool å¯¹è±¡åŒ…å«:
# - name: "multiply"
# - description: docstring çš„ç¬¬ä¸€è¡Œ
# - args_schema: è‡ªåŠ¨ä»å‚æ•°æ¨æ–­
```

**é‡è¦**: docstring æ˜¯å·¥å…·çš„"è¯´æ˜ä¹¦",LLM æ ¹æ®å®ƒå†³å®šä½•æ—¶è°ƒç”¨å·¥å…·!

**æ–¹æ³• 2: ä½¿ç”¨ `Tool` ç±»**

```python
from langchain_core.tools import Tool

def my_function(input: str) -> str:
    return f"Processed: {input}"

tool = Tool(
    name="my_tool",
    description="A tool that processes input",
    func=my_function
)
```

**æ–¹æ³• 3: ä½¿ç”¨ `StructuredTool`** (å¤æ‚å‚æ•°)

```python
from langchain_core.tools import StructuredTool
from pydantic import BaseModel, Field

class SearchInput(BaseModel):
    query: str = Field(description="The search query")
    num_results: int = Field(default=5, description="Number of results")

def search_function(query: str, num_results: int) -> str:
    return f"Found {num_results} results for '{query}'"

search_tool = StructuredTool(
    name="search",
    description="Search the web",
    func=search_function,
    args_schema=SearchInput
)
```

---

**`bind_tools` çš„ä½œç”¨:**

`bind_tools` å°†å·¥å…·"ç»‘å®š"åˆ° LLM,ä½¿ LLM çŸ¥é“æœ‰å“ªäº›å·¥å…·å¯ç”¨ã€‚

**ä¸ä½¿ç”¨ bind_tools:**
```python
llm = ChatOpenAI(model="gpt-4")
response = llm.invoke("What's 25 * 4?")
# LLM åªèƒ½æ–‡æœ¬å›ç­”: "25 * 4 = 100"
# ä¸ä¼šè°ƒç”¨è®¡ç®—å™¨å·¥å…·
```

**ä½¿ç”¨ bind_tools:**
```python
llm = ChatOpenAI(model="gpt-4")
tools = [multiply, search]
llm_with_tools = llm.bind_tools(tools)  # ğŸ”‘ å…³é”®æ­¥éª¤

response = llm_with_tools.invoke("What's 25 * 4?")
# LLM å†³å®šè°ƒç”¨ multiply å·¥å…·
# response.tool_calls = [{
#     "id": "call_123",
#     "name": "multiply",
#     "args": {"a": 25, "b": 4}
# }]
```

**bind_tools åšäº†ä»€ä¹ˆ:**
1. **è½¬æ¢å·¥å…·å®šä¹‰** â†’ OpenAI function calling æ ¼å¼
2. **å‘é€ç»™ LLM** â†’ LLM çŸ¥é“æœ‰å“ªäº›å·¥å…·
3. **LLM è¿”å›** â†’ å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·

**OpenAI API æ ¼å¼:**
```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "multiply",
        "description": "Multiply two numbers",
        "parameters": {
          "type": "object",
          "properties": {
            "a": {"type": "integer"},
            "b": {"type": "integer"}
          }
        }
      }
    }
  ]
}
```

---

**åœ¨ LangGraph ä¸­ä½¿ç”¨å·¥å…·:**

**å®Œæ•´çš„ ReAct Agent å®ç°:**

```python
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import BaseMessage, ToolMessage
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode
from typing import Annotated
from operator import add

# 1. å®šä¹‰å·¥å…·
@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

@tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

tools = [multiply, add_numbers]

# 2. åˆ›å»ºå¸¦å·¥å…·çš„ LLM
llm = ChatOpenAI(model="gpt-4")
llm_with_tools = llm.bind_tools(tools)

# 3. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    messages: Annotated[list[BaseMessage], add]

# 4. å®šä¹‰èŠ‚ç‚¹
def agent_node(state):
    """LLM å†³ç­–èŠ‚ç‚¹"""
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}

# ä½¿ç”¨ LangGraph å†…ç½®çš„ ToolNode (è‡ªåŠ¨æ‰§è¡Œå·¥å…·)
tool_node = ToolNode(tools)

# 5. è·¯ç”±å‡½æ•°
def should_continue(state) -> Literal["tools", "end"]:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·"""
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    return "end"

# 6. æ„å»ºå›¾
graph = StateGraph(AgentState)
graph.add_node("agent", agent_node)
graph.add_node("tools", tool_node)

graph.add_edge(START, "agent")
graph.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        "end": END
    }
)
graph.add_edge("tools", "agent")  # å·¥å…·æ‰§è¡Œåå›åˆ° agent

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# 7. ä½¿ç”¨
result = app.invoke({
    "messages": [HumanMessage("What's 25 * 4 plus 10?")]
})

# æ‰§è¡Œæµç¨‹:
# User: "What's 25 * 4 plus 10?"
#   â†“
# Agent: "æˆ‘éœ€è¦å…ˆç®— 25*4, ç„¶ååŠ 10"
#   â†’ tool_calls: [multiply(25, 4)]
#   â†“
# Tools: æ‰§è¡Œ multiply(25, 4) â†’ 100
#   â†’ ToolMessage(content="100", tool_call_id="...")
#   â†“
# Agent: "25*4=100, ç°åœ¨éœ€è¦ 100+10"
#   â†’ tool_calls: [add_numbers(100, 10)]
#   â†“
# Tools: æ‰§è¡Œ add_numbers(100, 10) â†’ 110
#   â†“
# Agent: "æœ€ç»ˆç­”æ¡ˆæ˜¯ 110"
#   â†’ ä¸è°ƒç”¨å·¥å…·,ç›´æ¥è¿”å›
#   â†“
# END
```

---

**æ‰‹åŠ¨å®ç° Tool Node (ç†è§£åŸç†):**

```python
def manual_tool_node(state):
    """æ‰‹åŠ¨æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    last_message = state["messages"][-1]
    tool_calls = last_message.tool_calls

    # å·¥å…·æ˜ å°„
    tool_map = {
        "multiply": multiply,
        "add_numbers": add_numbers
    }

    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
    results = []
    for tool_call in tool_calls:
        # 1. æ‰¾åˆ°å·¥å…·å‡½æ•°
        tool_func = tool_map[tool_call["name"]]

        # 2. æ‰§è¡Œå·¥å…·
        result = tool_func.invoke(tool_call["args"])

        # 3. æ„å»º ToolMessage
        results.append(ToolMessage(
            content=str(result),
            tool_call_id=tool_call["id"]
        ))

    return {"messages": results}
```

---

**å·¥å…·è®¾è®¡çš„æœ€ä½³å®è·µ:**

**1. æ¸…æ™°çš„ docstring**
```python
@tool
def good_tool(query: str, max_results: int = 5) -> str:
    """Search for information on the web.

    Use this tool when you need to find current information,
    news, or facts that you don't have in your knowledge base.

    Args:
        query: The search query (be specific!)
        max_results: Number of results to return (1-10)

    Returns:
        A formatted list of search results
    """
    pass

# âŒ ä¸å¥½çš„ docstring
@tool
def bad_tool(query: str) -> str:
    """Search."""  # å¤ªç®€å•,LLM ä¸çŸ¥é“ä½•æ—¶ç”¨
    pass
```

**2. ç±»å‹æç¤º**
```python
@tool
def typed_tool(a: int, b: int) -> int:
    #             ^^^       ^^^
    # ç±»å‹æç¤ºå¸®åŠ© LLM ç†è§£å‚æ•°ç±»å‹
    """Add two integers."""
    return a + b
```

**3. é”™è¯¯å¤„ç†**
```python
@tool
def safe_tool(url: str) -> str:
    """Fetch content from a URL."""
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        return f"Error fetching URL: {str(e)}"
```

**4. å·¥å…·è¿”å›æ ¼å¼**
```python
# âœ… è¿”å›å­—ç¬¦ä¸² (LLM å®¹æ˜“ç†è§£)
@tool
def format_result(data: dict) -> str:
    """Process data and return formatted string."""
    return json.dumps(data, indent=2)

# âš ï¸ è¿”å›å¤æ‚å¯¹è±¡ (å¯èƒ½éœ€è¦é¢å¤–å¤„ç†)
@tool
def raw_result(data: dict) -> dict:
    return data  # LLM ä¼šæ”¶åˆ° str(data)
```

---

**å…³é”®æ´å¯Ÿ:**

1. **Tools è®© AI ä»"è¯­è¨€æ¨¡å‹"å˜ä¸º"Agent"**
   ```
   LLM â†’ åªèƒ½æ–‡æœ¬ç”Ÿæˆ
   LLM + Tools â†’ å¯ä»¥æ‰§è¡Œå®é™…ä»»åŠ¡
   ```

2. **bind_tools æ˜¯"èƒ½åŠ›æ³¨å†Œ"**
   ```
   bind_tools(tools) â†’ å‘Šè¯‰ LLM å®ƒæœ‰å“ªäº›"è¶…èƒ½åŠ›"
   ```

3. **LangGraph çš„å·¥å…·å¾ªç¯æ˜¯"æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯"**
   ```
   æ€è€ƒ (Agent Node) â†’ å†³å®šè°ƒç”¨å·¥å…·
   è¡ŒåŠ¨ (Tool Node) â†’ æ‰§è¡Œå·¥å…·
   è§‚å¯Ÿ (å›åˆ° Agent Node) â†’ çœ‹ç»“æœ,å†³å®šä¸‹ä¸€æ­¥
   ```

4. **ToolNode æ˜¯ä¾¿æ·å°è£…**
   ```python
   # ä¸ç”¨æ‰‹åŠ¨å†™å·¥å…·æ‰§è¡Œé€»è¾‘
   tool_node = ToolNode(tools)  # è‡ªåŠ¨å¤„ç†æ‰€æœ‰å·¥å…·è°ƒç”¨
   ```

</details>

---

#### **é—®é¢˜ 13: è§£é‡Š LangChain çš„ LCEL (LangChain Expression Language)ã€‚å®ƒä¸ LangGraph æ˜¯ä»€ä¹ˆå…³ç³»?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**LCEL (LangChain Expression Language) æ˜¯ä»€ä¹ˆ?**

LCEL æ˜¯ LangChain çš„**é“¾å¼è°ƒç”¨è¯­æ³•**,ä½¿ç”¨ `|` æ“ä½œç¬¦è¿æ¥ç»„ä»¶ã€‚

**åŸºç¡€è¯­æ³•:**

```python
# LCEL è¯­æ³•
chain = prompt | llm | output_parser

# ç­‰ä»·äº
def chain(input):
    step1 = prompt.invoke(input)
    step2 = llm.invoke(step1)
    step3 = output_parser.invoke(step2)
    return step3

# ä½¿ç”¨
result = chain.invoke({"topic": "LangGraph"})
```

**LCEL çš„æ ¸å¿ƒæ¦‚å¿µ:**

**1. Runnable æ¥å£**

æ‰€æœ‰ LCEL ç»„ä»¶éƒ½å®ç° `Runnable` æ¥å£:

```python
class Runnable:
    def invoke(self, input):
        """åŒæ­¥è°ƒç”¨"""
        pass

    async def ainvoke(self, input):
        """å¼‚æ­¥è°ƒç”¨"""
        pass

    def stream(self, input):
        """æµå¼è°ƒç”¨"""
        pass

    def batch(self, inputs):
        """æ‰¹é‡è°ƒç”¨"""
        pass
```

**2. ç®¡é“æ“ä½œç¬¦ `|`**

```python
# ä¸²è”ç»„ä»¶
component1 | component2 | component3

# æ•°æ®æµ:
# input â†’ component1 â†’ output1 â†’ component2 â†’ output2 â†’ component3 â†’ final_output
```

**3. å¸¸ç”¨ç»„ä»¶**

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# Prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("human", "{question}")
])

# LLM
llm = ChatOpenAI(model="gpt-4")

# Output Parser
output_parser = StrOutputParser()

# ç»„åˆ
chain = prompt | llm | output_parser

# ä½¿ç”¨
result = chain.invoke({"question": "What is LangGraph?"})
```

---

**LCEL ç¤ºä¾‹:**

**ç¤ºä¾‹ 1: ç®€å•é—®ç­”**

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
llm = ChatOpenAI(model="gpt-4")
parser = StrOutputParser()

joke_chain = prompt | llm | parser

result = joke_chain.invoke({"topic": "programming"})
# "Why do programmers prefer dark mode? Because light attracts bugs!"
```

**ç¤ºä¾‹ 2: å¸¦å·¥å…·çš„é“¾**

```python
from langchain_core.tools import tool

@tool
def get_weather(city: str) -> str:
    """Get weather for a city."""
    return f"Weather in {city}: Sunny, 22Â°C"

llm_with_tools = llm.bind_tools([get_weather])

# é“¾: è¾“å…¥ â†’ LLM (å¯èƒ½è°ƒç”¨å·¥å…·) â†’ è¾“å‡º
agent_chain = llm_with_tools | tool_executor | llm | parser
```

**ç¤ºä¾‹ 3: æ¡ä»¶åˆ†æ”¯ (RunnableBranch)**

```python
from langchain_core.runnables import RunnableBranch

# æ ¹æ®è¾“å…¥é€‰æ‹©ä¸åŒçš„é“¾
branch = RunnableBranch(
    (lambda x: "weather" in x["query"], weather_chain),
    (lambda x: "news" in x["query"], news_chain),
    default_chain  # é»˜è®¤é“¾
)

result = branch.invoke({"query": "What's the weather today?"})
```

---

**LCEL vs LangGraph:**

| ç‰¹æ€§ | LCEL | LangGraph |
|------|------|-----------|
| **ç»“æ„** | çº¿æ€§ç®¡é“ | å›¾ç»“æ„ |
| **æµç¨‹æ§åˆ¶** | ç®€å•æ¡ä»¶åˆ†æ”¯ | å¤æ‚æ¡ä»¶è·¯ç”± |
| **å¾ªç¯** | ä¸æ”¯æŒ | åŸç”Ÿæ”¯æŒ |
| **çŠ¶æ€ç®¡ç†** | éšå¼ä¼ é€’ | æ˜¾å¼çŠ¶æ€ |
| **å¯è§†åŒ–** | éš¾ä»¥å¯è§†åŒ– | å›¾å¯è§†åŒ– |
| **é€‚ç”¨åœºæ™¯** | ç®€å•é“¾å¼ä»»åŠ¡ | å¤æ‚ Agent ç³»ç»Ÿ |

**å¯¹æ¯”ç¤ºä¾‹:**

**ä»»åŠ¡**: æ„å»ºä¸€ä¸ª Agent,å¯ä»¥å¤šæ¬¡è°ƒç”¨å·¥å…·

**LCEL å®ç°** (ä¸ä¼˜é›…):
```python
# LCEL ä¸é€‚åˆè¿™ç§åœºæ™¯,éœ€è¦é€’å½’æˆ–å¾ªç¯
def lcel_agent(input):
    result = llm_with_tools.invoke(input)
    while result.tool_calls:
        tool_results = execute_tools(result.tool_calls)
        result = llm_with_tools.invoke(input + tool_results)
    return result

# é—®é¢˜:
# 1. å¾ªç¯é€»è¾‘æ··æ‚åœ¨ä»£ç ä¸­
# 2. çŠ¶æ€ç®¡ç†å›°éš¾
# 3. éš¾ä»¥å¯è§†åŒ–å’Œè°ƒè¯•
```

**LangGraph å®ç°** (æ¸…æ™°):
```python
graph = StateGraph(State)
graph.add_node("agent", agent_node)
graph.add_node("tools", tool_node)
graph.add_conditional_edges("agent", should_continue, {
    "tools": "tools",
    "end": END
})
graph.add_edge("tools", "agent")  # å¾ªç¯

# ä¼˜åŠ¿:
# 1. å¾ªç¯ç»“æ„æ¸…æ™°
# 2. çŠ¶æ€æ˜¾å¼ç®¡ç†
# 3. å¯è§†åŒ–å›¾ç»“æ„
```

---

**LCEL ä¸ LangGraph çš„å…³ç³»:**

**1. LangGraph åº•å±‚ä½¿ç”¨ LCEL**

```python
# LangGraph èŠ‚ç‚¹å¯ä»¥æ˜¯ LCEL é“¾
def my_node(state):
    chain = prompt | llm | parser
    result = chain.invoke(state["input"])
    return {"output": result}

graph.add_node("process", my_node)
```

**2. LCEL æ˜¯ LangGraph çš„"ç»„ä»¶"**

```
LangGraph (å›¾ç¼–æ’)
â”œâ”€ Node 1 (LCEL é“¾)
â”œâ”€ Node 2 (LCEL é“¾)
â””â”€ Node 3 (LCEL é“¾)
```

**3. äº’è¡¥å…³ç³»**

```python
# LCEL: èŠ‚ç‚¹å†…éƒ¨çš„å¤„ç†é€»è¾‘
node_logic = prompt | llm | parser

# LangGraph: èŠ‚ç‚¹ä¹‹é—´çš„æµç¨‹æ§åˆ¶
def node(state):
    result = node_logic.invoke(state["input"])
    return {"output": result}

graph = StateGraph(State)
graph.add_node("node1", node)
graph.add_edge(START, "node1")
```

---

**ä½•æ—¶ä½¿ç”¨ LCEL vs LangGraph:**

**ä½¿ç”¨ LCEL çš„åœºæ™¯** âœ…:
```python
# 1. ç®€å•çš„çº¿æ€§å¤„ç†
chain = prompt | llm | parser

# 2. å•æ¬¡è°ƒç”¨
result = chain.invoke(input)

# 3. ä¸éœ€è¦çŠ¶æ€ç®¡ç†
# 4. ä¸éœ€è¦å¾ªç¯
# 5. ä¸éœ€è¦å¤æ‚è·¯ç”±
```

**ç¤ºä¾‹**: æ–‡æ¡£æ€»ç»“ã€ç¿»è¯‘ã€ç®€å•é—®ç­”

**ä½¿ç”¨ LangGraph çš„åœºæ™¯** âœ…:
```python
# 1. å¤šè½®å¯¹è¯
# 2. å·¥å…·è°ƒç”¨å¾ªç¯
# 3. å¤æ‚å†³ç­–æ ‘
# 4. éœ€è¦äººå·¥ä»‹å…¥
# 5. éœ€è¦çŠ¶æ€æŒä¹…åŒ–
```

**ç¤ºä¾‹**: ReAct Agentã€Multi-Agent ç³»ç»Ÿã€å¤æ‚å·¥ä½œæµ

---

**ç»„åˆä½¿ç”¨ (æœ€ä½³å®è·µ):**

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END

# 1. ç”¨ LCEL å®šä¹‰èŠ‚ç‚¹å†…éƒ¨é€»è¾‘
summarize_chain = (
    ChatPromptTemplate.from_template("Summarize: {text}")
    | ChatOpenAI(model="gpt-4")
    | StrOutputParser()
)

translate_chain = (
    ChatPromptTemplate.from_template("Translate to Chinese: {text}")
    | ChatOpenAI(model="gpt-4")
    | StrOutputParser()
)

# 2. ç”¨ LangGraph ç»„ç»‡æµç¨‹
class State(TypedDict):
    text: str
    summary: str
    translation: str

def summarize_node(state):
    summary = summarize_chain.invoke({"text": state["text"]})
    return {"summary": summary}

def translate_node(state):
    translation = translate_chain.invoke({"text": state["summary"]})
    return {"translation": translation}

graph = StateGraph(State)
graph.add_node("summarize", summarize_node)
graph.add_node("translate", translate_node)
graph.add_edge(START, "summarize")
graph.add_edge("summarize", "translate")
graph.add_edge("translate", END)

app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# ä½¿ç”¨
result = app.invoke({"text": "Long document..."})
# æµç¨‹: æ–‡æ¡£ â†’ æ€»ç»“ (LCEL) â†’ ç¿»è¯‘ (LCEL) â†’ ç»“æœ
```

---

**å…³é”®æ´å¯Ÿ:**

1. **LCEL æ˜¯"é¡ºåºæ‰§è¡Œ"** ğŸš‚
   ```
   A | B | C  â†’  A() â†’ B() â†’ C()
   ```

2. **LangGraph æ˜¯"å›¾æ‰§è¡Œ"** ğŸ—ºï¸
   ```
   A â†’ [å†³ç­–] â†’ B æˆ– C â†’ [å¯èƒ½å›åˆ° A]
   ```

3. **ç»„åˆä½¿ç”¨æœ€å¼ºå¤§** ğŸ’ª
   ```
   LCEL (èŠ‚ç‚¹å†…éƒ¨) + LangGraph (èŠ‚ç‚¹ä¹‹é—´)
   ```

4. **ä» LCEL è¿ç§»åˆ° LangGraph**
   ```python
   # LCEL
   chain = step1 | step2 | step3

   # LangGraph
   graph.add_node("step1", step1_func)
   graph.add_node("step2", step2_func)
   graph.add_node("step3", step3_func)
   graph.add_edge("step1", "step2")
   graph.add_edge("step2", "step3")
   ```

</details>

---

### ç¬¬å››éƒ¨åˆ†: å®è·µä¸åº”ç”¨ (2 é¢˜)

---

#### **é—®é¢˜ 14: è®¾è®¡ä¸€ä¸ªæ™ºèƒ½å®¢æœè·¯ç”±ç³»ç»Ÿçš„ LangGraph æ¶æ„ã€‚è¦æ±‚æ”¯æŒæ„å›¾è¯†åˆ«ã€FAQ è‡ªåŠ¨å›å¤ã€çŸ¥è¯†åº“æŸ¥è¯¢ã€äººå·¥è½¬æ¥å››ç§è·¯å¾„ã€‚**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

**ç³»ç»Ÿæ¶æ„è®¾è®¡:**

```
ç”¨æˆ·è¾“å…¥
    â†“
[æ„å›¾åˆ†ç±»]
    â†“
  [è·¯ç”±å†³ç­–]
    â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
   â†“      â†“      â†“      â†“
[FAQ] [çŸ¥è¯†åº“] [æŠ€æœ¯] [äººå·¥]
   â†“      â†“      â†“      â†“
   â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
             â†“
         [å“åº”ç”Ÿæˆ]
             â†“
           ç»“æŸ
```

**å®Œæ•´å®ç°:**

```python
from typing_extensions import TypedDict, Literal
from typing import Annotated
from operator import add
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START, END

# ========== 1. çŠ¶æ€å®šä¹‰ ==========
class CustomerServiceState(TypedDict):
    # ç”¨æˆ·ä¿¡æ¯
    user_id: str
    user_tier: str  # vip/premium/regular

    # æ¶ˆæ¯å†å²
    messages: Annotated[list[BaseMessage], add]

    # åˆ†ç±»ç»“æœ
    intent: str  # greeting/faq/query/complaint/other
    category: str  # technical/billing/product/other
    sentiment: str  # positive/negative/neutral
    urgency: str  # high/medium/low

    # å¤„ç†è·¯å¾„
    routing_path: str  # faq/knowledge_base/technical/human
    requires_human: bool

    # å“åº”
    response: str
    confidence: float

    # å…ƒæ•°æ®
    timestamp: str
    processing_time: float

# ========== 2. èŠ‚ç‚¹å‡½æ•° ==========

# 2.1 æ„å›¾åˆ†ç±»èŠ‚ç‚¹
def intent_classification_node(state: CustomerServiceState) -> dict:
    """åˆ†æç”¨æˆ·æ¶ˆæ¯,è¯†åˆ«æ„å›¾ã€ç±»åˆ«ã€æƒ…æ„Ÿã€ç´§æ€¥åº¦"""

    user_message = state["messages"][-1].content

    # ä½¿ç”¨ LLM è¿›è¡Œåˆ†ç±»
    classifier_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªå®¢æœæ¶ˆæ¯åˆ†ç±»å™¨ã€‚åˆ†æç”¨æˆ·æ¶ˆæ¯,è¿”å› JSON æ ¼å¼:
        {{
          "intent": "greeting/faq/query/complaint/other",
          "category": "technical/billing/product/other",
          "sentiment": "positive/negative/neutral",
          "urgency": "high/medium/low"
        }}
        """),
        ("human", "{message}")
    ])

    llm = ChatOpenAI(model="gpt-4", temperature=0)
    chain = classifier_prompt | llm

    result = chain.invoke({"message": user_message})

    # è§£æç»“æœ (ç®€åŒ–,å®é™…åº”ç”¨åº”è¯¥ç”¨ output parser)
    import json
    classification = json.loads(result.content)

    return {
        "intent": classification["intent"],
        "category": classification["category"],
        "sentiment": classification["sentiment"],
        "urgency": classification["urgency"]
    }

# 2.2 FAQ å¤„ç†èŠ‚ç‚¹
def faq_node(state: CustomerServiceState) -> dict:
    """å¤„ç†å¸¸è§é—®é¢˜"""

    # FAQ æ•°æ®åº“ (å®é™…åº”ç”¨åº”è¯¥ç”¨å‘é‡æ•°æ®åº“)
    faq_database = {
        "è¥ä¸šæ—¶é—´": "æˆ‘ä»¬çš„è¥ä¸šæ—¶é—´æ˜¯å‘¨ä¸€è‡³å‘¨äº” 9:00-18:00",
        "é€€è´§æ”¿ç­–": "è´­ä¹°å 7 å¤©å†…å¯ä»¥æ— ç†ç”±é€€è´§",
        "é…é€æ—¶é—´": "ä¸€èˆ¬ 3-5 ä¸ªå·¥ä½œæ—¥é€è¾¾",
        "æ”¯ä»˜æ–¹å¼": "æ”¯æŒæ”¯ä»˜å®ã€å¾®ä¿¡ã€é“¶è¡Œå¡æ”¯ä»˜"
    }

    user_message = state["messages"][-1].content

    # ç®€å•åŒ¹é… (å®é™…åº”ç”¨åº”è¯¥ç”¨è¯­ä¹‰æœç´¢)
    response = "æŠ±æ­‰,æœªæ‰¾åˆ°ç›¸å…³ FAQ"
    for key, value in faq_database.items():
        if key in user_message:
            response = value
            break

    return {
        "response": response,
        "confidence": 0.9 if response != "æŠ±æ­‰,æœªæ‰¾åˆ°ç›¸å…³ FAQ" else 0.3,
        "routing_path": "faq"
    }

# 2.3 çŸ¥è¯†åº“æŸ¥è¯¢èŠ‚ç‚¹
def knowledge_base_node(state: CustomerServiceState) -> dict:
    """æŸ¥è¯¢çŸ¥è¯†åº“ (RAG)"""

    user_message = state["messages"][-1].content

    # æ¨¡æ‹Ÿ RAG æµç¨‹
    # 1. å‘é‡æ£€ç´¢
    retrieved_docs = [
        "äº§å“ X çš„æŠ€æœ¯è§„æ ¼æ˜¯...",
        "å…³äºè®¡è´¹æ–¹å¼,æˆ‘ä»¬æ”¯æŒ...",
    ]

    # 2. LLM ç”Ÿæˆç­”æ¡ˆ
    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", """åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜:
        {context}

        å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯,è¯·è¯´æ˜å¹¶å»ºè®®è½¬äººå·¥å®¢æœã€‚
        """),
        ("human", "{question}")
    ])

    llm = ChatOpenAI(model="gpt-4")
    chain = rag_prompt | llm

    response = chain.invoke({
        "context": "\n".join(retrieved_docs),
        "question": user_message
    })

    return {
        "response": response.content,
        "confidence": 0.8,
        "routing_path": "knowledge_base"
    }

# 2.4 æŠ€æœ¯æ”¯æŒèŠ‚ç‚¹
def technical_support_node(state: CustomerServiceState) -> dict:
    """å¤„ç†æŠ€æœ¯é—®é¢˜"""

    user_message = state["messages"][-1].content

    # è°ƒç”¨ä¸“é—¨çš„æŠ€æœ¯æ”¯æŒ LLM (å¯èƒ½æœ‰ç‰¹æ®Šæç¤ºæˆ–å·¥å…·)
    tech_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸“ä¸šçš„æŠ€æœ¯æ”¯æŒå·¥ç¨‹å¸ˆã€‚
        åˆ†æç”¨æˆ·çš„æŠ€æœ¯é—®é¢˜,æä¾›è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆã€‚
        å¦‚æœé—®é¢˜å¤æ‚,å»ºè®®è½¬äººå·¥å·¥ç¨‹å¸ˆã€‚
        """),
        ("human", "{problem}")
    ])

    llm = ChatOpenAI(model="gpt-4")
    chain = tech_prompt | llm

    response = chain.invoke({"problem": user_message})

    # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥
    requires_human = "è½¬äººå·¥" in response.content or state["urgency"] == "high"

    return {
        "response": response.content,
        "requires_human": requires_human,
        "confidence": 0.7,
        "routing_path": "technical"
    }

# 2.5 äººå·¥è½¬æ¥èŠ‚ç‚¹
def human_handoff_node(state: CustomerServiceState) -> dict:
    """è½¬æ¥äººå·¥å®¢æœ"""

    # æ”¶é›†æ‰€æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯
    context = {
        "user_id": state["user_id"],
        "tier": state["user_tier"],
        "intent": state["intent"],
        "category": state["category"],
        "sentiment": state["sentiment"],
        "urgency": state["urgency"],
        "conversation": [m.content for m in state["messages"]]
    }

    response = f"""å·²ä¸ºæ‚¨è½¬æ¥äººå·¥å®¢æœã€‚
    å·¥å•ç¼–å·: {state['user_id']}-{state['timestamp']}
    é¢„è®¡ç­‰å¾…æ—¶é—´: {"ç«‹å³æ¥å…¥" if state['user_tier'] == 'vip' else "3-5 åˆ†é’Ÿ"}
    """

    return {
        "response": response,
        "routing_path": "human",
        "requires_human": True
    }

# 2.6 å“åº”ç”ŸæˆèŠ‚ç‚¹
def response_generation_node(state: CustomerServiceState) -> dict:
    """ç”Ÿæˆæœ€ç»ˆå“åº”"""

    # æ·»åŠ  AI å“åº”åˆ°æ¶ˆæ¯å†å²
    return {
        "messages": [AIMessage(content=state["response"])]
    }

# ========== 3. è·¯ç”±å‡½æ•° ==========

def route_after_classification(state: CustomerServiceState) -> Literal["faq", "knowledge_base", "technical", "human", "end"]:
    """æ ¹æ®åˆ†ç±»ç»“æœè·¯ç”±"""

    # è§„åˆ™ 1: VIP ç”¨æˆ·ç›´æ¥è½¬äººå·¥
    if state["user_tier"] == "vip":
        return "human"

    # è§„åˆ™ 2: æŠ•è¯‰ + è´Ÿé¢æƒ…ç»ª â†’ äººå·¥
    if state["intent"] == "complaint" and state["sentiment"] == "negative":
        return "human"

    # è§„åˆ™ 3: é«˜ç´§æ€¥åº¦ â†’ äººå·¥
    if state["urgency"] == "high":
        return "human"

    # è§„åˆ™ 4: é—®å€™ â†’ ç›´æ¥ç»“æŸ
    if state["intent"] == "greeting":
        return "end"

    # è§„åˆ™ 5: FAQ â†’ FAQ èŠ‚ç‚¹
    if state["intent"] == "faq":
        return "faq"

    # è§„åˆ™ 6: æŠ€æœ¯é—®é¢˜ â†’ æŠ€æœ¯æ”¯æŒ
    if state["category"] == "technical":
        return "technical"

    # è§„åˆ™ 7: å…¶ä»–æŸ¥è¯¢ â†’ çŸ¥è¯†åº“
    if state["intent"] == "query":
        return "knowledge_base"

    # é»˜è®¤: ç»“æŸ
    return "end"

def route_after_processing(state: CustomerServiceState) -> Literal["response", "human"]:
    """å¤„ç†åçš„è·¯ç”±: æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬äººå·¥"""

    # ç½®ä¿¡åº¦ä½ â†’ è½¬äººå·¥
    if state["confidence"] < 0.5:
        return "human"

    # æ˜ç¡®æ ‡è®°éœ€è¦äººå·¥
    if state.get("requires_human", False):
        return "human"

    # æ­£å¸¸å“åº”
    return "response"

# ========== 4. æ„å»ºå›¾ ==========

graph = StateGraph(CustomerServiceState)

# æ·»åŠ èŠ‚ç‚¹
graph.add_node("classify", intent_classification_node)
graph.add_node("faq", faq_node)
graph.add_node("knowledge_base", knowledge_base_node)
graph.add_node("technical", technical_support_node)
graph.add_node("human", human_handoff_node)
graph.add_node("respond", response_generation_node)

# æ·»åŠ è¾¹
graph.add_edge(START, "classify")

# åˆ†ç±»åçš„æ¡ä»¶è·¯ç”±
graph.add_conditional_edges(
    "classify",
    route_after_classification,
    {
        "faq": "faq",
        "knowledge_base": "knowledge_base",
        "technical": "technical",
        "human": "human",
        "end": END
    }
)

# FAQ å¤„ç†åæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬äººå·¥
graph.add_conditional_edges(
    "faq",
    route_after_processing,
    {
        "response": "respond",
        "human": "human"
    }
)

# çŸ¥è¯†åº“æŸ¥è¯¢åæ£€æŸ¥
graph.add_conditional_edges(
    "knowledge_base",
    route_after_processing,
    {
        "response": "respond",
        "human": "human"
    }
)

# æŠ€æœ¯æ”¯æŒåæ£€æŸ¥
graph.add_conditional_edges(
    "technical",
    route_after_processing,
    {
        "response": "respond",
        "human": "human"
    }
)

# æ‰€æœ‰è·¯å¾„æœ€ç»ˆéƒ½åˆ° END
graph.add_edge("respond", END)
graph.add_edge("human", END)

# ç¼–è¯‘
app = graph.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(app.get_graph().draw_mermaid_png()))

# ========== 5. ä½¿ç”¨ç¤ºä¾‹ ==========

# æµ‹è¯•ç”¨ä¾‹ 1: FAQ
result1 = app.invoke({
    "user_id": "user123",
    "user_tier": "regular",
    "messages": [HumanMessage("ä½ ä»¬çš„è¥ä¸šæ—¶é—´æ˜¯?")],
    "timestamp": "2024-10-30 10:00:00"
})

print("æµ‹è¯• 1 - FAQ:")
print(f"è·¯å¾„: {result1['routing_path']}")
print(f"å“åº”: {result1['response']}")
print()

# æµ‹è¯•ç”¨ä¾‹ 2: æŠ€æœ¯é—®é¢˜
result2 = app.invoke({
    "user_id": "user456",
    "user_tier": "premium",
    "messages": [HumanMessage("æˆ‘çš„è½¯ä»¶å´©æºƒäº†,æ— æ³•å¯åŠ¨")],
    "timestamp": "2024-10-30 10:05:00"
})

print("æµ‹è¯• 2 - æŠ€æœ¯é—®é¢˜:")
print(f"è·¯å¾„: {result2['routing_path']}")
print(f"å“åº”: {result2['response'][:100]}...")
print()

# æµ‹è¯•ç”¨ä¾‹ 3: VIP ç”¨æˆ·
result3 = app.invoke({
    "user_id": "user789",
    "user_tier": "vip",
    "messages": [HumanMessage("æˆ‘éœ€è¦å’¨è¯¢ä¸€ä¸ªé—®é¢˜")],
    "timestamp": "2024-10-30 10:10:00"
})

print("æµ‹è¯• 3 - VIP ç”¨æˆ·:")
print(f"è·¯å¾„: {result3['routing_path']}")
print(f"å“åº”: {result3['response']}")
```

**æ¶æ„äº®ç‚¹:**

1. **å¤šç»´åº¦åˆ†ç±»**: æ„å›¾ã€ç±»åˆ«ã€æƒ…æ„Ÿã€ç´§æ€¥åº¦
2. **æ™ºèƒ½è·¯ç”±**: åŸºäºè§„åˆ™ + åŠ¨æ€å†³ç­–
3. **ç½®ä¿¡åº¦æ£€æŸ¥**: ä½ç½®ä¿¡åº¦è‡ªåŠ¨è½¬äººå·¥
4. **VIP ä¼˜å…ˆ**: é«˜ä»·å€¼ç”¨æˆ·å¿«é€Ÿé€šé“
5. **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°çš„å¤„ç†èŠ‚ç‚¹

**ç”Ÿäº§ç¯å¢ƒå¢å¼º:**

```python
# 1. æ·»åŠ ç›‘æ§
def monitoring_node(state):
    log_metrics({
        "user_tier": state["user_tier"],
        "intent": state["intent"],
        "routing_path": state["routing_path"],
        "confidence": state["confidence"],
        "requires_human": state.get("requires_human", False)
    })
    return {}

# 2. æ·»åŠ ç¼“å­˜
def cached_faq_node(state):
    cache_key = hash(state["messages"][-1].content)
    if cache_key in faq_cache:
        return faq_cache[cache_key]
    result = faq_node(state)
    faq_cache[cache_key] = result
    return result

# 3. æ·»åŠ  A/B æµ‹è¯•
def ab_test_router(state):
    if state["user_id"] % 2 == 0:
        return "strategy_a"
    return "strategy_b"
```

**å…³é”®æ´å¯Ÿ:**

1. **çŠ¶æ€é©±åŠ¨è·¯ç”±**: åŸºäºå¤šä¸ªç»´åº¦åšå†³ç­–
2. **æ¸è¿›å¼é™çº§**: è‡ªåŠ¨ â†’ åŠè‡ªåŠ¨ â†’ äººå·¥
3. **ç”¨æˆ·åˆ†å±‚**: ä¸åŒç”¨æˆ·ä¸åŒæœåŠ¡ç­–ç•¥
4. **å¯è§‚æµ‹æ€§**: æ¯ä¸€æ­¥éƒ½è®°å½•è·¯å¾„å’Œç½®ä¿¡åº¦

</details>

---

#### **é—®é¢˜ 15: æ€»ç»“ Module 1 çš„æ ¸å¿ƒè¦ç‚¹ã€‚å¦‚æœåªèƒ½è®°ä½ 5 ä¸ªæœ€é‡è¦çš„æ¦‚å¿µ,ä½ ä¼šé€‰æ‹©å“ªäº›?ä¸ºä»€ä¹ˆ?**

<details>
<summary>ğŸ’¡ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ</summary>

**ç­”æ¡ˆ:**

å¦‚æœåªèƒ½è®°ä½ 5 ä¸ªæ¦‚å¿µ,æˆ‘ä¼šé€‰æ‹©:

---

**1. å›¾çŠ¶æ€æœºæ€ç»´ (æ ¸å¿ƒå“²å­¦)** ğŸ¯

**æ˜¯ä»€ä¹ˆ:**
LangGraph å°† Agent å»ºæ¨¡ä¸º**çŠ¶æ€æœº**,æ‰§è¡Œè¿‡ç¨‹æ˜¯**çŠ¶æ€çš„è¿ç»­è½¬æ¢**ã€‚

**ä¸ºä»€ä¹ˆé‡è¦:**
è¿™æ˜¯ä»"å¸Œæœ› AI å·¥ä½œ"åˆ°"ç¡®ä¿ AI å·¥ä½œ"çš„æ ¹æœ¬è½¬å˜ã€‚

**è®°ä½è¿™ä¸ª:**
```
ä¼ ç»Ÿ: è°ƒç”¨ â†’ é»‘ç›’ â†’ å¸Œæœ›å¾—åˆ°æ­£ç¡®ç»“æœ
LangGraph: çŠ¶æ€ â†’ èŠ‚ç‚¹ â†’ æ–°çŠ¶æ€ â†’ å¯æ§è·¯ç”± â†’ å¯é¢„æµ‹ç»“æœ
```

**åº”ç”¨:**
è®¾è®¡ä»»ä½• Agent æ—¶,å…ˆé—®:
- éœ€è¦å“ªäº›çŠ¶æ€?
- çŠ¶æ€å¦‚ä½•è½¬æ¢?
- è½¬æ¢çš„æ¡ä»¶æ˜¯ä»€ä¹ˆ?

---

**2. State = æ•°æ® + Channels + Reducers (æŠ€æœ¯æ ¸å¿ƒ)** ğŸ“Š

**æ˜¯ä»€ä¹ˆ:**
```python
class State(TypedDict):
    messages: Annotated[list[BaseMessage], add]  # Channel + Reducer
    #         ^^^^^^^^ æ•°æ®ç±»å‹
    #                  ^^^^^^^^^^^^^^^^^^^^ æ›´æ–°è§„åˆ™
```

**ä¸ºä»€ä¹ˆé‡è¦:**
State è®¾è®¡å†³å®šäº†:
- Agent èƒ½è®°ä½ä»€ä¹ˆ
- èŠ‚ç‚¹é—´å¦‚ä½•é€šä¿¡
- æ•°æ®å¦‚ä½•ç´¯ç§¯

**è®°ä½è¿™ä¸ª:**
- State æ˜¯"å…±äº«å†…å­˜"
- Channels æ˜¯"ç‹¬ç«‹é€šé“"
- Reducers å†³å®š"åˆå¹¶ç­–ç•¥" (è¦†ç›– vs è¿½åŠ )

**å¸¸è§é”™è¯¯:**
```python
# âŒ å¿˜è®° add reducer
messages: list  # ä¼šè¢«è¦†ç›–!

# âœ… æ­£ç¡®
messages: Annotated[list, add]  # ä¼šè¿½åŠ 
```

---

**3. Node = State â†’ State (å‡½æ•°å¼æ€ç»´)** âš™ï¸

**æ˜¯ä»€ä¹ˆ:**
èŠ‚ç‚¹æ˜¯**çº¯å‡½æ•°**,è¾“å…¥çŠ¶æ€,è¾“å‡ºæ›´æ–°ã€‚

```python
def node(state: State) -> dict:
    # 1. è¯»å–çŠ¶æ€
    data = state["field"]

    # 2. å¤„ç†é€»è¾‘
    result = process(data)

    # 3. è¿”å›æ›´æ–° (éƒ¨åˆ†æ›´æ–°!)
    return {"field": result}
```

**ä¸ºä»€ä¹ˆé‡è¦:**
- èŠ‚ç‚¹æ˜¯å¯æµ‹è¯•çš„å•å…ƒ
- èŠ‚ç‚¹æ˜¯å¯å¤ç”¨çš„ç»„ä»¶
- èŠ‚ç‚¹ç»„åˆæˆå¤æ‚ç³»ç»Ÿ

**è®°ä½è¿™ä¸ª:**
```
èŠ‚ç‚¹ç­¾å: (State) â†’ dict
ä¸æ˜¯: (State) â†’ State  â† ä¸è¦è¿”å›å®Œæ•´çŠ¶æ€
```

**æœ€ä½³å®è·µ:**
- ä¸€ä¸ªèŠ‚ç‚¹åšä¸€ä»¶äº‹
- çº¯å‡½æ•°,æ— å‰¯ä½œç”¨
- åªè¿”å›æ”¹å˜çš„å­—æ®µ

---

**4. Conditional Edge = åŠ¨æ€è·¯ç”± (æ§åˆ¶æµçš„çµé­‚)** ğŸš¦

**æ˜¯ä»€ä¹ˆ:**
æ ¹æ®çŠ¶æ€åŠ¨æ€å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚

```python
def router(state) -> Literal["path_a", "path_b"]:
    if state["score"] > 0.8:
        return "path_a"
    return "path_b"

graph.add_conditional_edges("source", router, {
    "path_a": "node_a",
    "path_b": "node_b"
})
```

**ä¸ºä»€ä¹ˆé‡è¦:**
è¿™æ˜¯ Agent **æ™ºèƒ½**çš„ä½“ç°:
- ä¸åŒæƒ…å†µèµ°ä¸åŒè·¯å¾„
- æ”¯æŒå¾ªç¯ (ReAct)
- å®ç°å¤æ‚å†³ç­–æ ‘

**è®°ä½è¿™ä¸ª:**
```
æ²¡æœ‰æ¡ä»¶è¾¹ = æµæ°´çº¿ (å›ºå®šæµç¨‹)
æœ‰æ¡ä»¶è¾¹ = Agent (åŠ¨æ€å†³ç­–)
```

**åº”ç”¨æ¨¡å¼:**
- ReAct: `agent â†’ tools â†’ agent` (å¾ªç¯)
- é”™è¯¯å¤„ç†: `try â†’ [success/retry/fail]`
- ç”¨æˆ·åˆ†æµ: `classify â†’ [vip/regular/...]`

---

**5. Human-in-the-Loop = å¯æ§æ€§ (ç”Ÿäº§ç¯å¢ƒçš„å¿…éœ€å“)** ğŸ‘¤

**æ˜¯ä»€ä¹ˆ:**
åœ¨å…³é”®èŠ‚ç‚¹æš‚åœ,ç­‰å¾…äººå·¥å†³ç­–ã€‚

```python
app = graph.compile(
    checkpointer=memory,  # å¿…éœ€
    interrupt_before=["critical_action"]
)

# æ‰§è¡Œåˆ°æ–­ç‚¹æš‚åœ
result = app.invoke(input, config)

# äººå·¥å®¡æŸ¥åç»§ç»­
result = app.invoke(None, config)
```

**ä¸ºä»€ä¹ˆé‡è¦:**
- **å®‰å…¨**: é˜²æ­¢ AI åšå±é™©æ“ä½œ
- **å¯é **: äººå·¥å…œåº•,æé«˜å‡†ç¡®æ€§
- **åˆè§„**: æŸäº›åœºæ™¯æ³•å¾‹è¦æ±‚äººå·¥å®¡æ‰¹

**è®°ä½è¿™ä¸ª:**
```
å®éªŒå®¤ AI: å®Œå…¨è‡ªåŠ¨
ç”Ÿäº§çº§ AI: è‡ªåŠ¨ + äººå·¥å®¡æ‰¹
```

**åº”ç”¨åœºæ™¯:**
- é‡‘è: å¤§é¢äº¤æ˜“å®¡æ‰¹
- åŒ»ç–—: è¯Šæ–­å»ºè®®ç¡®è®¤
- æ³•å¾‹: åˆåŒå®¡æŸ¥
- è¿ç»´: å±é™©æ“ä½œç¡®è®¤

---

**ä¸ºä»€ä¹ˆæ˜¯è¿™ 5 ä¸ª?**

**1. è¦†ç›–å®Œæ•´æ€§** ğŸ“
```
å›¾æ€ç»´ (å“²å­¦)
  â†“
State (æ•°æ®)
  â†“
Node (å¤„ç†)
  â†“
Conditional Edge (æ§åˆ¶æµ)
  â†“
Human-in-the-Loop (å¯æ§æ€§)
```

**2. ç†è®º + å®è·µå¹³è¡¡** âš–ï¸
- ç†è®º: å›¾æ€ç»´ã€State è®¾è®¡
- å®è·µ: Node ç¼–å†™ã€Edge è·¯ç”±ã€HITL å®ç°

**3. ä»ç®€å•åˆ°å¤æ‚çš„è·¯å¾„** ğŸ“ˆ
```
Level 1: ç†è§£ State å’Œ Node (åŸºç¡€)
Level 2: ä½¿ç”¨ Conditional Edge (è¿›é˜¶)
Level 3: å®ç° HITL (ç”Ÿäº§çº§)
```

**4. æœ€é«˜ ROI** ğŸ’°
è¿™ 5 ä¸ªæ¦‚å¿µ:
- å ç”¨ 20% çš„å­¦ä¹ æ—¶é—´
- è§£å†³ 80% çš„å®é™…é—®é¢˜
- æ˜¯æ‰€æœ‰é«˜çº§ç‰¹æ€§çš„åŸºç¡€

---

**å…¶ä»–é‡è¦ä½†å¯ä»¥åå­¦çš„æ¦‚å¿µ:**

- **Persistence (æŒä¹…åŒ–)**: é‡è¦,ä½†åŸºäº State ç†è§£
- **Streaming (æµå¼è¾“å‡º)**: ä½“éªŒä¼˜åŒ–,éæ ¸å¿ƒé€»è¾‘
- **Sub-graphs (å­å›¾)**: æ¨¡å—åŒ–,å»ºç«‹åœ¨å›¾ç†è§£ä¹‹ä¸Š
- **Multi-Agent (å¤šæ™ºèƒ½ä½“)**: é«˜çº§åº”ç”¨,ä¾èµ–åŸºç¡€æ¦‚å¿µ

---

**å­¦ä¹ æ£€éªŒ:**

å¦‚æœä½ çœŸæ­£ç†è§£è¿™ 5 ä¸ªæ¦‚å¿µ,ä½ åº”è¯¥èƒ½:

1. âœ… è®¾è®¡ä¸€ä¸ª Agent çš„ State Schema
2. âœ… å†™å‡ºè‡³å°‘ 3 ä¸ªèŠ‚ç‚¹å‡½æ•°
3. âœ… å®ç°ä¸€ä¸ªæ¡ä»¶è¾¹è·¯ç”±
4. âœ… åœ¨å…³é”®èŠ‚ç‚¹æ·»åŠ  Breakpoint
5. âœ… è§£é‡Šä¸ºä»€ä¹ˆ LangGraph æ¯”çº¿æ€§ Chain æ›´å¼ºå¤§

**æœ€ç»ˆæ´å¯Ÿ:**

> **LangGraph ä¸æ˜¯ä¸€ä¸ªæ¡†æ¶,è€Œæ˜¯ä¸€ç§æ€ç»´æ–¹å¼ã€‚**
>
> å½“ä½ å¼€å§‹ç”¨"çŠ¶æ€"ã€"èŠ‚ç‚¹"ã€"è¾¹"çš„è¯­è¨€æ€è€ƒé—®é¢˜æ—¶,ä½ å°±æŒæ¡äº† LangGraph çš„ç²¾é«“ã€‚
>
> ä»è¿™ 5 ä¸ªæ¦‚å¿µå‡ºå‘,ä½ å¯ä»¥æ„å»ºä»»æ„å¤æ‚åº¦çš„ Agent ç³»ç»Ÿã€‚

</details>

---

## ğŸ¯ å­¦ä¹ æ•ˆæœè¯„ä¼°

å®Œæˆè¿™ 15 ä¸ªé—®é¢˜å,è¯·è¯šå®åœ°è¯„ä¼°è‡ªå·±:

**åŸºç¡€ç†è§£ (é—®é¢˜ 1-5)**
- [ ] æˆ‘èƒ½è§£é‡Šä¸ºä»€ä¹ˆé€‰æ‹© LangGraph
- [ ] æˆ‘ç†è§£å›¾æ€ç»´ vs é“¾å¼æ€ç»´
- [ ] æˆ‘çŸ¥é“ Human-in-the-Loop çš„åº”ç”¨åœºæ™¯

**æ ¸å¿ƒæ¦‚å¿µ (é—®é¢˜ 6-10)**
- [ ] æˆ‘èƒ½ç”»å‡º Stateã€Nodeã€Edge çš„å…³ç³»å›¾
- [ ] æˆ‘ç†è§£ Channels å’Œ Reducers
- [ ] æˆ‘èƒ½å†™å‡ºå®Œæ•´çš„ LangGraph åº”ç”¨æ¨¡æ¿
- [ ] æˆ‘èƒ½å®ç°æ¡ä»¶è¾¹è·¯ç”±
- [ ] æˆ‘ç†è§£å¦‚ä½•å®ç°å¾ªç¯

**LangChain åŸºç¡€ (é—®é¢˜ 11-13)**
- [ ] æˆ‘ç†è§£ Messages ç±»å‹ä½“ç³»
- [ ] æˆ‘èƒ½å®šä¹‰å’Œä½¿ç”¨ Tools
- [ ] æˆ‘ç†è§£ LCEL ä¸ LangGraph çš„å…³ç³»

**å®è·µåº”ç”¨ (é—®é¢˜ 14-15)**
- [ ] æˆ‘èƒ½è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„ Agent æ¶æ„
- [ ] æˆ‘èƒ½æ€»ç»“æ ¸å¿ƒè¦ç‚¹

**å¦‚æœæœ‰è¶…è¿‡ 3 ä¸ªæœªé€‰ä¸­,å»ºè®®:**
- ğŸ”„ é‡æ–°å­¦ä¹ ç›¸å…³ç« èŠ‚
- ğŸ’» åŠ¨æ‰‹å®ç°å¯¹åº”çš„ä»£ç ç¤ºä¾‹
- ğŸ¤ åœ¨ç¤¾åŒºå¯»æ±‚å¸®åŠ©

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

æƒ³è¦æ›´æ·±å…¥ç†è§£ Module 1 çš„å†…å®¹?æ¨èä»¥ä¸‹èµ„æº:

1. **LangGraph å®˜æ–¹æ•™ç¨‹**: https://langchain-ai.github.io/langgraph/tutorials/
2. **ReAct è®ºæ–‡**: ç†è§£ Reasoning + Acting å¾ªç¯
3. **LangChain Academy**: å®˜æ–¹è¯¾ç¨‹ Module 0-1

---

## ğŸ’¬ ç»“è¯­

æ­å–œä½ å®Œæˆ Module 1 çš„å­¦ä¹ å’Œå¤ä¹ !

è¿™ 15 ä¸ªé—®é¢˜æ¶µç›–äº† LangGraph çš„æ ¸å¿ƒçŸ¥è¯†ã€‚å¦‚æœä½ èƒ½æµç•…å›ç­”å…¶ä¸­ 12 ä¸ªä»¥ä¸Š,è¯´æ˜ä½ å·²ç»å»ºç«‹äº†æ‰å®çš„åŸºç¡€ã€‚

**è®°ä½:**
- **ç†è§£ > è®°å¿†**: ç†è§£æ¦‚å¿µæ¯”è®°ä½ API æ›´é‡è¦
- **å®è·µ > ç†è®º**: åŠ¨æ‰‹å†™ä»£ç æ¯”çœ‹æ–‡æ¡£æ›´æœ‰æ•ˆ
- **æ€»ç»“ > ç§¯ç´¯**: å®šæœŸå›é¡¾æ¯”ä¸€æ¬¡æ€§å­¦ä¹ æ›´æŒä¹…

---

**å‡†å¤‡å¥½è¿›å…¥ Module 2 äº†å—?**

åœ¨ Module 2 ä¸­,æˆ‘ä»¬å°†å­¦ä¹ :
- æ›´å¤æ‚çš„ Agent æ¶æ„æ¨¡å¼
- è®°å¿†ç®¡ç†å’ŒæŒä¹…åŒ–
- ç”Ÿäº§çº§éƒ¨ç½²

â¡ï¸ **[è¿›å…¥ Module 2: LangGraph æ ¸å¿ƒæ¨¡å¼](../module-2/2.0-æœ¬ç« ä»‹ç».md)**

---

*Module 1 å¤ä¹ æ’°å†™è€…*
*ä¸€ä½ç›¸ä¿¡"ä¸»åŠ¨å›é¡¾æ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹æ³•"çš„æ•™è‚²è€…*
*2024 å¹´ 10 æœˆ*
