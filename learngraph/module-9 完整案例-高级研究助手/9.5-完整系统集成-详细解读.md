# LangGraph å®Œæ•´ç³»ç»Ÿé›†æˆè¯¦ç»†è§£è¯»

## ğŸ“š æ¦‚è¿°

æœ¬ç« å°†å‰é¢æ‰€æœ‰ç»„ä»¶æ•´åˆä¸ºå®Œæ•´çš„ç«¯åˆ°ç«¯ **Deep Research ç³»ç»Ÿ**ã€‚åŒ…æ‹¬éœ€æ±‚æ¾„æ¸…ã€ç ”ç©¶è§„åˆ’ã€å¹¶è¡Œç ”ç©¶ã€æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆçš„å®Œæ•´æµç¨‹ã€‚

**å®Œæ•´æµç¨‹ï¼š**
```
ç”¨æˆ·è¾“å…¥
  â†“
Scoping (9.1)
  â”œâ”€ éœ€æ±‚æ¾„æ¸…
  â””â”€ ç ”ç©¶ç®€æŠ¥ç”Ÿæˆ
  â†“
Multi-Agent Research (9.2-9.4)
  â”œâ”€ Supervisor å†³ç­–
  â”œâ”€ å¹¶è¡Œ Research Agents
  â””â”€ ç»“æœèšåˆ
  â†“
Report Generation (æœ¬ç« æ–°å¢)
  â””â”€ ç»¼åˆæ‰€æœ‰å‘ç°ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
  â†“
è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶é›†æˆ

### 1. æœ€ç»ˆæŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹

```python
from langchain.chat_models import init_chat_model

writer_model = init_chat_model(
    model="openai:gpt-4.1",
    max_tokens=32000  # å…è®¸é•¿æŠ¥å‘Š
)

async def final_report_generation(state: AgentState):
    """
    ç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š

    è¾“å…¥:
    - research_brief: åŸå§‹ç ”ç©¶ä¸»é¢˜
    - notes: æ‰€æœ‰ Agent çš„å‹ç¼©ç ”ç©¶ç»“æœ
    - raw_notes: è¯¦ç»†çš„åŸå§‹ç¬”è®°

    è¾“å‡º:
    - final_report: ç»“æ„åŒ–çš„æœ€ç»ˆæŠ¥å‘Š
    """
    # è·å–æ‰€æœ‰ç ”ç©¶å‘ç°
    notes = state.get("notes", [])
    findings = "\n\n".join(notes)

    # ç”ŸæˆæŠ¥å‘Šçš„ Prompt
    final_report_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶æŠ¥å‘Šæ’°å†™ä¸“å®¶ã€‚

    åŸå§‹ç ”ç©¶ä¸»é¢˜:
    {state.get("research_brief", "")}

    ç ”ç©¶å‘ç°:
    {findings}

    ä»Šå¤©çš„æ—¥æœŸ: {get_today_str()}

    è¯·åŸºäºä¸Šè¿°ç ”ç©¶å‘ç°ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…æ‹¬:

    1. **æ‰§è¡Œæ‘˜è¦** (2-3 æ®µ)
       - ç ”ç©¶ç›®çš„æ¦‚è¿°
       - æ ¸å¿ƒå‘ç°æ€»ç»“
       - å…³é”®ç»“è®º

    2. **è¯¦ç»†å‘ç°** (æŒ‰ä¸»é¢˜ç»„ç»‡)
       - ä½¿ç”¨æ ‡é¢˜å’Œå­æ ‡é¢˜
       - å¼•ç”¨å…·ä½“æ•°æ®å’Œæ¥æº
       - æä¾›è¯æ®é“¾

    3. **ç»¼åˆåˆ†æ**
       - äº¤å‰éªŒè¯å‘ç°
       - è¯†åˆ«æ¨¡å¼å’Œè¶‹åŠ¿
       - æŒ‡å‡ºå·®å¼‚å’Œå…±è¯†

    4. **ç»“è®ºä¸å»ºè®®**
       - å›ç­”åŸå§‹ç ”ç©¶é—®é¢˜
       - æä¾›å¯æ“ä½œå»ºè®®

    5. **æ¥æºåˆ—è¡¨**
       - åˆ—å‡ºæ‰€æœ‰å¼•ç”¨çš„æ¥æº

    ä½¿ç”¨ Markdown æ ¼å¼ï¼Œç¡®ä¿ä¸“ä¸šã€æ¸…æ™°ã€å…¨é¢ã€‚
    """

    # ç”ŸæˆæŠ¥å‘Š
    response = await writer_model.ainvoke([
        HumanMessage(content=final_report_prompt)
    ])

    return {
        "final_report": response.content,
        "messages": [
            AIMessage(content=f"ç ”ç©¶å®Œæˆï¼ä»¥ä¸‹æ˜¯æœ€ç»ˆæŠ¥å‘Š:\n\n{response.content}")
        ]
    }
```

**å…³é”®ç‚¹ï¼š**
- ä½¿ç”¨ `max_tokens=32000` å…è®¸é•¿æŠ¥å‘Š
- Prompt æ˜ç¡®æŒ‡å®šæŠ¥å‘Šç»“æ„
- å¼•ç”¨åŸå§‹ç ”ç©¶ä¸»é¢˜ä¿æŒèšç„¦

---

### 2. å®Œæ•´å›¾æ„å»º

```python
from langgraph.graph import StateGraph, START, END

# å¯¼å…¥ä¹‹å‰æ„å»ºçš„ç»„ä»¶
from deep_research_from_scratch.research_agent_scope import (
    clarify_with_user,
    write_research_brief
)
from deep_research_from_scratch.multi_agent_supervisor import supervisor_agent

# æ„å»ºå®Œæ•´å·¥ä½œæµ
deep_researcher_builder = StateGraph(AgentState, input_schema=AgentInputState)

# æ·»åŠ èŠ‚ç‚¹
deep_researcher_builder.add_node("clarify_with_user", clarify_with_user)
deep_researcher_builder.add_node("write_research_brief", write_research_brief)
deep_researcher_builder.add_node("supervisor_subgraph", supervisor_agent)  # å­å›¾ï¼
deep_researcher_builder.add_node("final_report_generation", final_report_generation)

# æ·»åŠ è¾¹ï¼ˆçº¿æ€§æµç¨‹ï¼‰
deep_researcher_builder.add_edge(START, "clarify_with_user")
deep_researcher_builder.add_edge("write_research_brief", "supervisor_subgraph")
deep_researcher_builder.add_edge("supervisor_subgraph", "final_report_generation")
deep_researcher_builder.add_edge("final_report_generation", END)

# ç¼–è¯‘
agent = deep_researcher_builder.compile()

# ğŸ¨ å¯è§†åŒ–å›¾ç»“æ„
from IPython.display import Image, display
display(Image(agent.get_graph().draw_mermaid_png()))
```

**å…³é”®æŠ€æœ¯ï¼šå­å›¾é›†æˆ**

```python
# supervisor_agent æœ¬èº«å°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„å›¾
supervisor_agent = supervisor_builder.compile()

# å°†å…¶ä½œä¸ºèŠ‚ç‚¹æ·»åŠ åˆ°ä¸»å›¾ä¸­
deep_researcher_builder.add_node("supervisor_subgraph", supervisor_agent)
```

**å¥½å¤„ï¼š**
- æ¨¡å—åŒ– - æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•
- å¯é‡ç”¨ - supervisor_agent å¯ä»¥å•ç‹¬ä½¿ç”¨
- æ¸…æ™° - ä¸»å›¾åªå…³æ³¨é«˜å±‚æµç¨‹

---

## âš™ï¸ å…³é”®é…ç½®

### 1. é€’å½’é™åˆ¶é…ç½®

**é—®é¢˜ï¼š**
LangGraph é»˜è®¤é€’å½’é™åˆ¶æ˜¯ 25 æ­¥ï¼Œå¤æ‚ç ”ç©¶ä»»åŠ¡å¯èƒ½è¶…å‡ºã€‚

```python
# è®¡ç®—æ­¥éª¤æ•°
Scoping:
  - clarify_with_user: 1 æ­¥
  - (å¯èƒ½çš„ç”¨æˆ·äº¤äº’): 1-2 è½®
  - write_research_brief: 1 æ­¥

Supervisor:
  - supervisor: 1 æ­¥
  - supervisor_tools: 1 æ­¥
  - (å¯èƒ½å¤šè½®): 2-3 è½®

æ¯ä¸ª Research Agent (å‡è®¾ 3 ä¸ªå¹¶è¡Œ):
  - llm_call: 2-3 æ¬¡
  - tool_node: 2-3 æ¬¡
  - compress_research: 1 æ¬¡
  æ¯ä¸ª Agent: 6-8 æ­¥

Final Report:
  - final_report_generation: 1 æ­¥

æ€»è®¡: å¯èƒ½è¾¾åˆ° 30-40 æ­¥
```

**è§£å†³æ–¹æ¡ˆï¼šæé«˜é€’å½’é™åˆ¶**

```python
# åœ¨ invoke æ—¶é…ç½®
thread = {
    "configurable": {
        "thread_id": "1",
        "recursion_limit": 50  # æé«˜åˆ° 50
    }
}

result = await agent.ainvoke(
    {"messages": [HumanMessage(content="...")]},
    config=thread
)
```

---

### 2. Checkpointer é…ç½®ï¼ˆå¯é€‰ï¼‰

```python
from langgraph.checkpoint.memory import MemorySaver

# ç”¨äºæ”¯æŒä¸­æ–­å’Œæ¢å¤
checkpointer = MemorySaver()

agent = deep_researcher_builder.compile(
    checkpointer=checkpointer
)

# ç°åœ¨æ”¯æŒ:
# - ä¸­æ–­åæ¢å¤
# - æŸ¥çœ‹ä¸­é—´çŠ¶æ€
# - æ—¶é—´æ—…è¡Œè°ƒè¯•
```

---

## ğŸ­ å®Œæ•´æ‰§è¡Œç¤ºä¾‹

### ç¤ºä¾‹ï¼šå¯¹æ¯” Gemini vs OpenAI Deep Research

```python
thread = {"configurable": {"thread_id": "1", "recursion_limit": 50}}

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šæ¨¡ç³Šè¯·æ±‚
result = await agent.ainvoke({
    "messages": [HumanMessage(content="Compare Gemini to OpenAI Deep Research")]
}, config=thread)

# Scoping æ¾„æ¸…
print(result['messages'][-1].content)
# è¾“å‡º: "Could you clarify what you mean by 'OpenAI Deep Research'?
#        Are you referring to a specific product or feature?"

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šæä¾›æ¾„æ¸…
result = await agent.ainvoke({
    "messages": [HumanMessage(content="The specific Deep Research products")]
}, config=thread)

# å®Œæ•´æ‰§è¡Œ
print(result['final_report'])
```

**æ‰§è¡Œæµç¨‹ï¼ˆè¯¦ç»†ï¼‰ï¼š**

```
Step 1-2: Scoping
  â”œâ”€ clarify_with_user
  â”‚  â””â”€ æ£€æµ‹åˆ° "Deep Research" éœ€è¦æ¾„æ¸…
  â”‚  â””â”€ è¿”å›é—®é¢˜ç»™ç”¨æˆ·
  â”œâ”€ (ç”¨æˆ·æä¾›æ¾„æ¸…)
  â””â”€ write_research_brief
     â””â”€ ç”Ÿæˆ: "å¯¹æ¯” Gemini å’Œ OpenAI Deep Research äº§å“çš„åŠŸèƒ½ã€æ€§èƒ½ã€ä½¿ç”¨ä½“éªŒ..."

Step 3-20: Multi-Agent Research
  â”œâ”€ supervisor (Step 3)
  â”‚  â””â”€ å†³ç­–: "è¿™æ˜¯å¯¹æ¯”ä»»åŠ¡ï¼Œéœ€è¦ 2 ä¸ªå¹¶è¡Œ Agent"
  â”‚  â””â”€ è°ƒç”¨: ConductResearch(Gemini) + ConductResearch(OpenAI)
  â”‚
  â”œâ”€ supervisor_tools (Step 4)
  â”‚  â””â”€ å¹¶è¡Œå¯åŠ¨ 2 ä¸ª Research Agents
  â”‚
  â”œâ”€ Agent A: Gemini ç ”ç©¶ (Step 5-12)
  â”‚  â”œâ”€ llm_call (Step 5)
  â”‚  â”œâ”€ tool_node: Search "Gemini Deep Research" (Step 6)
  â”‚  â”œâ”€ llm_call (Step 7)
  â”‚  â”œâ”€ tool_node: think_tool (Step 8)
  â”‚  â”œâ”€ llm_call (Step 9)
  â”‚  â”œâ”€ tool_node: Search "Gemini features" (Step 10)
  â”‚  â”œâ”€ llm_call (Step 11)
  â”‚  â””â”€ compress_research (Step 12)
  â”‚
  â”œâ”€ Agent B: OpenAI ç ”ç©¶ (Step 5-12, å¹¶è¡Œ)
  â”‚  â””â”€ (ç±»ä¼¼æµç¨‹)
  â”‚
  â””â”€ supervisor (Step 13)
     â””â”€ å†³ç­–: "æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè°ƒç”¨ ResearchComplete"

Step 21: Final Report
  â””â”€ final_report_generation
     â””â”€ ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š

æ€»æ­¥éª¤: çº¦ 21-25 æ­¥
```

---

## ğŸ’¡ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥

| ç»„ä»¶ | æ¨èæ¨¡å‹ | åŸå›  |
|------|----------|------|
| Scoping | GPT-4.1 | éœ€è¦æ¨ç†èƒ½åŠ› |
| Research LLM | Claude Sonnet 4 | æ·±åº¦åˆ†æèƒ½åŠ›å¼º |
| Compression | GPT-4.1 | é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½ |
| Report | GPT-4.1 | å†™ä½œèƒ½åŠ›å¼ºï¼Œè¾“å‡ºå¿« |

**æˆæœ¬ç¤ºä¾‹ï¼ˆå•æ¬¡ç ”ç©¶ï¼‰ï¼š**
```
Scoping: $0.05 (2 æ¬¡ LLM è°ƒç”¨)
Research (3 ä¸ª Agent, æ¯ä¸ª 5 æ¬¡è°ƒç”¨): $0.60
Compression (3 æ¬¡): $0.15
Report: $0.10
æ€»è®¡: ~$0.90
```

---

### 2. ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_search(query: str):
    """ç¼“å­˜æœç´¢ç»“æœ"""
    return tavily_search.invoke({"query": query})
```

---

### 3. å¹¶è¡Œä¼˜åŒ–

```python
# å¦‚æœ Supervisor å†³å®š 3 ä¸ª Agent
# ä¸²è¡Œ: 3 * 30s = 90s
# å¹¶è¡Œ: max(30s, 30s, 30s) = 30s
# åŠ é€Ÿæ¯”: 3x
```

---

## ğŸ“ éƒ¨ç½²è€ƒè™‘

### 1. LangGraph Studio æœ¬åœ°è°ƒè¯•

```bash
# ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
uvx --refresh --from "langgraph-cli[inmem]" --with-editable . --python 3.11 langgraph dev --allow-blocking
```

**langgraph.json é…ç½®ï¼š**

```json
{
  "graphs": {
    "research_agent_full": "./src/deep_research_from_scratch/research_agent_full.py:agent"
  },
  "dependencies": ["."],
  "env": ".env"
}
```

---

### 2. LangGraph Platform éƒ¨ç½²

```python
# ä½¿ç”¨æŒä¹…åŒ– Checkpointer
from langgraph.checkpoint.postgres import PostgresSaver

checkpointer = PostgresSaver(
    connection_string="postgresql://user:pass@host:port/db"
)

agent = deep_researcher_builder.compile(
    checkpointer=checkpointer
)
```

---

### 3. API éƒ¨ç½²

```python
from fastapi import FastAPI
from langserve import add_routes

app = FastAPI(title="Deep Research API")

# æ·»åŠ è·¯ç”±
add_routes(
    app,
    agent,
    path="/research",
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True
)
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æŸ¥çœ‹ä¸­é—´çŠ¶æ€

```python
# ä½¿ç”¨ Checkpointer
result = agent.invoke(input, config=thread)

# æŸ¥çœ‹å†å²
history = agent.get_state_history(thread)
for state in history:
    print(f"Step: {state.metadata}")
    print(f"Messages: {len(state.values['messages'])}")
```

---

### 2. å•æ­¥è°ƒè¯•

```python
# å•ç‹¬æµ‹è¯•æ¯ä¸ªç»„ä»¶
scoping_result = scope_research.invoke(input, config=thread)
print("Scoping result:", scoping_result)

supervisor_result = supervisor_agent.invoke({
    "supervisor_messages": [HumanMessage(scoping_result["research_brief"])]
})
print("Research result:", supervisor_result)
```

---

### 3. æ—¥å¿—è¾“å‡º

```python
import logging

logging.basicConfig(level=logging.INFO)

# åœ¨èŠ‚ç‚¹ä¸­æ·»åŠ æ—¥å¿—
def llm_call(state):
    logging.info(f"LLM call with {len(state['messages'])} messages")
    result = model.invoke(...)
    logging.info(f"LLM returned {len(result.tool_calls)} tool calls")
    return {"messages": [result]}
```

---

## ğŸ’¡ æœ€ä½³å®è·µæ€»ç»“

### 1. æ¨¡å—åŒ–è®¾è®¡

```python
# âœ… æ¯ä¸ªç»„ä»¶ç‹¬ç«‹
scoping = scope_builder.compile()
supervisor = supervisor_builder.compile()
full_system = full_builder.compile()

# å¯ä»¥å•ç‹¬æµ‹è¯•å’Œä¼˜åŒ–æ¯ä¸ªç»„ä»¶
```

### 2. é”™è¯¯å¤„ç†

```python
async def safe_invoke(agent, input, config):
    try:
        return await agent.ainvoke(input, config)
    except RecursionError:
        # é€’å½’é™åˆ¶é”™è¯¯
        config["configurable"]["recursion_limit"] = 100
        return await agent.ainvoke(input, config)
    except Exception as e:
        # å…¶ä»–é”™è¯¯
        return {"error": str(e)}
```

### 3. æ€§èƒ½ç›‘æ§

```python
import time

async def timed_invoke(agent, input, config):
    start = time.time()
    result = await agent.ainvoke(input, config)
    duration = time.time() - start

    print(f"Total time: {duration:.2f}s")
    print(f"Cost estimate: ${estimate_cost(result):.2f}")

    return result
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬èŠ‚ï¼Œä½ å·²ç»æŒæ¡äº†å®Œæ•´ Deep Research ç³»ç»Ÿçš„æ„å»ºã€‚

**ä¸‹ä¸€ç« ï¼š9.6 å°ç»“å’Œå¤ä¹ ** - å›é¡¾æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µï¼Œæ€»ç»“æœ€ä½³å®è·µï¼Œè®¨è®ºç”Ÿäº§éƒ¨ç½²çš„å…³é”®è€ƒè™‘ï¼
