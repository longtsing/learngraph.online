# 7.3 å®æˆ˜ï¼šå¤š API é›†æˆç³»ç»Ÿ

> **ğŸ¯ å°ç™½ç†è§£**ï¼šä¸ºä»€ä¹ˆéœ€è¦"å¤š API é›†æˆ"ï¼Ÿ
>
> ç°å®ä¸­ï¼Œä½ å¯èƒ½éœ€è¦åŒæ—¶ä½¿ç”¨å¤šä¸ª AI æœåŠ¡ï¼š
>
> | åœºæ™¯ | è§£å†³æ–¹æ¡ˆ |
> |------|---------|
> | OpenAI å¤ªè´µ | æœ‰äº›ä»»åŠ¡ç”¨ä¾¿å®œçš„æ¨¡å‹ |
> | OpenAI æŒ‚äº† | è‡ªåŠ¨åˆ‡æ¢åˆ° Anthropic |
> | éœ€è¦å¯¹æ¯” | åŒæ—¶é—®å¤šä¸ª AIï¼Œçœ‹è°ç­”å¾—å¥½ |
>
> **æœ¬èŠ‚çš„æ¶æ„**ï¼š
>
> ```
>                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>                 â”‚     MultiLLMManager         â”‚
>                 â”‚   (å¤š API ç®¡ç†å™¨)            â”‚
>                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
>                            â”‚
>        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>        â”‚                   â”‚                   â”‚
>        â–¼                   â–¼                   â–¼
> â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
> â”‚ OpenAI      â”‚    â”‚ Anthropic   â”‚    â”‚ Google      â”‚
> â”‚ Client      â”‚    â”‚ Client      â”‚    â”‚ Client      â”‚
> â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
> ```
>
> **æ ¸å¿ƒæ€æƒ³**ï¼šç»Ÿä¸€æ¥å£ + æ™ºèƒ½è·¯ç”± + è‡ªåŠ¨é™çº§

## ç»Ÿä¸€ API æ¥å£è®¾è®¡

> **ğŸ¯ å°ç™½ç†è§£**ï¼šä»€ä¹ˆæ˜¯"ç»Ÿä¸€æ¥å£"ï¼Ÿ
>
> ä¸åŒçš„ AI å…¬å¸ï¼ŒAPI æ ¼å¼éƒ½ä¸ä¸€æ ·ï¼š
>
> - **OpenAI**ï¼š`response["choices"][0]["message"]["content"]`
> - **Anthropic**ï¼š`response["content"][0]["text"]`
>
> æ¯æ¬¡æ¢ API éƒ½è¦æ”¹ä»£ç ï¼Ÿå¤ªéº»çƒ¦ï¼
>
> **ç»Ÿä¸€æ¥å£**çš„åšæ³•ï¼š
>
> ```python
> # ä¸ç®¡ç”¨å“ªä¸ª APIï¼Œè°ƒç”¨æ–¹å¼éƒ½ä¸€æ ·
> response = client.chat_completion(messages)
> print(response.content)  # ç»Ÿä¸€çš„æ ¼å¼ï¼
> ```
>
> è¿™å°±æ˜¯"é¢å‘æ¥å£ç¼–ç¨‹"â€”â€”æŠŠä¸åŒçš„å®ç°è—åœ¨ç»Ÿä¸€çš„æ¥å£åé¢ã€‚

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

class APIProvider(Enum):
    """API æä¾›å•†"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    DEEPSEEK = "deepseek"

@dataclass
class Message:
    """ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼"""
    role: str  # system, user, assistant
    content: str

@dataclass
class CompletionResponse:
    """ç»Ÿä¸€å“åº”æ ¼å¼"""
    content: str
    model: str
    provider: APIProvider
    usage: Dict[str, int]
    raw_response: Dict[str, Any]

class BaseLLMClient(ABC):
    """LLM å®¢æˆ·ç«¯åŸºç±»"""
    
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
    
    @abstractmethod
    def chat_completion(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """èŠå¤©è¡¥å…¨æ¥å£"""
        pass
    
    @abstractmethod
    async def achat_completion(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """å¼‚æ­¥èŠå¤©è¡¥å…¨æ¥å£"""
        pass
```

## OpenAI å®¢æˆ·ç«¯å®ç°

```python
import requests
import httpx
from typing import List, Dict, Any

class OpenAIClient(BaseLLMClient):
    """OpenAI API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: str = "gpt-4"):
        super().__init__(api_key, model)
        self.base_url = "https://api.openai.com/v1"
    
    def _headers(self) -> Dict[str, str]:
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def _convert_messages(self, messages: List[Message]) -> List[Dict[str, str]]:
        """è½¬æ¢ä¸º OpenAI æ ¼å¼"""
        return [
            {"role": msg.role, "content": msg.content}
            for msg in messages
        ]
    
    def chat_completion(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """åŒæ­¥è°ƒç”¨"""
        url = f"{self.base_url}/chat/completions"
        
        payload = {
            "model": self.model,
            "messages": self._convert_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        response = requests.post(
            url,
            headers=self._headers(),
            json=payload,
            timeout=30
        )
        
        response.raise_for_status()
        data = response.json()
        
        return CompletionResponse(
            content=data["choices"][0]["message"]["content"],
            model=data["model"],
            provider=APIProvider.OPENAI,
            usage=data["usage"],
            raw_response=data
        )
    
    async def achat_completion(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """å¼‚æ­¥è°ƒç”¨"""
        url = f"{self.base_url}/chat/completions"
        
        payload = {
            "model": self.model,
            "messages": self._convert_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                url,
                headers=self._headers(),
                json=payload,
                timeout=30
            )
            
            response.raise_for_status()
            data = response.json()
            
            return CompletionResponse(
                content=data["choices"][0]["message"]["content"],
                model=data["model"],
                provider=APIProvider.OPENAI,
                usage=data["usage"],
                raw_response=data
            )
```

## Anthropic å®¢æˆ·ç«¯å®ç°

```python
class AnthropicClient(BaseLLMClient):
    """Anthropic API å®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: str = "claude-3-5-sonnet-20241022"):
        super().__init__(api_key, model)
        self.base_url = "https://api.anthropic.com/v1"
    
    def _headers(self) -> Dict[str, str]:
        return {
            "x-api-key": self.api_key,
            "anthropic-version": "2023-06-01",
            "Content-Type": "application/json"
        }
    
    def _convert_messages(self, messages: List[Message]) -> tuple:
        """è½¬æ¢ä¸º Anthropic æ ¼å¼ï¼ˆåˆ†ç¦» system å’Œ messagesï¼‰"""
        system = None
        converted_messages = []
        
        for msg in messages:
            if msg.role == "system":
                system = msg.content
            else:
                converted_messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
        
        return system, converted_messages
    
    def chat_completion(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """åŒæ­¥è°ƒç”¨"""
        url = f"{self.base_url}/messages"
        
        system, converted_messages = self._convert_messages(messages)
        
        payload = {
            "model": self.model,
            "messages": converted_messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        if system:
            payload["system"] = system
        
        response = requests.post(
            url,
            headers=self._headers(),
            json=payload,
            timeout=30
        )
        
        response.raise_for_status()
        data = response.json()
        
        return CompletionResponse(
            content=data["content"][0]["text"],
            model=data["model"],
            provider=APIProvider.ANTHROPIC,
            usage={
                "prompt_tokens": data["usage"]["input_tokens"],
                "completion_tokens": data["usage"]["output_tokens"],
                "total_tokens": data["usage"]["input_tokens"] + data["usage"]["output_tokens"]
            },
            raw_response=data
        )
    
    async def achat_completion(
        self,
        messages: List[Message],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """å¼‚æ­¥è°ƒç”¨"""
        url = f"{self.base_url}/messages"
        
        system, converted_messages = self._convert_messages(messages)
        
        payload = {
            "model": self.model,
            "messages": converted_messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        if system:
            payload["system"] = system
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                url,
                headers=self._headers(),
                json=payload,
                timeout=30
            )
            
            response.raise_for_status()
            data = response.json()
            
            return CompletionResponse(
                content=data["content"][0]["text"],
                model=data["model"],
                provider=APIProvider.ANTHROPIC,
                usage={
                    "prompt_tokens": data["usage"]["input_tokens"],
                    "completion_tokens": data["usage"]["output_tokens"],
                    "total_tokens": data["usage"]["input_tokens"] + data["usage"]["output_tokens"]
                },
                raw_response=data
            )
```

## å¤š API ç®¡ç†å™¨

```python
import os
from typing import Optional, Dict, List
import asyncio

class MultiLLMManager:
    """å¤š LLM ç®¡ç†å™¨"""
    
    def __init__(self):
        self.clients: Dict[APIProvider, BaseLLMClient] = {}
        self._initialize_clients()
    
    def _initialize_clients(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„å®¢æˆ·ç«¯"""
        # OpenAI
        if openai_key := os.getenv("OPENAI_API_KEY"):
            self.clients[APIProvider.OPENAI] = OpenAIClient(
                api_key=openai_key,
                model=os.getenv("OPENAI_MODEL", "gpt-4")
            )
        
        # Anthropic
        if anthropic_key := os.getenv("ANTHROPIC_API_KEY"):
            self.clients[APIProvider.ANTHROPIC] = AnthropicClient(
                api_key=anthropic_key,
                model=os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022")
            )
    
    def get_client(self, provider: APIProvider) -> Optional[BaseLLMClient]:
        """è·å–æŒ‡å®šçš„å®¢æˆ·ç«¯"""
        return self.clients.get(provider)
    
    def list_providers(self) -> List[APIProvider]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æä¾›å•†"""
        return list(self.clients.keys())
    
    def chat_completion(
        self,
        messages: List[Message],
        provider: APIProvider = APIProvider.OPENAI,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """ä½¿ç”¨æŒ‡å®šæä¾›å•†å®ŒæˆèŠå¤©"""
        client = self.get_client(provider)
        
        if not client:
            raise ValueError(f"æä¾›å•† {provider.value} æœªé…ç½®")
        
        return client.chat_completion(messages, temperature, max_tokens)
    
    async def achat_completion(
        self,
        messages: List[Message],
        provider: APIProvider = APIProvider.OPENAI,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """å¼‚æ­¥èŠå¤©è¡¥å…¨"""
        client = self.get_client(provider)
        
        if not client:
            raise ValueError(f"æä¾›å•† {provider.value} æœªé…ç½®")
        
        return await client.achat_completion(messages, temperature, max_tokens)
    
    async def parallel_completion(
        self,
        messages: List[Message],
        providers: Optional[List[APIProvider]] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Dict[APIProvider, CompletionResponse]:
        """å¹¶å‘è°ƒç”¨å¤šä¸ªæä¾›å•†"""
        if providers is None:
            providers = self.list_providers()
        
        tasks = []
        valid_providers = []
        
        for provider in providers:
            if provider in self.clients:
                tasks.append(
                    self.achat_completion(messages, provider, temperature, max_tokens)
                )
                valid_providers.append(provider)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return {
            provider: result
            for provider, result in zip(valid_providers, results)
            if not isinstance(result, Exception)
        }

# ä½¿ç”¨ç¤ºä¾‹
manager = MultiLLMManager()

# åˆ—å‡ºå¯ç”¨æä¾›å•†
print(f"å¯ç”¨æä¾›å•†: {[p.value for p in manager.list_providers()]}")

# ä½¿ç”¨ OpenAI
messages = [
    Message(role="system", content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"),
    Message(role="user", content="è§£é‡Šä»€ä¹ˆæ˜¯ API")
]

try:
    response = manager.chat_completion(messages, provider=APIProvider.OPENAI)
    print(f"\n{response.provider.value} å“åº”:")
    print(response.content)
    print(f"Token ä½¿ç”¨: {response.usage}")

except Exception as e:
    print(f"é”™è¯¯: {e}")

# å¼‚æ­¥å¹¶å‘è°ƒç”¨
async def demo_parallel():
    responses = await manager.parallel_completion(messages)
    
    for provider, response in responses.items():
        print(f"\n{provider.value} å“åº”:")
        print(response.content[:200] + "...")
        print(f"Token: {response.usage}")

# asyncio.run(demo_parallel())
```

## æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ

> **ğŸ¯ å°ç™½ç†è§£**ï¼šä»€ä¹ˆæ˜¯"æ™ºèƒ½è·¯ç”±"ï¼Ÿ
>
> æƒ³è±¡ä½ æ˜¯å¤–å–å¹³å°ï¼Œè¦æŠŠè®¢å•åˆ†é…ç»™éª‘æ‰‹ï¼š
>
> - **ç¬¨æ–¹æ³•**ï¼šæ°¸è¿œåªç”¨ç¬¬ä¸€ä¸ªéª‘æ‰‹ï¼ˆå¯èƒ½ä»–å·²ç»é€ä¸è¿‡æ¥äº†ï¼‰
> - **æ™ºèƒ½æ–¹æ³•**ï¼šçœ‹è°ç¦»å¾—è¿‘ã€è°è¯„åˆ†é«˜ã€è°è¿˜æœ‰ç©º
>
> **æ™ºèƒ½è·¯ç”±åšäº†ä»€ä¹ˆï¼Ÿ**
>
> ```
> 1. ç»Ÿè®¡æ¯ä¸ª API çš„è¡¨ç°ï¼ˆæˆåŠŸç‡ã€é€Ÿåº¦ã€æˆæœ¬ï¼‰
> 2. è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ API
> 3. å¦‚æœå¤±è´¥äº†ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨ APIï¼ˆé™çº§ï¼‰
> ```
>
> **å…³é”®æŒ‡æ ‡**ï¼š
>
> | æŒ‡æ ‡ | å«ä¹‰ |
> |------|------|
> | é”™è¯¯ç‡ | è°ƒç”¨å¤±è´¥çš„æ¯”ä¾‹ |
> | å¹³å‡å»¶è¿Ÿ | å“åº”é€Ÿåº¦ |
> | æ€» Token | æ¶ˆè€—äº†å¤šå°‘èµ„æº |
>
> é€šè¿‡è¿™äº›æŒ‡æ ‡ï¼Œè‡ªåŠ¨é€‰æ‹©"åˆå¿«åˆç¨³"çš„ APIï¼

```python
from typing import Optional, List
import time

class SmartRouter:
    """æ™ºèƒ½ API è·¯ç”±å™¨"""
    
    def __init__(self, manager: MultiLLMManager):
        self.manager = manager
        self.usage_stats: Dict[APIProvider, Dict[str, Any]] = {
            provider: {
                "calls": 0,
                "errors": 0,
                "total_tokens": 0,
                "avg_latency": 0.0
            }
            for provider in APIProvider
        }
    
    def _update_stats(
        self,
        provider: APIProvider,
        success: bool,
        tokens: int = 0,
        latency: float = 0.0
    ):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.usage_stats[provider]
        stats["calls"] += 1
        
        if not success:
            stats["errors"] += 1
        else:
            stats["total_tokens"] += tokens
            # è®¡ç®—ç§»åŠ¨å¹³å‡å»¶è¿Ÿ
            if stats["avg_latency"] == 0:
                stats["avg_latency"] = latency
            else:
                stats["avg_latency"] = (stats["avg_latency"] * 0.9 + latency * 0.1)
    
    def select_provider(
        self,
        preferred: Optional[APIProvider] = None,
        fallback: bool = True
    ) -> APIProvider:
        """æ™ºèƒ½é€‰æ‹©æä¾›å•†"""
        available_providers = self.manager.list_providers()
        
        if not available_providers:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ API æä¾›å•†")
        
        # å¦‚æœæŒ‡å®šäº†é¦–é€‰æä¾›å•†ä¸”å¯ç”¨
        if preferred and preferred in available_providers:
            return preferred
        
        # å¦åˆ™é€‰æ‹©é”™è¯¯ç‡æœ€ä½çš„æä¾›å•†
        best_provider = None
        best_score = float('inf')
        
        for provider in available_providers:
            stats = self.usage_stats[provider]
            
            if stats["calls"] == 0:
                # æ–°æä¾›å•†ï¼Œä¼˜å…ˆå°è¯•
                return provider
            
            # è®¡ç®—åˆ†æ•°ï¼ˆé”™è¯¯ç‡ + å»¶è¿Ÿï¼‰
            error_rate = stats["errors"] / stats["calls"]
            score = error_rate * 100 + stats["avg_latency"]
            
            if score < best_score:
                best_score = score
                best_provider = provider
        
        return best_provider or available_providers[0]
    
    async def chat_completion_with_fallback(
        self,
        messages: List[Message],
        preferred: Optional[APIProvider] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> CompletionResponse:
        """å¸¦é™çº§çš„èŠå¤©è¡¥å…¨"""
        providers = self.manager.list_providers()
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        if preferred and preferred in providers:
            providers.remove(preferred)
            providers.insert(0, preferred)
        
        last_error = None
        
        for provider in providers:
            try:
                start = time.time()
                
                response = await self.manager.achat_completion(
                    messages,
                    provider=provider,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                latency = time.time() - start
                
                # æ›´æ–°ç»Ÿè®¡
                self._update_stats(
                    provider,
                    success=True,
                    tokens=response.usage.get("total_tokens", 0),
                    latency=latency
                )
                
                return response
            
            except Exception as e:
                print(f"{provider.value} å¤±è´¥: {e}ï¼Œå°è¯•é™çº§...")
                self._update_stats(provider, success=False)
                last_error = e
                continue
        
        # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥
        raise Exception(f"æ‰€æœ‰ API æä¾›å•†éƒ½å¤±è´¥äº†ã€‚æœ€åé”™è¯¯: {last_error}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æŠ¥å‘Š"""
        return {
            provider.value: stats
            for provider, stats in self.usage_stats.items()
            if stats["calls"] > 0
        }

# ä½¿ç”¨ç¤ºä¾‹
async def demo_smart_router():
    manager = MultiLLMManager()
    router = SmartRouter(manager)
    
    messages = [
        Message(role="user", content="ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹è‡ªå·±")
    ]
    
    # ä½¿ç”¨æ™ºèƒ½è·¯ç”±
    for i in range(5):
        try:
            response = await router.chat_completion_with_fallback(
                messages,
                preferred=APIProvider.OPENAI
            )
            
            print(f"\nè¯·æ±‚ {i+1}:")
            print(f"æä¾›å•†: {response.provider.value}")
            print(f"å“åº”: {response.content[:100]}...")
        
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
    
    # æ‰“å°ç»Ÿè®¡
    print("\n=== ç»Ÿè®¡æŠ¥å‘Š ===")
    for provider, stats in router.get_stats().items():
        print(f"\n{provider}:")
        print(f"  è°ƒç”¨æ¬¡æ•°: {stats['calls']}")
        print(f"  é”™è¯¯æ¬¡æ•°: {stats['errors']}")
        print(f"  å¹³å‡å»¶è¿Ÿ: {stats['avg_latency']:.2f}ç§’")
        print(f"  æ€» Token: {stats['total_tokens']}")

# asyncio.run(demo_smart_router())
```

## æˆæœ¬ç®¡ç†

> **ğŸ¯ å°ç™½ç†è§£**ï¼šä¸ºä»€ä¹ˆè¦ç®¡ç† API æˆæœ¬ï¼Ÿ
>
> AI API æ˜¯æŒ‰é‡ä»˜è´¹çš„ï¼Œç”¨å¤šå°‘ä»˜å¤šå°‘ï¼š
>
> | æ¨¡å‹ | è¾“å…¥ä»·æ ¼ (æ¯1000 tokens) | è¾“å‡ºä»·æ ¼ |
> |------|-------------------------|---------|
> | GPT-4 | $0.03 | $0.06 |
> | GPT-3.5 | $0.001 | $0.002 |
> | Claude 3.5 | $0.003 | $0.015 |
>
> **ä¸ç®¡ç†æˆæœ¬ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**
>
> ```
> ä¸€ä¸ª bug å¯¼è‡´æ— é™å¾ªç¯è°ƒç”¨ API
>            â†“
> ä¸€æ™šä¸ŠèŠ±æ‰å‡ ç™¾ç¾å…ƒ
>            â†“
> æ”¶åˆ°ä¿¡ç”¨å¡è´¦å•æ—¶å´©æºƒ
> ```
>
> **æˆæœ¬è¿½è¸ªå™¨èƒ½åšä»€ä¹ˆï¼Ÿ**
>
> 1. **è®°å½•æ¯æ¬¡è°ƒç”¨çš„èŠ±è´¹**
> 2. **ç»Ÿè®¡æ¯æ—¥/æ¯æœˆæ€»æˆæœ¬**
> 3. **è®¾ç½®é¢„ç®—ä¸Šé™ï¼Œè¶…äº†å°±æŠ¥è­¦**

```python
from dataclasses import dataclass
from typing import Dict
from datetime import datetime, timedelta

@dataclass
class PricingConfig:
    """å®šä»·é…ç½®"""
    input_price_per_1k: float  # è¾“å…¥ token ä»·æ ¼ï¼ˆæ¯ 1000 tokensï¼‰
    output_price_per_1k: float  # è¾“å‡º token ä»·æ ¼

# å„æä¾›å•†å®šä»·ï¼ˆç¤ºä¾‹ï¼‰
PRICING = {
    APIProvider.OPENAI: {
        "gpt-4": PricingConfig(input_price_per_1k=0.03, output_price_per_1k=0.06),
        "gpt-3.5-turbo": PricingConfig(input_price_per_1k=0.001, output_price_per_1k=0.002),
    },
    APIProvider.ANTHROPIC: {
        "claude-3-5-sonnet-20241022": PricingConfig(input_price_per_1k=0.003, output_price_per_1k=0.015),
    }
}

class CostTracker:
    """æˆæœ¬è¿½è¸ªå™¨"""
    
    def __init__(self):
        self.daily_costs: Dict[str, float] = {}
        self.monthly_limit: float = 100.0  # æ¯æœˆé¢„ç®—ï¼ˆç¾å…ƒï¼‰
    
    def calculate_cost(
        self,
        provider: APIProvider,
        model: str,
        input_tokens: int,
        output_tokens: int
    ) -> float:
        """è®¡ç®—å•æ¬¡è°ƒç”¨æˆæœ¬"""
        pricing = PRICING.get(provider, {}).get(model)
        
        if not pricing:
            return 0.0
        
        input_cost = (input_tokens / 1000) * pricing.input_price_per_1k
        output_cost = (output_tokens / 1000) * pricing.output_price_per_1k
        
        return input_cost + output_cost
    
    def track_usage(
        self,
        provider: APIProvider,
        model: str,
        usage: Dict[str, int]
    ) -> float:
        """è¿½è¸ªä½¿ç”¨æƒ…å†µ"""
        cost = self.calculate_cost(
            provider,
            model,
            usage.get("prompt_tokens", 0),
            usage.get("completion_tokens", 0)
        )
        
        today = datetime.now().strftime("%Y-%m-%d")
        self.daily_costs[today] = self.daily_costs.get(today, 0.0) + cost
        
        return cost
    
    def get_monthly_cost(self) -> float:
        """è·å–æœ¬æœˆæ€»æˆæœ¬"""
        now = datetime.now()
        month_start = now.replace(day=1).strftime("%Y-%m-%d")
        
        return sum(
            cost for date, cost in self.daily_costs.items()
            if date >= month_start
        )
    
    def check_budget(self) -> Dict[str, Any]:
        """æ£€æŸ¥é¢„ç®—çŠ¶æ€"""
        monthly_cost = self.get_monthly_cost()
        remaining = self.monthly_limit - monthly_cost
        usage_percent = (monthly_cost / self.monthly_limit) * 100
        
        return {
            "monthly_cost": monthly_cost,
            "monthly_limit": self.monthly_limit,
            "remaining": remaining,
            "usage_percent": usage_percent,
            "alert": usage_percent > 80
        }

# ä½¿ç”¨ç¤ºä¾‹
tracker = CostTracker()

# è¿½è¸ªä¸€æ¬¡è°ƒç”¨
usage = {"prompt_tokens": 100, "completion_tokens": 500}
cost = tracker.track_usage(APIProvider.OPENAI, "gpt-4", usage)
print(f"æœ¬æ¬¡è°ƒç”¨æˆæœ¬: ${cost:.4f}")

# æ£€æŸ¥é¢„ç®—
budget_status = tracker.check_budget()
print(f"\næœ¬æœˆæˆæœ¬: ${budget_status['monthly_cost']:.2f}")
print(f"é¢„ç®—å‰©ä½™: ${budget_status['remaining']:.2f}")
print(f"ä½¿ç”¨ç‡: {budget_status['usage_percent']:.1f}%")

if budget_status['alert']:
    print("âš ï¸ è­¦å‘Š: é¢„ç®—ä½¿ç”¨å·²è¶…è¿‡ 80%!")
```

## å…³é”®è¦ç‚¹

1. **ç»Ÿä¸€æ¥å£**ï¼šä½¿ç”¨æŠ½è±¡åŸºç±»ç»Ÿä¸€ä¸åŒæä¾›å•†çš„ API
2. **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®æ€§èƒ½å’Œå¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä½³æä¾›å•†
3. **é™çº§ç­–ç•¥**ï¼šä¸»æä¾›å•†å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æä¾›å•†
4. **æˆæœ¬ç®¡ç†**ï¼šè¿½è¸ªå’Œæ§åˆ¶ API ä½¿ç”¨æˆæœ¬
5. **ç»Ÿè®¡ç›‘æ§**ï¼šè·Ÿè¸ªè°ƒç”¨æˆåŠŸç‡ã€å»¶è¿Ÿç­‰å…³é”®æŒ‡æ ‡

---

**ä¸‹ä¸€èŠ‚ï¼š[7.4 å°ç»“å’Œå¤ä¹ ](7.4-å°ç»“å’Œå¤ä¹ .md)**
