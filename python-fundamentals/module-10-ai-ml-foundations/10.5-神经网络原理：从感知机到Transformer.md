# 10.5 ç¥ç»ç½‘ç»œåŸç†ï¼šä»æ„ŸçŸ¥æœºåˆ° Transformer

## ç¥ç»ç½‘ç»œç®€å²ï¼šä»ç”Ÿç‰©å¯å‘åˆ° AI é©å‘½

> **ğŸ’¡ å“²å­¦é—®é¢˜**ï¼šä¸€ä¸ªç”±æ•°åäº¿ä¸ªç®€å•çš„"å¼€å…³"ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆçš„ç³»ç»Ÿï¼Œå¦‚ä½•èƒ½å¤Ÿç†è§£è¯­è¨€ã€è¯†åˆ«å›¾åƒã€ç”šè‡³åˆ›ä½œè‰ºæœ¯ï¼Ÿ

è¿™æ˜¯æ·±åº¦å­¦ä¹ æœ€ä»¤äººç€è¿·çš„åœ°æ–¹â€”â€”**ç®€å•å•å…ƒçš„å¤æ‚ç»„åˆ**äº§ç”Ÿäº†æ™ºèƒ½è¡Œä¸ºã€‚

**ç¥ç»ç½‘ç»œå‘å±•timeline**ï¼š
```
1943: McCulloch-Pitts ç¥ç»å…ƒæ¨¡å‹
1958: Rosenblatt æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰
1986: Rumelhart åå‘ä¼ æ’­ç®—æ³•
1998: LeCun CNN (LeNet-5)
2006: Hinton æ·±åº¦ä¿¡å¿µç½‘ç»œï¼ˆDeep Belief Networksï¼‰
2012: AlexNet å¼•çˆ†æ·±åº¦å­¦ä¹ 
2014: GAN (ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ)
2017: Transformer æ¶æ„ â† ç°ä»£ LLM çš„åŸºçŸ³
2018: BERTã€GPT
2020: GPT-3
2022: ChatGPT
```

æœ¬ç« æ˜¯æ•´ä¸ªè¯¾ç¨‹çš„**ç†è®ºé«˜å³°**ï¼Œæˆ‘ä»¬å°†ï¼š
1. ä»å•ä¸ªç¥ç»å…ƒå‡ºå‘ï¼Œç†è§£æ·±åº¦å­¦ä¹ çš„æ•°å­¦åŸºç¡€
2. æ¨å¯¼åå‘ä¼ æ’­ç®—æ³•â€”â€”æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒ
3. å­¦ä¹  CNNã€RNN/LSTM ç­‰ç»å…¸æ¶æ„
4. **æ·±å…¥ Transformer**â€”â€”ç†è§£ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„ç§˜å¯†

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šæ„ŸçŸ¥æœºä¸ç¥ç»å…ƒ

### 1. ç”Ÿç‰©ç¥ç»å…ƒ vs äººå·¥ç¥ç»å…ƒ

**ç”Ÿç‰©ç¥ç»å…ƒ**ï¼š
```
æ ‘çªï¼ˆæ¥æ”¶ä¿¡å·ï¼‰â†’ ç»†èƒä½“ï¼ˆå¤„ç†ï¼‰â†’ è½´çªï¼ˆä¼ è¾“ï¼‰â†’ çªè§¦ï¼ˆè¿æ¥ä¸‹ä¸€ä¸ªç¥ç»å…ƒï¼‰
```

**äººå·¥ç¥ç»å…ƒï¼ˆæ„ŸçŸ¥æœºï¼‰**ï¼š

```python
"""
æ•°å­¦æ¨¡å‹ï¼š
    z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b
    y = activation(z)

å…¶ä¸­ï¼š
- x: è¾“å…¥
- w: æƒé‡ï¼ˆçªè§¦å¼ºåº¦ï¼‰
- b: åç½®
- activation: æ¿€æ´»å‡½æ•°ï¼ˆå†³å®šç¥ç»å…ƒæ˜¯å¦"æ¿€å‘"ï¼‰
"""

import numpy as np

class Perceptron:
    """
    å•å±‚æ„ŸçŸ¥æœºï¼šäºŒåˆ†ç±»å™¨

    æ•°å­¦å…¬å¼ï¼š
        y = sign(wÂ·x + b)
        å…¶ä¸­ sign(z) = 1 if z >= 0 else -1
    """

    def __init__(self, input_dim: int, learning_rate: float = 0.01):
        self.w = np.zeros(input_dim)  # æƒé‡åˆå§‹åŒ–ä¸º 0
        self.b = 0.0  # åç½®
        self.lr = learning_rate

    def predict(self, x: np.ndarray) -> int:
        """é¢„æµ‹"""
        z = np.dot(self.w, x) + self.b
        return 1 if z >= 0 else -1

    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 100):
        """
        è®­ç»ƒæ„ŸçŸ¥æœº

        å­¦ä¹ è§„åˆ™ï¼š
            å¦‚æœé¢„æµ‹æ­£ç¡®ï¼šä¸æ›´æ–°
            å¦‚æœé¢„æµ‹é”™è¯¯ï¼šw = w + lr * y * x
                          b = b + lr * y
        """
        for epoch in range(epochs):
            errors = 0
            for xi, yi in zip(X, y):
                prediction = self.predict(xi)
                if prediction != yi:
                    # æ›´æ–°æƒé‡
                    self.w += self.lr * yi * xi
                    self.b += self.lr * yi
                    errors += 1

            if errors == 0:
                print(f"æ”¶æ•›äºç¬¬ {epoch + 1} è½®")
                break

# ç¤ºä¾‹ï¼šå­¦ä¹  AND é—¨
X_and = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y_and = np.array([-1, -1, -1, 1])  # åªæœ‰ (1,1) è¾“å‡º 1

perceptron = Perceptron(input_dim=2, learning_rate=0.1)
perceptron.train(X_and, y_and)

print("æƒé‡:", perceptron.w)
print("åç½®:", perceptron.b)

# æµ‹è¯•
for xi in X_and:
    print(f"è¾“å…¥: {xi}, é¢„æµ‹: {perceptron.predict(xi)}")
```

**æ„ŸçŸ¥æœºçš„å±€é™æ€§**ï¼š

```python
# XOR é—®é¢˜ï¼šæ„ŸçŸ¥æœºæ— æ³•è§£å†³çº¿æ€§ä¸å¯åˆ†é—®é¢˜
X_xor = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])
y_xor = np.array([-1, 1, 1, -1])  # XOR è¾“å‡º

# æ— è®ºå¦‚ä½•è®­ç»ƒï¼Œå•å±‚æ„ŸçŸ¥æœºéƒ½æ— æ³•å­¦ä¼š XOR
# åŸå› ï¼šXOR ä¸æ˜¯çº¿æ€§å¯åˆ†çš„
```

> **ğŸ’¡ å…³é”®æ´å¯Ÿ**ï¼šå•å±‚æ„ŸçŸ¥æœºåªèƒ½è§£å†³**çº¿æ€§å¯åˆ†**é—®é¢˜ã€‚è¦è§£å†³ XORï¼Œæˆ‘ä»¬éœ€è¦**å¤šå±‚ç¥ç»ç½‘ç»œ**ã€‚

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šå¤šå±‚ç¥ç»ç½‘ç»œä¸åå‘ä¼ æ’­

### 1. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰

é€šè¿‡å †å å¤šå±‚ç¥ç»å…ƒï¼Œæˆ‘ä»¬å¯ä»¥å­¦ä¹ éçº¿æ€§å‡½æ•°ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    """
    å¤šå±‚æ„ŸçŸ¥æœº

    æ¶æ„ï¼š
        è¾“å…¥å±‚ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚
    """

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # éšè—å±‚ï¼šçº¿æ€§å˜æ¢ + éçº¿æ€§æ¿€æ´»
        h = F.relu(self.fc1(x))
        # è¾“å‡ºå±‚
        y = self.fc2(h)
        return y

# è§£å†³ XOR é—®é¢˜
X_xor = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])
y_xor = torch.tensor([[0.], [1.], [1.], [0.]])

model = MLP(input_dim=2, hidden_dim=4, output_dim=1)
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# è®­ç»ƒ
for epoch in range(5000):
    # å‰å‘ä¼ æ’­
    predictions = model(X_xor)
    loss = criterion(predictions, y_xor)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 1000 == 0:
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# æµ‹è¯•
model.eval()
with torch.no_grad():
    predictions = model(X_xor)
    print("\nXOR é¢„æµ‹:")
    for x, y, pred in zip(X_xor, y_xor, predictions):
        print(f"è¾“å…¥: {x.numpy()}, çœŸå®: {y.item():.0f}, é¢„æµ‹: {pred.item():.4f}")
```

### 2. æ¿€æ´»å‡½æ•°ï¼šå¼•å…¥éçº¿æ€§

å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç¥ç»ç½‘ç»œç­‰ä»·äºå•å±‚ç½‘ç»œï¼ˆçº¿æ€§å˜æ¢çš„ç»„åˆä»æ˜¯çº¿æ€§çš„ï¼‰ã€‚

```python
import matplotlib.pyplot as plt

# å¸¸è§æ¿€æ´»å‡½æ•°
x = np.linspace(-5, 5, 100)

# 1. Sigmoid: Ïƒ(x) = 1 / (1 + e^(-x))
sigmoid = 1 / (1 + np.exp(-x))

# 2. Tanh: tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
tanh = np.tanh(x)

# 3. ReLU: max(0, x)
relu = np.maximum(0, x)

# 4. Leaky ReLU: max(0.01x, x)
leaky_relu = np.where(x > 0, x, 0.01 * x)

# 5. GELU (ç”¨äº Transformer)
gelu = 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

# å¯è§†åŒ–
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

activations = [
    (sigmoid, 'Sigmoid'),
    (tanh, 'Tanh'),
    (relu, 'ReLU'),
    (leaky_relu, 'Leaky ReLU'),
    (gelu, 'GELU'),
]

for i, (y, name) in enumerate(activations):
    ax = axes[i // 3, i % 3]
    ax.plot(x, y)
    ax.set_title(name)
    ax.grid(True)
    ax.axhline(0, color='black', linewidth=0.5)
    ax.axvline(0, color='black', linewidth=0.5)

plt.tight_layout()
plt.show()
```

**æ¿€æ´»å‡½æ•°å¯¹æ¯”**ï¼š

| æ¿€æ´»å‡½æ•° | å…¬å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | åº”ç”¨åœºæ™¯ |
|---------|------|------|------|----------|
| **Sigmoid** | Ïƒ(x) = 1/(1+eâ»Ë£) | è¾“å‡º (0,1)ï¼Œå¯è§£é‡Šä¸ºæ¦‚ç‡ | æ¢¯åº¦æ¶ˆå¤± | äºŒåˆ†ç±»è¾“å‡ºå±‚ |
| **Tanh** | tanh(x) | è¾“å‡º (-1,1)ï¼Œé›¶ä¸­å¿ƒåŒ– | æ¢¯åº¦æ¶ˆå¤± | ä¼ ç»Ÿ RNN |
| **ReLU** | max(0, x) | è®¡ç®—ç®€å•ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤± | æ­»äº¡ ReLU é—®é¢˜ | CNNã€MLP éšè—å±‚ |
| **Leaky ReLU** | max(Î±x, x) | è§£å†³æ­»äº¡ ReLU | è¶…å‚æ•° Î± | æ·±åº¦ç½‘ç»œ |
| **GELU** | å¤æ‚å…¬å¼ | å¹³æ»‘ï¼Œæ€§èƒ½å¥½ | è®¡ç®—å¼€é”€å¤§ | Transformer |

### 3. åå‘ä¼ æ’­ç®—æ³•ï¼šæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒ

> **ğŸ“ æ•°å­¦æ¨å¯¼**ï¼šåå‘ä¼ æ’­æœ¬è´¨æ˜¯**é“¾å¼æ³•åˆ™**çš„åº”ç”¨ã€‚

**ç®€å•ä¾‹å­**ï¼šä¸¤å±‚ç½‘ç»œ

```python
"""
ç½‘ç»œç»“æ„ï¼š
    x â†’ [w1] â†’ h â†’ [w2] â†’ y

å‰å‘ä¼ æ’­ï¼š
    h = Ïƒ(w1 * x)
    y = w2 * h

æŸå¤±å‡½æ•°ï¼š
    L = (y - y_true)Â²

åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰ï¼š
    âˆ‚L/âˆ‚w2 = âˆ‚L/âˆ‚y * âˆ‚y/âˆ‚w2 = 2(y - y_true) * h
    âˆ‚L/âˆ‚w1 = âˆ‚L/âˆ‚y * âˆ‚y/âˆ‚h * âˆ‚h/âˆ‚w1
            = 2(y - y_true) * w2 * Ïƒ'(w1*x) * x
"""

class TwoLayerNet:
    """ä»é›¶å®ç°ä¸¤å±‚ç¥ç»ç½‘ç»œ"""

    def __init__(self):
        # éšæœºåˆå§‹åŒ–æƒé‡
        self.w1 = np.random.randn()
        self.w2 = np.random.randn()

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        """Sigmoid å¯¼æ•°: Ïƒ'(x) = Ïƒ(x)(1 - Ïƒ(x))"""
        s = self.sigmoid(x)
        return s * (1 - s)

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        self.x = x
        self.z1 = self.w1 * x
        self.h = self.sigmoid(self.z1)
        self.y = self.w2 * self.h
        return self.y

    def backward(self, y_true, learning_rate=0.1):
        """åå‘ä¼ æ’­"""
        # è®¡ç®—æŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦
        loss_gradient = 2 * (self.y - y_true)

        # è¾“å‡ºå±‚æ¢¯åº¦
        grad_w2 = loss_gradient * self.h

        # éšè—å±‚æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
        grad_h = loss_gradient * self.w2
        grad_z1 = grad_h * self.sigmoid_derivative(self.z1)
        grad_w1 = grad_z1 * self.x

        # æ›´æ–°æƒé‡
        self.w2 -= learning_rate * grad_w2
        self.w1 -= learning_rate * grad_w1

        return (self.y - y_true) ** 2

# è®­ç»ƒ
net = TwoLayerNet()
x_train = np.array([0.5])
y_train = np.array([0.8])

for epoch in range(1000):
    y_pred = net.forward(x_train)
    loss = net.backward(y_train)

    if (epoch + 1) % 200 == 0:
        print(f"Epoch {epoch+1}: Loss = {loss:.6f}, w1 = {net.w1:.4f}, w2 = {net.w2:.4f}")
```

**é€šç”¨åå‘ä¼ æ’­å…¬å¼**ï¼š

å¯¹äºä»»æ„å±‚ lï¼š
```
Î´_l = âˆ‚L/âˆ‚z_l  (è¯¯å·®é¡¹)

1. è¾“å‡ºå±‚ï¼šÎ´_L = âˆ‚L/âˆ‚y âŠ™ Ïƒ'(z_L)
2. éšè—å±‚ï¼šÎ´_l = (W_{l+1}^T Î´_{l+1}) âŠ™ Ïƒ'(z_l)
3. æƒé‡æ¢¯åº¦ï¼šâˆ‚L/âˆ‚W_l = Î´_l * a_{l-1}^T
4. åç½®æ¢¯åº¦ï¼šâˆ‚L/âˆ‚b_l = Î´_l

å…¶ä¸­ âŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

### 1. ä¸ºä»€ä¹ˆéœ€è¦ CNNï¼Ÿ

**å…¨è¿æ¥å±‚çš„é—®é¢˜**ï¼š
- å›¾åƒ 28Ã—28Ã—3 = 2352 ç»´
- ç¬¬ä¸€å±‚ 1000 ä¸ªç¥ç»å…ƒ â†’ 2,352,000 ä¸ªå‚æ•°
- å‚æ•°å¤ªå¤šï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè®¡ç®—å¼€é”€å¤§

**CNN çš„æ ¸å¿ƒæ€æƒ³**ï¼š
1. **å±€éƒ¨è¿æ¥**ï¼šæ¯ä¸ªç¥ç»å…ƒåªçœ‹å›¾åƒçš„ä¸€å°å—åŒºåŸŸ
2. **æƒé‡å…±äº«**ï¼šåŒä¸€ä¸ªå·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šæ»‘åŠ¨
3. **ç©ºé—´å±‚æ¬¡ç»“æ„**ï¼šä½å±‚æ£€æµ‹è¾¹ç¼˜ï¼Œé«˜å±‚æ£€æµ‹å¤æ‚ç‰¹å¾

### 2. å·ç§¯æ“ä½œ

```python
import torch
import torch.nn as nn

# æ‰‹åŠ¨å®ç° 2D å·ç§¯
def conv2d_manual(image: np.ndarray, kernel: np.ndarray) -> np.ndarray:
    """
    æ‰‹åŠ¨å®ç° 2D å·ç§¯

    Args:
        image: (H, W) å›¾åƒ
        kernel: (k, k) å·ç§¯æ ¸

    Returns:
        å·ç§¯ç»“æœ
    """
    H, W = image.shape
    k = kernel.shape[0]
    output_h = H - k + 1
    output_w = W - k + 1

    output = np.zeros((output_h, output_w))

    for i in range(output_h):
        for j in range(output_w):
            # æå–æ„Ÿå—é‡
            patch = image[i:i+k, j:j+k]
            # é€å…ƒç´ ä¹˜æ³•å¹¶æ±‚å’Œ
            output[i, j] = np.sum(patch * kernel)

    return output

# ç¤ºä¾‹ï¼šè¾¹ç¼˜æ£€æµ‹å·ç§¯æ ¸
image = np.array([
    [0, 0, 0, 0, 0],
    [0, 1, 1, 1, 0],
    [0, 1, 1, 1, 0],
    [0, 1, 1, 1, 0],
    [0, 0, 0, 0, 0]
], dtype=float)

# å‚ç›´è¾¹ç¼˜æ£€æµ‹
vertical_kernel = np.array([
    [-1, 0, 1],
    [-1, 0, 1],
    [-1, 0, 1]
])

# æ°´å¹³è¾¹ç¼˜æ£€æµ‹
horizontal_kernel = np.array([
    [-1, -1, -1],
    [ 0,  0,  0],
    [ 1,  1,  1]
])

vertical_edges = conv2d_manual(image, vertical_kernel)
horizontal_edges = conv2d_manual(image, horizontal_kernel)

print("åŸå§‹å›¾åƒ:")
print(image)
print("\nå‚ç›´è¾¹ç¼˜:")
print(vertical_edges)
print("\næ°´å¹³è¾¹ç¼˜:")
print(horizontal_edges)
```

### 3. CNN æ¶æ„ç»„ä»¶

```python
class ConvNet(nn.Module):
    """
    ç»å…¸ CNN æ¶æ„

    ç»“æ„ï¼š
        Conv â†’ ReLU â†’ Pool â†’ Conv â†’ ReLU â†’ Pool â†’ FC â†’ ReLU â†’ FC
    """

    def __init__(self, num_classes: int = 10):
        super(ConvNet, self).__init__()

        # å·ç§¯å±‚ 1
        self.conv1 = nn.Conv2d(
            in_channels=1,      # è¾“å…¥é€šé“æ•°ï¼ˆç°åº¦å›¾ï¼‰
            out_channels=32,    # è¾“å‡ºé€šé“æ•°ï¼ˆç‰¹å¾å›¾æ•°é‡ï¼‰
            kernel_size=3,      # å·ç§¯æ ¸å¤§å°
            stride=1,           # æ­¥é•¿
            padding=1           # å¡«å……
        )

        # å·ç§¯å±‚ 2
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)

        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        # è¾“å…¥: (batch, 1, 28, 28)

        # å·ç§¯å±‚ 1 + ReLU + æ± åŒ–
        x = self.conv1(x)         # (batch, 32, 28, 28)
        x = F.relu(x)
        x = self.pool(x)          # (batch, 32, 14, 14)

        # å·ç§¯å±‚ 2 + ReLU + æ± åŒ–
        x = self.conv2(x)         # (batch, 64, 14, 14)
        x = F.relu(x)
        x = self.pool(x)          # (batch, 64, 7, 7)

        # å±•å¹³
        x = x.view(-1, 64 * 7 * 7)  # (batch, 3136)

        # å…¨è¿æ¥å±‚
        x = self.fc1(x)           # (batch, 128)
        x = F.relu(x)
        x = self.fc2(x)           # (batch, 10)

        return x

model = ConvNet()
print(model)

# è®¡ç®—å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"\næ€»å‚æ•°æ•°: {total_params:,}")
```

**å…³é”®æ¦‚å¿µ**ï¼š

1. **å¡«å……ï¼ˆPaddingï¼‰**ï¼šåœ¨å›¾åƒè¾¹ç¼˜å¡«å…… 0ï¼Œä¿æŒè¾“å‡ºå°ºå¯¸
   ```
   è¾“å‡ºå°ºå¯¸ = (è¾“å…¥å°ºå¯¸ - å·ç§¯æ ¸å°ºå¯¸ + 2*padding) / stride + 1
   ```

2. **æ± åŒ–ï¼ˆPoolingï¼‰**ï¼šé™ä½ç©ºé—´ç»´åº¦
   - **æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰**ï¼šå–çª—å£å†…æœ€å¤§å€¼
   - **å¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰**ï¼šå–çª—å£å†…å¹³å‡å€¼

3. **æ„Ÿå—é‡ï¼ˆReceptive Fieldï¼‰**ï¼šç¥ç»å…ƒ"çœ‹åˆ°"çš„è¾“å…¥åŒºåŸŸå¤§å°
   - å †å å¤šå±‚å·ç§¯ â†’ å¢å¤§æ„Ÿå—é‡
   - é«˜å±‚ç¥ç»å…ƒå¯ä»¥"çœ‹åˆ°"æ›´å¤§çš„å›¾åƒåŒºåŸŸ

---

## ç¬¬å››éƒ¨åˆ†ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ä¸ LSTM

### 1. RNNï¼šå¤„ç†åºåˆ—æ•°æ®

**ä¸ºä»€ä¹ˆéœ€è¦ RNNï¼Ÿ**
- ä¼ ç»Ÿç¥ç»ç½‘ç»œæ— æ³•å¤„ç†å˜é•¿åºåˆ—
- æ— æ³•è®°å¿†å†å²ä¿¡æ¯

**RNN çš„æ ¸å¿ƒæ€æƒ³**ï¼šç»´æŠ¤ä¸€ä¸ª**éšè—çŠ¶æ€**ï¼Œåœ¨æ—¶é—´æ­¥ä¹‹é—´ä¼ é€’ä¿¡æ¯ã€‚

```python
class SimpleRNN(nn.Module):
    """
    ç®€å• RNN

    å…¬å¼ï¼š
        h_t = tanh(W_hh * h_{t-1} + W_xh * x_t + b_h)
        y_t = W_hy * h_t + b_y
    """

    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        super(SimpleRNN, self).__init__()

        self.hidden_size = hidden_size

        # æƒé‡çŸ©é˜µ
        self.W_xh = nn.Linear(input_size, hidden_size)   # è¾“å…¥åˆ°éšè—
        self.W_hh = nn.Linear(hidden_size, hidden_size)  # éšè—åˆ°éšè—
        self.W_hy = nn.Linear(hidden_size, output_size)  # éšè—åˆ°è¾“å‡º

    def forward(self, x, h_prev):
        """
        Args:
            x: å½“å‰è¾“å…¥ (batch, input_size)
            h_prev: å‰ä¸€æ—¶åˆ»éšè—çŠ¶æ€ (batch, hidden_size)

        Returns:
            y: è¾“å‡º (batch, output_size)
            h: æ–°éšè—çŠ¶æ€ (batch, hidden_size)
        """
        # æ›´æ–°éšè—çŠ¶æ€
        h = torch.tanh(self.W_xh(x) + self.W_hh(h_prev))

        # è®¡ç®—è¾“å‡º
        y = self.W_hy(h)

        return y, h

# å¤„ç†åºåˆ—
rnn = SimpleRNN(input_size=10, hidden_size=20, output_size=5)

# åˆå§‹åŒ–éšè—çŠ¶æ€
batch_size = 3
h = torch.zeros(batch_size, 20)

# é€æ—¶é—´æ­¥å¤„ç†
sequence_length = 5
for t in range(sequence_length):
    x_t = torch.randn(batch_size, 10)  # å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥
    y_t, h = rnn(x_t, h)  # æ›´æ–°éšè—çŠ¶æ€
    print(f"æ—¶é—´æ­¥ {t+1}: è¾“å‡ºå½¢çŠ¶ {y_t.shape}, éšè—çŠ¶æ€å½¢çŠ¶ {h.shape}")
```

### 2. LSTMï¼šé•¿çŸ­æœŸè®°å¿†ç½‘ç»œ

**RNN çš„é—®é¢˜**ï¼š**æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸**ï¼Œæ— æ³•å­¦ä¹ é•¿æœŸä¾èµ–ã€‚

**LSTM çš„è§£å†³æ–¹æ¡ˆ**ï¼šå¼•å…¥**é—¨æ§æœºåˆ¶**ï¼Œé€‰æ‹©æ€§åœ°è®°å¿†å’Œé—å¿˜ä¿¡æ¯ã€‚

```python
"""
LSTM æ¶æ„ï¼š

1. é—å¿˜é—¨ï¼ˆForget Gateï¼‰ï¼šå†³å®šä¸¢å¼ƒå“ªäº›ä¿¡æ¯
   f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)

2. è¾“å…¥é—¨ï¼ˆInput Gateï¼‰ï¼šå†³å®šæ›´æ–°å“ªäº›ä¿¡æ¯
   i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
   CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)

3. æ›´æ–°ç»†èƒçŠ¶æ€ï¼ˆCell Stateï¼‰
   C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t

4. è¾“å‡ºé—¨ï¼ˆOutput Gateï¼‰ï¼šå†³å®šè¾“å‡ºä»€ä¹ˆ
   o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
   h_t = o_t âŠ™ tanh(C_t)
"""

class LSTMCell(nn.Module):
    """ä»é›¶å®ç° LSTM Cell"""

    def __init__(self, input_size: int, hidden_size: int):
        super(LSTMCell, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size

        # å››ä¸ªé—¨çš„æƒé‡ï¼ˆåˆå¹¶è®¡ç®—æé«˜æ•ˆç‡ï¼‰
        self.W = nn.Linear(input_size + hidden_size, 4 * hidden_size)

    def forward(self, x, h_prev, c_prev):
        """
        Args:
            x: è¾“å…¥ (batch, input_size)
            h_prev: å‰ä¸€éšè—çŠ¶æ€ (batch, hidden_size)
            c_prev: å‰ä¸€ç»†èƒçŠ¶æ€ (batch, hidden_size)

        Returns:
            h: æ–°éšè—çŠ¶æ€
            c: æ–°ç»†èƒçŠ¶æ€
        """
        # æ‹¼æ¥è¾“å…¥å’Œéšè—çŠ¶æ€
        combined = torch.cat([x, h_prev], dim=1)

        # è®¡ç®—å››ä¸ªé—¨
        gates = self.W(combined)

        # åˆ†å‰²ä¸ºå››ä¸ªé—¨
        i, f, g, o = gates.chunk(4, dim=1)

        # åº”ç”¨æ¿€æ´»å‡½æ•°
        i = torch.sigmoid(i)  # è¾“å…¥é—¨
        f = torch.sigmoid(f)  # é—å¿˜é—¨
        g = torch.tanh(g)     # å€™é€‰ç»†èƒçŠ¶æ€
        o = torch.sigmoid(o)  # è¾“å‡ºé—¨

        # æ›´æ–°ç»†èƒçŠ¶æ€
        c = f * c_prev + i * g

        # è®¡ç®—éšè—çŠ¶æ€
        h = o * torch.tanh(c)

        return h, c

# ä½¿ç”¨ PyTorch å†…ç½® LSTM
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

# è¾“å…¥ï¼š(batch, seq_len, input_size)
x = torch.randn(3, 5, 10)  # 3 ä¸ªæ ·æœ¬ï¼Œåºåˆ—é•¿åº¦ 5ï¼Œç‰¹å¾ç»´åº¦ 10

# è¾“å‡ºï¼š(batch, seq_len, hidden_size), (h_n, c_n)
output, (h_n, c_n) = lstm(x)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"æœ€ç»ˆéšè—çŠ¶æ€: {h_n.shape}")
print(f"æœ€ç»ˆç»†èƒçŠ¶æ€: {c_n.shape}")
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šTransformer æ¶æ„è¯¦è§£

> **ğŸš€ åˆ’æ—¶ä»£çš„åˆ›æ–°**ï¼š2017 å¹´ï¼ŒGoogle å‘è¡¨è®ºæ–‡ã€ŠAttention Is All You Needã€‹ï¼Œæå‡º Transformer æ¶æ„ï¼Œå½»åº•æ”¹å˜äº† NLP é¢†åŸŸã€‚

### 1. ä¸ºä»€ä¹ˆéœ€è¦ Transformerï¼Ÿ

**RNN/LSTM çš„é—®é¢˜**ï¼š
- **ä¸²è¡Œè®¡ç®—**ï¼šå¿…é¡»æŒ‰é¡ºåºå¤„ç†ï¼Œæ— æ³•å¹¶è¡ŒåŒ–
- **é•¿è·ç¦»ä¾èµ–**ï¼šå³ä½¿æœ‰ LSTMï¼Œè¶…é•¿åºåˆ—ä»æœ‰é—®é¢˜
- **è®¡ç®—æ•ˆç‡ä½**ï¼šè®­ç»ƒæ…¢

**Transformer çš„ä¼˜åŠ¿**ï¼š
- **å¹¶è¡ŒåŒ–**ï¼šæ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—
- **é•¿è·ç¦»ä¾èµ–**ï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ç›´æ¥å»ºæ¨¡
- **å¯æ‰©å±•æ€§**ï¼šå®¹æ˜“æ‰©å±•åˆ°å¤§è§„æ¨¡æ¨¡å‹

### 2. è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰

> **ğŸ’¡ æ ¸å¿ƒæ€æƒ³**ï¼šè®©åºåˆ—ä¸­çš„æ¯ä¸ªè¯éƒ½èƒ½"å…³æ³¨"åˆ°å…¶ä»–æ‰€æœ‰è¯ã€‚

**ç›´è§‰ç†è§£**ï¼š
```
è¾“å…¥ï¼šThe cat sat on the mat

"sat" åº”è¯¥å…³æ³¨ï¼š
- "cat" (ä¸»è¯­)
- "on" (ä»‹è¯)
- "mat" (å®¾è¯­)

æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹ è¿™äº›å…³ç³»ï¼
```

**æ•°å­¦å…¬å¼**ï¼š

```python
"""
è‡ªæ³¨æ„åŠ›å…¬å¼ï¼š

1. è®¡ç®— Query, Key, Value
   Q = X Â· W_Q
   K = X Â· W_K
   V = X Â· W_V

2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
   scores = Q Â· K^T / sqrt(d_k)

3. Softmax å½’ä¸€åŒ–
   attention_weights = softmax(scores)

4. åŠ æƒæ±‚å’Œ
   output = attention_weights Â· V
"""

import math

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

    Args:
        Q: Query (batch, seq_len, d_k)
        K: Key (batch, seq_len, d_k)
        V: Value (batch, seq_len, d_v)
        mask: æ³¨æ„åŠ›æ©ç ï¼ˆå¯é€‰ï¼‰

    Returns:
        output: (batch, seq_len, d_v)
        attention_weights: (batch, seq_len, seq_len)
    """
    d_k = Q.size(-1)

    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # scores: (batch, seq_len, seq_len)

    # 2. åº”ç”¨æ©ç ï¼ˆå¯é€‰ï¼Œç”¨äºé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # 3. Softmax å½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)

    # 4. åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)

    return output, attention_weights

# ç¤ºä¾‹
batch_size = 2
seq_len = 4
d_k = 64

Q = torch.randn(batch_size, seq_len, d_k)
K = torch.randn(batch_size, seq_len, d_k)
V = torch.randn(batch_size, seq_len, d_k)

output, attn_weights = scaled_dot_product_attention(Q, K, V)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰:")
print(attn_weights[0].detach().numpy())
```

**å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡**ï¼š

```python
import matplotlib.pyplot as plt
import seaborn as sns

# ç¤ºä¾‹å¥å­
sentence = ["The", "cat", "sat", "on", "mat"]
seq_len = len(sentence)

# ç”Ÿæˆéšæœºæ³¨æ„åŠ›æƒé‡ï¼ˆå®é™…åº”ä»æ¨¡å‹è·å–ï¼‰
attn_weights = torch.softmax(torch.randn(seq_len, seq_len), dim=-1)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(attn_weights.numpy(), annot=True, fmt='.2f',
            xticklabels=sentence, yticklabels=sentence,
            cmap='YlOrRd')
plt.title('Self-Attention Weights')
plt.xlabel('Key')
plt.ylabel('Query')
plt.show()
```

### 3. å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ**
- å•ä¸ªæ³¨æ„åŠ›å¤´åªèƒ½å­¦ä¹ ä¸€ç§æ¨¡å¼
- å¤šå¤´å¯ä»¥å¹¶è¡Œå­¦ä¹ å¤šç§å…³ç³»ï¼ˆè¯­æ³•ã€è¯­ä¹‰ã€ä½ç½®ç­‰ï¼‰

```python
class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

    å…¬å¼ï¼š
        MultiHead(Q, K, V) = Concat(head_1, ..., head_h) Â· W_O
        å…¶ä¸­ head_i = Attention(QÂ·W_Q^i, KÂ·W_K^i, VÂ·W_V^i)
    """

    def __init__(self, d_model: int, num_heads: int):
        super(MultiHeadAttention, self).__init__()

        assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # çº¿æ€§å˜æ¢çŸ©é˜µ
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def split_heads(self, x):
        """
        å°†æœ€åä¸€ç»´åˆ†å‰²ä¸º (num_heads, d_k)

        Args:
            x: (batch, seq_len, d_model)

        Returns:
            (batch, num_heads, seq_len, d_k)
        """
        batch_size, seq_len, d_model = x.size()
        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # 1. çº¿æ€§å˜æ¢
        Q = self.W_Q(Q)  # (batch, seq_len, d_model)
        K = self.W_K(K)
        V = self.W_V(V)

        # 2. åˆ†å‰²ä¸ºå¤šå¤´
        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)

        # 3. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        d_k = Q.size(-1)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)
        # output: (batch, num_heads, seq_len, d_k)

        # 4. åˆå¹¶å¤šå¤´
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, -1, self.d_model)
        # output: (batch, seq_len, d_model)

        # 5. æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_O(output)

        return output, attention_weights

# æµ‹è¯•
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 2

mha = MultiHeadAttention(d_model, num_heads)
x = torch.randn(batch_size, seq_len, d_model)

output, attn_weights = mha(x, x, x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}")
```

### 4. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

**é—®é¢˜**ï¼šæ³¨æ„åŠ›æœºåˆ¶æ˜¯**ä½ç½®æ— å…³**çš„ï¼Œæ— æ³•åŒºåˆ†è¯çš„é¡ºåºã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šåœ¨è¾“å…¥åµŒå…¥ä¸­åŠ å…¥ä½ç½®ä¿¡æ¯ã€‚

```python
class PositionalEncoding(nn.Module):
    """
    ä½ç½®ç¼–ç 

    å…¬å¼ï¼š
        PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

    å…¶ä¸­ï¼š
        pos: ä½ç½®
        i: ç»´åº¦
    """

    def __init__(self, d_model: int, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()

        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´åº¦
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´åº¦

        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: (batch, seq_len, d_model)

        Returns:
            x + positional encoding
        """
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len, :]

# å¯è§†åŒ–ä½ç½®ç¼–ç 
d_model = 128
max_len = 100

pos_enc = PositionalEncoding(d_model, max_len)
pe_matrix = pos_enc.pe.squeeze(0).numpy()

plt.figure(figsize=(15, 5))
plt.imshow(pe_matrix.T, aspect='auto', cmap='RdBu')
plt.colorbar()
plt.xlabel('Position')
plt.ylabel('Dimension')
plt.title('Positional Encoding')
plt.show()
```

### 5. å®Œæ•´çš„ Transformer Encoder å±‚

```python
class TransformerEncoderLayer(nn.Module):
    """
    Transformer Encoder å±‚

    ç»“æ„ï¼š
        è¾“å…¥ â†’ Multi-Head Attention â†’ Add & Norm
             â†’ Feed-Forward â†’ Add & Norm â†’ è¾“å‡º
    """

    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super(TransformerEncoderLayer, self).__init__()

        # å¤šå¤´æ³¨æ„åŠ›
        self.self_attn = MultiHeadAttention(d_model, num_heads)

        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, seq_len, d_model)
            mask: æ³¨æ„åŠ›æ©ç 

        Returns:
            output: (batch, seq_len, d_model)
        """
        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥ + Layer Norm
        attn_output, _ = self.self_attn(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + Layer Norm
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

# å®Œæ•´çš„ Transformer Encoder
class TransformerEncoder(nn.Module):
    """
    å®Œæ•´çš„ Transformer Encoder
    """

    def __init__(
        self,
        vocab_size: int,
        d_model: int = 512,
        num_heads: int = 8,
        num_layers: int = 6,
        d_ff: int = 2048,
        max_len: int = 5000,
        dropout: float = 0.1
    ):
        super(TransformerEncoder, self).__init__()

        # è¯åµŒå…¥
        self.embedding = nn.Embedding(vocab_size, d_model)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_len)

        # Transformer å±‚
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        """
        Args:
            x: (batch, seq_len) è¯IDåºåˆ—

        Returns:
            output: (batch, seq_len, d_model)
        """
        # è¯åµŒå…¥
        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)

        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        x = self.dropout(x)

        # é€šè¿‡æ‰€æœ‰ Transformer å±‚
        for layer in self.layers:
            x = layer(x, mask)

        return x

# æµ‹è¯•å®Œæ•´æ¨¡å‹
vocab_size = 10000
model = TransformerEncoder(vocab_size)

# è¾“å…¥ï¼šä¸€æ‰¹è¯IDåºåˆ—
batch_size = 2
seq_len = 20
input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

output = model(input_ids)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

### 6. Transformer Decoderï¼ˆç”¨äºç”Ÿæˆä»»åŠ¡ï¼‰

```python
class TransformerDecoderLayer(nn.Module):
    """
    Transformer Decoder å±‚

    ç»“æ„ï¼š
        è¾“å…¥ â†’ Masked Self-Attention â†’ Add & Norm
             â†’ Cross-Attention â†’ Add & Norm
             â†’ Feed-Forward â†’ Add & Norm â†’ è¾“å‡º
    """

    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super(TransformerDecoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):
        # 1. Masked Self-Attentionï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        attn_output, _ = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # 2. Cross-Attentionï¼ˆå…³æ³¨ç¼–ç å™¨è¾“å‡ºï¼‰
        attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(attn_output))

        # 3. Feed-Forward
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x
```

---

## å°ç»“

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å®Œæˆäº†ä»åŸºç¡€åˆ°å‰æ²¿çš„å®Œæ•´æ—…ç¨‹ï¼š

âœ… **æ„ŸçŸ¥æœºä¸ç¥ç»å…ƒ**
- ç”Ÿç‰©ç¥ç»å…ƒ vs äººå·¥ç¥ç»å…ƒ
- å•å±‚æ„ŸçŸ¥æœºçš„å±€é™æ€§

âœ… **å¤šå±‚ç¥ç»ç½‘ç»œ**
- åå‘ä¼ æ’­ç®—æ³•æ¨å¯¼
- æ¿€æ´»å‡½æ•°çš„ä½œç”¨

âœ… **CNNï¼šå¾æœè§†è§‰**
- å·ç§¯ã€æ± åŒ–æ“ä½œ
- æ„Ÿå—é‡å’Œå‚æ•°å…±äº«

âœ… **RNN/LSTMï¼šå¤„ç†åºåˆ—**
- éšè—çŠ¶æ€å’Œè®°å¿†æœºåˆ¶
- é—¨æ§å•å…ƒ

âœ… **Transformerï¼šç°ä»£ AI çš„åŸºçŸ³**
- è‡ªæ³¨æ„åŠ›æœºåˆ¶
- å¤šå¤´æ³¨æ„åŠ›
- ä½ç½®ç¼–ç 
- å®Œæ•´çš„ Encoder-Decoder æ¶æ„

---

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜
1. æ‰‹åŠ¨è®¡ç®—ä¸€ä¸ª 3x3 å·ç§¯æ ¸åœ¨ 5x5 å›¾åƒä¸Šçš„å·ç§¯ç»“æœ
2. å®ç°ä¸€ä¸ªä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼Œç”¨åå‘ä¼ æ’­è®­ç»ƒ XOR é—®é¢˜
3. å¯è§†åŒ– LSTM çš„é—¨æ§æœºåˆ¶

### è¿›é˜¶é¢˜
4. ä»é›¶å®ç°ä¸€ä¸ªç®€å•çš„ CNNï¼Œåœ¨ MNIST ä¸Šè¾¾åˆ° 95% å‡†ç¡®ç‡
5. å®ç° Bi-LSTMï¼ˆåŒå‘ LSTMï¼‰ï¼Œç”¨äºæƒ…æ„Ÿåˆ†ç±»
6. å®ç°å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„ Seq2Seq æ¨¡å‹

### æŒ‘æˆ˜é¢˜
7. ä»é›¶å®ç°å®Œæ•´çš„ Transformerï¼Œç”¨äºæœºå™¨ç¿»è¯‘
8. åˆ†æä¸åŒæ•°é‡çš„æ³¨æ„åŠ›å¤´å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“
9. å®ç° Transformer-XLï¼ˆå¤„ç†è¶…é•¿åºåˆ—ï¼‰

---

**ä¸‹ä¸€èŠ‚ï¼š[10.6 å¤§è¯­è¨€æ¨¡å‹ï¼šTransformers ä¸ç°ä»£ NLP](10.6-å¤§è¯­è¨€æ¨¡å‹ï¼šTransformersä¸ç°ä»£NLP.md)**

åœ¨ä¸‹ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Hugging Face Transformers åº“ï¼ŒæŒæ¡ BERTã€GPT ç­‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å®ç°å¾®è°ƒå’Œæç¤ºå·¥ç¨‹ï¼
