# 10.6 å¤§è¯­è¨€æ¨¡å‹ï¼šTransformers ä¸ç°ä»£ NLP

## ä» Transformer åˆ° GPTï¼šNLP çš„èŒƒå¼è½¬å˜

åœ¨ä¸Šä¸€èŠ‚ï¼Œæˆ‘ä»¬ä»é›¶å®ç°äº† Transformer æ¶æ„ï¼Œç†è§£äº†å…¶æ•°å­¦åŸç†ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥**å·¥ä¸šçº§åº”ç”¨**â€”â€”å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹è§£å†³å®é™…é—®é¢˜ã€‚

> **ğŸ’¡ èŒƒå¼è½¬å˜**ï¼š
>
> **ä¼ ç»Ÿ NLP**ï¼šç‰¹å¾å·¥ç¨‹ + ä»»åŠ¡ç‰¹å®šæ¨¡å‹
> ```
> æ–‡æœ¬ â†’ è¯è¢‹/TF-IDF â†’ é€»è¾‘å›å½’/SVM â†’ é¢„æµ‹
> ```
>
> **ç°ä»£ NLP**ï¼šé¢„è®­ç»ƒ + å¾®è°ƒ
> ```
> é¢„è®­ç»ƒï¼ˆæµ·é‡æ— æ ‡æ³¨æ•°æ®ï¼‰â†’ å¤§æ¨¡å‹ â†’ å¾®è°ƒï¼ˆå°‘é‡æ ‡æ³¨æ•°æ®ï¼‰â†’ ç‰¹å®šä»»åŠ¡
> ```

**è¿™ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ **ï¼š
1. Hugging Face Transformers ç”Ÿæ€ç³»ç»Ÿ
2. BERT vs GPT æ¶æ„å¯¹æ¯”
3. Tokenization æŠ€æœ¯ï¼ˆBPEã€WordPieceï¼‰
4. Fine-tuning ç­–ç•¥ï¼ˆå…¨å‚æ•°ã€LoRAã€Prompt Tuningï¼‰
5. Prompt Engineering ç§‘å­¦åŸºç¡€
6. å®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»ã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆ

---

## Hugging Face Transformersï¼šç°ä»£ NLP çš„ç‘å£«å†›åˆ€

### 1. ç”Ÿæ€ç³»ç»Ÿæ¦‚è§ˆ

```python
"""
Hugging Face æ ¸å¿ƒåº“ï¼š

1. transformersï¼šé¢„è®­ç»ƒæ¨¡å‹åº“
2. datasetsï¼šæ•°æ®é›†åº“
3. tokenizersï¼šé«˜æ•ˆåˆ†è¯å™¨
4. accelerateï¼šåˆ†å¸ƒå¼è®­ç»ƒ
5. PEFTï¼šé«˜æ•ˆå¾®è°ƒæ–¹æ³•
"""

# å®‰è£…
# pip install transformers datasets tokenizers accelerate

from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    pipeline
)

# æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼ï¼špipeline
classifier = pipeline("sentiment-analysis")
result = classifier("I love this movie!")
print(result)
# [{'label': 'POSITIVE', 'score': 0.9998}]

# æ–‡æœ¬ç”Ÿæˆ
generator = pipeline("text-generation", model="gpt2")
result = generator("Once upon a time", max_length=30, num_return_sequences=1)
print(result[0]['generated_text'])
```

### 2. ä¸‰è¡Œä»£ç å®ç°æƒ…æ„Ÿåˆ†æ

```python
from transformers import pipeline

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
classifier = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

# 2. æ¨ç†
texts = [
    "This movie is absolutely fantastic!",
    "Worst film I've ever seen.",
    "It was okay, nothing special."
]

# 3. æ‰¹é‡é¢„æµ‹
results = classifier(texts)

for text, result in zip(texts, results):
    print(f"Text: {text}")
    print(f"Label: {result['label']}, Score: {result['score']:.4f}\n")
```

---

## BERT vs GPTï¼šç¼–ç å™¨ vs è§£ç å™¨

### 1. BERTï¼šåŒå‘ç¼–ç å™¨

**æ¶æ„**ï¼šTransformer Encoder
**è®­ç»ƒä»»åŠ¡**ï¼š
- Masked Language Modeling (MLM)
- Next Sentence Prediction (NSP)

```python
from transformers import BertTokenizer, BertModel
import torch

# åŠ è½½é¢„è®­ç»ƒ BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# è¾“å…¥æ–‡æœ¬
text = "The cat sat on the mat."

# åˆ†è¯
inputs = tokenizer(text, return_tensors='pt')
print("è¾“å…¥ ID:", inputs['input_ids'])
print("Token åˆ—è¡¨:", tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]))

# å‰å‘ä¼ æ’­
with torch.no_grad():
    outputs = model(**inputs)

# è·å–è¾“å‡º
last_hidden_state = outputs.last_hidden_state  # (1, seq_len, 768)
pooler_output = outputs.pooler_output  # (1, 768) - [CLS] token

print(f"\néšè—çŠ¶æ€å½¢çŠ¶: {last_hidden_state.shape}")
print(f"æ± åŒ–è¾“å‡ºå½¢çŠ¶: {pooler_output.shape}")
```

**BERT çš„ç‰¹ç‚¹**ï¼š
- **åŒå‘ä¸Šä¸‹æ–‡**ï¼šæ¯ä¸ªè¯éƒ½èƒ½çœ‹åˆ°å·¦å³ä¸¤ä¾§çš„ä¿¡æ¯
- **é€‚ç”¨ä»»åŠ¡**ï¼šåˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«
- **ä¸é€‚åˆç”Ÿæˆ**ï¼šå› ä¸ºæ˜¯åŒå‘çš„ï¼Œæ— æ³•ç”¨äºè‡ªå›å½’ç”Ÿæˆ

### 2. GPTï¼šè‡ªå›å½’è§£ç å™¨

**æ¶æ„**ï¼šTransformer Decoderï¼ˆå¸¦ Causal Maskï¼‰
**è®­ç»ƒä»»åŠ¡**ï¼šNext Token Prediction

```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# åŠ è½½ GPT-2
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# è®¾ç½® pad_token
tokenizer.pad_token = tokenizer.eos_token

# æ–‡æœ¬ç”Ÿæˆ
prompt = "In a galaxy far, far away"
inputs = tokenizer(prompt, return_tensors='pt')

# ç”Ÿæˆæ–‡æœ¬
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        max_length=50,
        num_return_sequences=3,
        temperature=0.8,
        top_k=50,
        top_p=0.95,
        do_sample=True
    )

# è§£ç ç”Ÿæˆçš„æ–‡æœ¬
print("ç”Ÿæˆçš„æ–‡æœ¬:\n")
for i, output in enumerate(outputs):
    text = tokenizer.decode(output, skip_special_tokens=True)
    print(f"{i+1}. {text}\n")
```

### 3. BERT vs GPT å¯¹æ¯”è¡¨

| ç‰¹æ€§ | BERT | GPT |
|------|------|-----|
| **æ¶æ„** | Encoder-only | Decoder-only |
| **æ³¨æ„åŠ›ç±»å‹** | åŒå‘æ³¨æ„åŠ› | Causalï¼ˆå•å‘ï¼‰æ³¨æ„åŠ› |
| **è®­ç»ƒç›®æ ‡** | MLM + NSP | Next Token Prediction |
| **æœ€ä½³åº”ç”¨** | ç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€é—®ç­”ï¼‰ | ç”Ÿæˆä»»åŠ¡ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰ |
| **è¾“å…¥-è¾“å‡º** | å…¨å¥ â†’ è¡¨ç¤ºå‘é‡ | å‰ç¼€ â†’ ç»­å†™ |
| **å…¸å‹æ¨¡å‹** | BERT, RoBERTa, ELECTRA | GPT-2, GPT-3, GPT-4 |

---

## Tokenizationï¼šä»æ–‡æœ¬åˆ°æ•°å­—

### 1. ä¸ºä»€ä¹ˆéœ€è¦ Tokenizationï¼Ÿ

ç¥ç»ç½‘ç»œåªèƒ½å¤„ç†æ•°å­—ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—ã€‚

**ä¸‰ç§ä¸»æµæ–¹æ³•**ï¼š
1. **Word-level**ï¼šæ¯ä¸ªè¯ä¸€ä¸ª IDï¼ˆè¯è¡¨å¤ªå¤§ï¼‰
2. **Character-level**ï¼šæ¯ä¸ªå­—ç¬¦ä¸€ä¸ª IDï¼ˆåºåˆ—å¤ªé•¿ï¼‰
3. **Subword-level**ï¼šä»‹äºè¯å’Œå­—ç¬¦ä¹‹é—´ï¼ˆæœ€ä½³å¹³è¡¡ï¼‰

### 2. BPEï¼ˆByte Pair Encodingï¼‰

GPT ç³»åˆ—ä½¿ç”¨çš„åˆ†è¯ç®—æ³•ã€‚

```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

text = "The quick brown fox jumps over the lazy dog."

# åˆ†è¯
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# è½¬æ¢ä¸º ID
input_ids = tokenizer.encode(text)
print("Input IDs:", input_ids)

# è§£ç 
decoded = tokenizer.decode(input_ids)
print("Decoded:", decoded)

# æŸ¥çœ‹è¯è¡¨å¤§å°
print(f"\nè¯è¡¨å¤§å°: {tokenizer.vocab_size}")

# å¤„ç†æœªçŸ¥è¯
oov_text = "supercalifragilisticexpialidocious"
oov_tokens = tokenizer.tokenize(oov_text)
print(f"\nOOV åˆ†è¯: {oov_tokens}")
```

### 3. WordPieceï¼ˆBERT ä½¿ç”¨ï¼‰

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "unhappiness"

# åˆ†è¯
tokens = tokenizer.tokenize(text)
print("WordPiece tokens:", tokens)
# ['un', '##happiness']

# ç‰¹æ®Šæ ‡è®°
special_tokens = ["[CLS]", "[SEP]", "[MASK]", "[PAD]", "[UNK]"]
print("\nç‰¹æ®Šæ ‡è®° IDs:")
for token in special_tokens:
    print(f"{token}: {tokenizer.convert_tokens_to_ids(token)}")
```

### 4. è‡ªå®šä¹‰ Tokenizer

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# åˆ›å»º BPE tokenizer
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

# é¢„åˆ†è¯ï¼ˆæŒ‰ç©ºæ ¼ï¼‰
tokenizer.pre_tokenizer = Whitespace()

# è®­ç»ƒï¼ˆéœ€è¦æä¾›æ–‡æœ¬æ–‡ä»¶åˆ—è¡¨ï¼‰
# tokenizer.train(files=["corpus.txt"], trainer=trainer)

# ä¿å­˜å’ŒåŠ è½½
# tokenizer.save("my_tokenizer.json")
# tokenizer = Tokenizer.from_file("my_tokenizer.json")
```

---

## Fine-tuningï¼šè®©å¤§æ¨¡å‹é€‚åº”ç‰¹å®šä»»åŠ¡

### 1. å®Œæ•´çš„ Fine-tuning Pipeline

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# 1. åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")

# æŸ¥çœ‹æ•°æ®
print(dataset['train'][0])

# 2. åŠ è½½ tokenizer å’Œæ¨¡å‹
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2  # äºŒåˆ†ç±»
)

# 3. æ•°æ®é¢„å¤„ç†
def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=512
    )

# åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# 4. åˆ›å»ºå°çš„è®­ç»ƒ/éªŒè¯é›†ï¼ˆæ¼”ç¤ºç”¨ï¼‰
small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(1000))

# 5. å®šä¹‰è¯„ä¼°æŒ‡æ ‡
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions)

    return {
        'accuracy': accuracy,
        'f1': f1
    }

# 6. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
)

# 7. åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

# 8. è®­ç»ƒ
# trainer.train()

# 9. è¯„ä¼°
# results = trainer.evaluate()
# print(results)

# 10. ä¿å­˜æ¨¡å‹
# model.save_pretrained('./my_finetuned_model')
# tokenizer.save_pretrained('./my_finetuned_model')
```

### 2. å†»ç»“éƒ¨åˆ†å±‚

```python
# å†»ç»“ BERT çš„åº•å±‚ï¼Œåªè®­ç»ƒé¡¶å±‚
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# å†»ç»“å‰ 10 å±‚
for name, param in model.named_parameters():
    if 'bert.encoder.layer' in name:
        layer_num = int(name.split('.')[3])
        if layer_num < 10:
            param.requires_grad = False

# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())

print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / {total_params:,} ({100*trainable_params/total_params:.2f}%)")
```

### 3. LoRAï¼šé«˜æ•ˆå¾®è°ƒ

LoRA (Low-Rank Adaptation) åªè®­ç»ƒä½ç§©çŸ©é˜µï¼Œå¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°ã€‚

```python
from peft import get_peft_model, LoraConfig, TaskType

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# LoRA é…ç½®
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,  # ä½ç§©ç»´åº¦
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["query", "value"]  # åªåœ¨ attention çš„ Q, V ä¸Šåº”ç”¨
)

# åº”ç”¨ LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 294,912 || all params: 67,584,002 || trainable%: 0.4363%

# è®­ç»ƒï¼ˆä½¿ç”¨ç›¸åŒçš„ Trainerï¼‰
# trainer = Trainer(model=model, ...)
# trainer.train()
```

---

## Prompt Engineeringï¼šä¸ LLM å¯¹è¯çš„è‰ºæœ¯

### 1. Zero-Shot Learning

```python
from transformers import pipeline

# ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œé›¶æ ·æœ¬åˆ†ç±»
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

text = "The new iPhone has an amazing camera and long battery life."
candidate_labels = ["technology", "sports", "politics", "entertainment"]

result = classifier(text, candidate_labels)
print(f"Text: {text}\n")
print("Predictions:")
for label, score in zip(result['labels'], result['scores']):
    print(f"  {label}: {score:.4f}")
```

### 2. Few-Shot Learning

```python
# GPT é£æ ¼çš„ few-shot prompt
prompt = """
Classify the sentiment of the following reviews:

Review: "This movie was absolutely fantastic!"
Sentiment: Positive

Review: "Waste of time and money."
Sentiment: Negative

Review: "It was okay, nothing special."
Sentiment: Neutral

Review: "I loved every minute of it!"
Sentiment:
"""

generator = pipeline("text-generation", model="gpt2")
result = generator(prompt, max_length=len(prompt.split()) + 5, num_return_sequences=1)
print(result[0]['generated_text'])
```

### 3. Prompt è®¾è®¡åŸåˆ™

```python
"""
å¥½çš„ Prompt åº”è¯¥ï¼š

1. æ¸…æ™°æ˜ç¡®
   âŒ "å‘Šè¯‰æˆ‘å…³äºç‹—çš„äº‹"
   âœ… "åˆ—å‡º 5 ä¸ªé€‚åˆå®¶åº­é¥²å…»çš„ç‹—å“ç§ï¼Œå¹¶è¯´æ˜æ¯ä¸ªå“ç§çš„ç‰¹ç‚¹"

2. æä¾›ä¸Šä¸‹æ–‡
   âŒ "ç¿»è¯‘è¿™ä¸ª"
   âœ… "å°†ä»¥ä¸‹è‹±æ–‡æŠ€æœ¯æ–‡æ¡£ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ï¼Œä¿æŒä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§"

3. ä½¿ç”¨ç¤ºä¾‹ï¼ˆFew-shotï¼‰
   åŒ…å« 2-3 ä¸ªè¾“å…¥-è¾“å‡ºç¤ºä¾‹

4. æŒ‡å®šè¾“å‡ºæ ¼å¼
   "ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å« 'name' å’Œ 'description' å­—æ®µ"

5. è®¾ç½®çº¦æŸ
   "å›ç­”æ§åˆ¶åœ¨ 100 å­—ä»¥å†…"
   "åªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ "
"""

# å®ç”¨çš„ Prompt æ¨¡æ¿
def create_classification_prompt(text, labels):
    """åˆ›å»ºåˆ†ç±»ä»»åŠ¡çš„ prompt"""
    prompt = f"""
Task: Classify the following text into one of these categories: {', '.join(labels)}

Text: {text}

Category:"""
    return prompt

# ç¤ºä¾‹
text = "The stock market crashed today, losing 500 points."
labels = ["business", "sports", "politics", "entertainment"]
prompt = create_classification_prompt(text, labels)
print(prompt)
```

---

## å®æˆ˜é¡¹ç›® 1ï¼šæ–‡æœ¬åˆ†ç±»ç³»ç»Ÿ

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from typing import List, Dict
import torch

class TextClassifier:
    """
    é€šç”¨æ–‡æœ¬åˆ†ç±»å™¨

    æ”¯æŒï¼š
    - å¤šç±»åˆ«åˆ†ç±»
    - æ‰¹é‡é¢„æµ‹
    - è‡ªå®šä¹‰é˜ˆå€¼
    """

    def __init__(self, model_name: str = "distilbert-base-uncased", num_labels: int = 2):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def predict(self, texts: List[str], batch_size: int = 32) -> List[Dict]:
        """
        æ‰¹é‡é¢„æµ‹

        Args:
            texts: å¾…åˆ†ç±»æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {label, score}
        """
        self.model.eval()
        results = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]

            # åˆ†è¯
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(self.device)

            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1)

            # è§£æç»“æœ
            for prob in probabilities:
                pred_label = prob.argmax().item()
                pred_score = prob.max().item()

                results.append({
                    'label': pred_label,
                    'score': pred_score
                })

        return results

    def fine_tune(self, train_texts: List[str], train_labels: List[int], epochs: int = 3):
        """
        å¾®è°ƒæ¨¡å‹

        Args:
            train_texts: è®­ç»ƒæ–‡æœ¬
            train_labels: è®­ç»ƒæ ‡ç­¾
            epochs: è®­ç»ƒè½®æ•°
        """
        from torch.utils.data import Dataset, DataLoader
        from torch.optim import AdamW

        # è‡ªå®šä¹‰æ•°æ®é›†
        class TextDataset(Dataset):
            def __init__(self, texts, labels, tokenizer):
                self.texts = texts
                self.labels = labels
                self.tokenizer = tokenizer

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                encoding = self.tokenizer(
                    self.texts[idx],
                    padding='max_length',
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                )

                return {
                    'input_ids': encoding['input_ids'].flatten(),
                    'attention_mask': encoding['attention_mask'].flatten(),
                    'labels': torch.tensor(self.labels[idx])
                }

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = TextDataset(train_texts, train_labels, self.tokenizer)
        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

        # ä¼˜åŒ–å™¨
        optimizer = AdamW(self.model.parameters(), lr=2e-5)

        # è®­ç»ƒå¾ªç¯
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0

            for batch in dataloader:
                # ç§»åŠ¨åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(dataloader)
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

# ä½¿ç”¨ç¤ºä¾‹
classifier = TextClassifier()

texts = [
    "This product is amazing!",
    "Terrible experience, would not recommend.",
    "It's okay, nothing special."
]

results = classifier.predict(texts)
for text, result in zip(texts, results):
    print(f"Text: {text}")
    print(f"Label: {result['label']}, Score: {result['score']:.4f}\n")
```

---

## å®æˆ˜é¡¹ç›® 2ï¼šé—®ç­”ç³»ç»Ÿ

```python
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import torch

class QASystem:
    """
    é—®ç­”ç³»ç»Ÿï¼ˆåŸºäº BERTï¼‰

    æ”¯æŒï¼š
    - æŠ½å–å¼é—®ç­”ï¼ˆä»ç»™å®šæ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼‰
    - æ‰¹é‡é—®ç­”
    - ç½®ä¿¡åº¦è¯„ä¼°
    """

    def __init__(self, model_name: str = "deepset/roberta-base-squad2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def answer(self, question: str, context: str, top_k: int = 3):
        """
        å›ç­”é—®é¢˜

        Args:
            question: é—®é¢˜
            context: ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬ï¼‰
            top_k: è¿”å›å‰ k ä¸ªç­”æ¡ˆ

        Returns:
            ç­”æ¡ˆåˆ—è¡¨
        """
        # åˆ†è¯
        inputs = self.tokenizer(
            question,
            context,
            return_tensors='pt',
            truncation=True,
            max_length=512
        ).to(self.device)

        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)

        # è·å–ç­”æ¡ˆèµ·å§‹å’Œç»“æŸä½ç½®
        start_logits = outputs.start_logits
        end_logits = outputs.end_logits

        # æ‰¾åˆ°æœ€å¯èƒ½çš„ç­”æ¡ˆ
        start_idx = torch.argmax(start_logits)
        end_idx = torch.argmax(end_logits)

        # æå–ç­”æ¡ˆ
        answer_tokens = inputs['input_ids'][0][start_idx:end_idx+1]
        answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True)

        # è®¡ç®—ç½®ä¿¡åº¦
        start_score = torch.softmax(start_logits, dim=-1)[0, start_idx].item()
        end_score = torch.softmax(end_logits, dim=-1)[0, end_idx].item()
        confidence = (start_score + end_score) / 2

        return {
            'answer': answer,
            'confidence': confidence,
            'start': start_idx.item(),
            'end': end_idx.item()
        }

# ä½¿ç”¨ç¤ºä¾‹
qa_system = QASystem()

context = """
The Transformer architecture was introduced in the paper "Attention Is All You Need"
by Vaswani et al. in 2017. It revolutionized natural language processing by replacing
recurrent neural networks with self-attention mechanisms. The key innovation was the
ability to process all positions in the input sequence in parallel, making training
much faster than RNNs.
"""

questions = [
    "When was the Transformer introduced?",
    "Who introduced the Transformer?",
    "What did the Transformer replace?"
]

for question in questions:
    result = qa_system.answer(question, context)
    print(f"Q: {question}")
    print(f"A: {result['answer']}")
    print(f"Confidence: {result['confidence']:.4f}\n")
```

---

## å®æˆ˜é¡¹ç›® 3ï¼šæ–‡æœ¬ç”Ÿæˆ

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

class TextGenerator:
    """
    æ–‡æœ¬ç”Ÿæˆå™¨ï¼ˆåŸºäº GPT-2ï¼‰

    æ”¯æŒï¼š
    - ç»­å†™æ–‡æœ¬
    - æ§åˆ¶ç”Ÿæˆé•¿åº¦ã€æ¸©åº¦ã€top-kã€top-p
    - æ‰¹é‡ç”Ÿæˆ
    """

    def __init__(self, model_name: str = "gpt2"):
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        self.model = GPT2LMHeadModel.from_pretrained(model_name)

        # è®¾ç½® pad_token
        self.tokenizer.pad_token = self.tokenizer.eos_token

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def generate(
        self,
        prompt: str,
        max_length: int = 100,
        num_return_sequences: int = 1,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.95,
        do_sample: bool = True
    ) -> List[str]:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§é•¿åº¦
            num_return_sequences: ç”Ÿæˆå‡ ä¸ªåºåˆ—
            temperature: æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
            top_k: Top-K é‡‡æ ·
            top_p: Nucleus é‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·ï¼ˆFalse ä¸ºè´ªå©ªè§£ç ï¼‰

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # ç¼–ç è¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                inputs['input_ids'],
                max_length=max_length,
                num_return_sequences=num_return_sequences,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        generated_texts = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            generated_texts.append(text)

        return generated_texts

# ä½¿ç”¨ç¤ºä¾‹
generator = TextGenerator()

prompts = [
    "In the year 2050, artificial intelligence",
    "The secret to happiness is",
    "Once upon a time in a distant land"
]

for prompt in prompts:
    print(f"Prompt: {prompt}\n")

    # è´ªå©ªè§£ç ï¼ˆç¡®å®šæ€§ï¼‰
    greedy = generator.generate(prompt, max_length=50, do_sample=False, num_return_sequences=1)
    print(f"Greedy: {greedy[0]}\n")

    # é«˜æ¸©åº¦é‡‡æ ·ï¼ˆæ›´éšæœºï¼‰
    creative = generator.generate(prompt, max_length=50, temperature=1.5, num_return_sequences=1)
    print(f"Creative: {creative[0]}\n")

    print("-" * 80 + "\n")
```

---

## æ¨¡å‹é€‰æ‹©æŒ‡å—

### 1. å¸¸ç”¨é¢„è®­ç»ƒæ¨¡å‹

| æ¨¡å‹ | å‚æ•°é‡ | ä»»åŠ¡ç±»å‹ | ä¼˜åŠ¿ | æ¨èåœºæ™¯ |
|------|--------|---------|------|----------|
| **BERT-base** | 110M | ç†è§£ | åŒå‘ä¸Šä¸‹æ–‡ | åˆ†ç±»ã€é—®ç­”ã€NER |
| **RoBERTa** | 125M | ç†è§£ | BERTæ”¹è¿›ç‰ˆ | é«˜ç²¾åº¦åˆ†ç±» |
| **DistilBERT** | 66M | ç†è§£ | å¿«é€Ÿæ¨ç† | èµ„æºå—é™ç¯å¢ƒ |
| **GPT-2** | 124M-1.5B | ç”Ÿæˆ | æµç•…ç”Ÿæˆ | æ–‡æœ¬ç»­å†™ |
| **T5** | 60M-11B | ä»»æ„ | ç»Ÿä¸€æ¡†æ¶ | å¤šä»»åŠ¡å­¦ä¹  |
| **BART** | 140M | ç”Ÿæˆ | Seq2Seq | æ‘˜è¦ã€ç¿»è¯‘ |

### 2. å¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Ÿ

```python
"""
å†³ç­–æ ‘ï¼š

1. ä»»åŠ¡ç±»å‹ï¼Ÿ
   - ç†è§£ï¼ˆåˆ†ç±»ã€é—®ç­”ï¼‰â†’ BERT ç³»åˆ—
   - ç”Ÿæˆï¼ˆç»­å†™ã€å¯¹è¯ï¼‰â†’ GPT ç³»åˆ—
   - Seq2Seqï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰â†’ T5/BART

2. èµ„æºé™åˆ¶ï¼Ÿ
   - è®¡ç®—èµ„æºå……è¶³ â†’ å¤§æ¨¡å‹ï¼ˆRoBERTa-large, GPT-2-largeï¼‰
   - èµ„æºå—é™ â†’ è’¸é¦æ¨¡å‹ï¼ˆDistilBERT, DistilGPT-2ï¼‰

3. è¯­è¨€ï¼Ÿ
   - è‹±æ–‡ â†’ åŸå§‹æ¨¡å‹
   - ä¸­æ–‡ â†’ BERT-Chinese, GPT-Chinese
   - å¤šè¯­è¨€ â†’ XLM-RoBERTa, mBERT

4. é¢†åŸŸï¼Ÿ
   - é€šç”¨ â†’ é¢„è®­ç»ƒæ¨¡å‹
   - ç‰¹å®šé¢†åŸŸï¼ˆåŒ»ç–—ã€æ³•å¾‹ï¼‰â†’ é¢†åŸŸé€‚é…æ¨¡å‹ï¼ˆBioBERT, LegalBERTï¼‰
"""
```

---

## å°ç»“

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

âœ… **Hugging Face ç”Ÿæ€ç³»ç»Ÿ**
- Transformers åº“çš„ä½¿ç”¨
- Pipeline API å¿«é€Ÿå¼€å‘

âœ… **é¢„è®­ç»ƒæ¨¡å‹**
- BERT vs GPT æ¶æ„å¯¹æ¯”
- æ¨¡å‹é€‰æ‹©æŒ‡å—

âœ… **Tokenization**
- BPEã€WordPiece åŸç†
- è‡ªå®šä¹‰ Tokenizer

âœ… **Fine-tuning ç­–ç•¥**
- å…¨å‚æ•°å¾®è°ƒ
- å†»ç»“å±‚å¾®è°ƒ
- LoRA é«˜æ•ˆå¾®è°ƒ

âœ… **Prompt Engineering**
- Zero-shot å’Œ Few-shot å­¦ä¹ 
- Prompt è®¾è®¡åŸåˆ™

âœ… **å®æˆ˜é¡¹ç›®**
- æ–‡æœ¬åˆ†ç±»ç³»ç»Ÿ
- é—®ç­”ç³»ç»Ÿ
- æ–‡æœ¬ç”Ÿæˆ

---

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜
1. ä½¿ç”¨ BERT å®ç°æƒ…æ„Ÿåˆ†ç±»ï¼ˆIMDb æ•°æ®é›†ï¼‰
2. ç”¨ GPT-2 ç”Ÿæˆç»™å®šä¸»é¢˜çš„çŸ­æ•…äº‹
3. å®ç°ä¸€ä¸ªç®€å•çš„é—®ç­”ç³»ç»Ÿ

### è¿›é˜¶é¢˜
4. å¯¹æ¯” DistilBERT å’Œ BERT-base åœ¨ç›¸åŒä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œæ¨ç†é€Ÿåº¦
5. ä½¿ç”¨ LoRA å¾®è°ƒ GPT-2ï¼Œæ§åˆ¶ç”Ÿæˆé£æ ¼
6. å®ç°å¤šæ ‡ç­¾æ–‡æœ¬åˆ†ç±»

### æŒ‘æˆ˜é¢˜
7. æ„å»ºä¸€ä¸ª RAG (Retrieval-Augmented Generation) ç³»ç»Ÿ
8. å®ç° Prompt Tuningï¼ˆåªè®­ç»ƒ soft promptsï¼‰
9. éƒ¨ç½² Hugging Face æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒï¼ˆFastAPI + Dockerï¼‰

---

**ä¸‹ä¸€èŠ‚ï¼š[10.7 ç»¼åˆå®æˆ˜ï¼šæ„å»ºç«¯åˆ°ç«¯ AI åº”ç”¨](10.7-ç»¼åˆå®æˆ˜ï¼šæ„å»ºç«¯åˆ°ç«¯AIåº”ç”¨.md)**

åœ¨æœ€åä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†æ•´åˆæ‰€æœ‰çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ AI åº”ç”¨ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éƒ¨ç½²å’Œç›‘æ§ï¼
