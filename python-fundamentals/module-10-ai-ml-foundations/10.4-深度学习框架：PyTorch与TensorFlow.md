# 10.4 æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šPyTorch ä¸ TensorFlow

## ä»æœºå™¨å­¦ä¹ åˆ°æ·±åº¦å­¦ä¹ 

åœ¨ä¸Šä¸€èŠ‚ï¼Œæˆ‘ä»¬å­¦ä¹ äº† Scikit-Learn ä¸­çš„ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿ˆå‘æ›´å¼ºå¤§çš„æ·±åº¦å­¦ä¹ ä¸–ç•Œã€‚

> **ğŸ’¡ æ ¸å¿ƒé—®é¢˜**ï¼šä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿ
>
> ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼šéœ€è¦äººå·¥è®¾è®¡ç‰¹å¾ï¼ˆfeature engineeringï¼‰
> ```
> å›¾åƒ â†’ æ‰‹å·¥æå–è¾¹ç¼˜ã€çº¹ç†ç­‰ç‰¹å¾ â†’ åˆ†ç±»å™¨ â†’ é¢„æµ‹
> ```
>
> æ·±åº¦å­¦ä¹ ï¼šè‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤º
> ```
> å›¾åƒ â†’ ç¥ç»ç½‘ç»œï¼ˆè‡ªåŠ¨å­¦ä¹ å±‚æ¬¡åŒ–ç‰¹å¾ï¼‰â†’ é¢„æµ‹
> ```

æ·±åº¦å­¦ä¹ åœ¨ä»¥ä¸‹ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼š
- **è®¡ç®—æœºè§†è§‰**ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒç”Ÿæˆ
- **è‡ªç„¶è¯­è¨€å¤„ç†**ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æ
- **è¯­éŸ³è¯†åˆ«**ï¼šè¯­éŸ³è½¬æ–‡å­—ã€è¯­éŸ³åˆæˆ
- **å¼ºåŒ–å­¦ä¹ **ï¼šæ¸¸æˆ AIã€æœºå™¨äººæ§åˆ¶

---

## PyTorch vs TensorFlowï¼šä¸¤å¤§æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | PyTorch | TensorFlow |
|------|---------|------------|
| **å¼€å‘è€…** | Meta (Facebook) | Google |
| **è®¡ç®—å›¾** | åŠ¨æ€å›¾ï¼ˆDefine-by-Runï¼‰ | 2.x é»˜è®¤åŠ¨æ€å›¾ï¼ˆEager Executionï¼‰ |
| **æ˜“ç”¨æ€§** | æ›´ Pythonicï¼Œæ˜“äºè°ƒè¯• | Keras API ç®€åŒ–äº†ä½¿ç”¨ |
| **ç”Ÿæ€ç³»ç»Ÿ** | å­¦æœ¯ç•Œä¸»æµï¼ŒHugging Face | å·¥ä¸šç•Œå¹¿æ³›åº”ç”¨ï¼ŒTensorFlow Serving |
| **ç§»åŠ¨éƒ¨ç½²** | TorchScript, PyTorch Mobile | TensorFlow Lite |
| **æ€§èƒ½** | ä¼˜ç§€ | ä¼˜ç§€ |

> **ğŸ“Œ æœ¬ç« é‡ç‚¹**ï¼šæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹  **PyTorch**ï¼Œå› ä¸ºå®ƒåœ¨ç ”ç©¶ç•Œå’Œ AI Agent å¼€å‘ä¸­æ›´å—æ¬¢è¿ã€‚æœ€åä¼šç®€è¦ä»‹ç» TensorFlow/Kerasã€‚

---

## PyTorch åŸºç¡€ï¼šå¼ é‡æ“ä½œ

### 1. å¼ é‡ï¼ˆTensorï¼‰ï¼šæ·±åº¦å­¦ä¹ çš„åŸºæœ¬å•å…ƒ

PyTorch çš„ `torch.Tensor` å’Œ NumPy çš„ `ndarray` éå¸¸ç›¸ä¼¼ï¼Œä½†æœ‰ä¸¤ä¸ªå…³é”®ä¼˜åŠ¿ï¼š
- **GPU åŠ é€Ÿ**ï¼šå¯ä»¥åœ¨ GPU ä¸Šè¿›è¡Œè®¡ç®—
- **è‡ªåŠ¨å¾®åˆ†**ï¼šå¯ä»¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦

```python
import torch
import numpy as np

# åˆ›å»ºå¼ é‡çš„å¤šç§æ–¹å¼
# 1. ä» Python åˆ—è¡¨åˆ›å»º
x = torch.tensor([1, 2, 3, 4, 5])
print(f"x: {x}, dtype: {x.dtype}")

# 2. ä» NumPy æ•°ç»„åˆ›å»º
np_array = np.array([1.0, 2.0, 3.0])
tensor_from_numpy = torch.from_numpy(np_array)
print(f"From NumPy: {tensor_from_numpy}")

# 3. åˆ›å»ºç‰¹æ®Šå¼ é‡
zeros = torch.zeros(3, 4)  # 3x4 é›¶çŸ©é˜µ
ones = torch.ones(2, 3)    # 2x3 å…¨ 1 çŸ©é˜µ
rand = torch.rand(2, 3)    # 2x3 éšæœºçŸ©é˜µ [0, 1)
randn = torch.randn(2, 3)  # 2x3 æ ‡å‡†æ­£æ€åˆ†å¸ƒ
eye = torch.eye(5)         # 5x5 å•ä½çŸ©é˜µ

print(f"zeros:\n{zeros}")
print(f"randn:\n{randn}")

# 4. æŒ‡å®šæ•°æ®ç±»å‹
x_float = torch.tensor([1, 2, 3], dtype=torch.float32)
x_double = torch.tensor([1, 2, 3], dtype=torch.float64)
x_int = torch.tensor([1, 2, 3], dtype=torch.int32)

print(f"float32: {x_float.dtype}, float64: {x_double.dtype}, int32: {x_int.dtype}")

# 5. å¼ é‡çš„å½¢çŠ¶æ“ä½œ
x = torch.randn(2, 3, 4)  # 2x3x4 å¼ é‡
print(f"åŸå§‹å½¢çŠ¶: {x.shape}")

# é‡å¡‘
x_reshaped = x.view(2, 12)  # å˜ä¸º 2x12
print(f"é‡å¡‘å: {x_reshaped.shape}")

# è‡ªåŠ¨æ¨æ–­ç»´åº¦
x_reshaped2 = x.view(2, -1)  # -1 è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
print(f"è‡ªåŠ¨æ¨æ–­: {x_reshaped2.shape}")

# è½¬ç½®
x_2d = torch.randn(3, 4)
x_transposed = x_2d.t()  # è½¬ç½®
print(f"è½¬ç½®å‰: {x_2d.shape}, è½¬ç½®å: {x_transposed.shape}")

# å¤šç»´è½¬ç½®
x_3d = torch.randn(2, 3, 4)
x_permuted = x_3d.permute(2, 0, 1)  # (2,3,4) â†’ (4,2,3)
print(f"permute å: {x_permuted.shape}")
```

### 2. å¼ é‡è¿ç®—

```python
# åŸºæœ¬è¿ç®—
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

# é€å…ƒç´ è¿ç®—
print(f"åŠ æ³•: {a + b}")
print(f"å‡æ³•: {a - b}")
print(f"ä¹˜æ³•: {a * b}")
print(f"é™¤æ³•: {a / b}")
print(f"å¹‚è¿ç®—: {a ** 2}")

# çŸ©é˜µè¿ç®—
A = torch.randn(3, 4)
B = torch.randn(4, 5)

# çŸ©é˜µä¹˜æ³•
C = torch.mm(A, B)  # æˆ–è€… A @ B
print(f"çŸ©é˜µä¹˜æ³•: {A.shape} @ {B.shape} = {C.shape}")

# æ‰¹é‡çŸ©é˜µä¹˜æ³•
batch_A = torch.randn(10, 3, 4)  # 10 ä¸ª 3x4 çŸ©é˜µ
batch_B = torch.randn(10, 4, 5)  # 10 ä¸ª 4x5 çŸ©é˜µ
batch_C = torch.bmm(batch_A, batch_B)  # 10 ä¸ª 3x5 çŸ©é˜µ
print(f"æ‰¹é‡çŸ©é˜µä¹˜æ³•: {batch_C.shape}")

# ç»Ÿè®¡è¿ç®—
x = torch.randn(3, 4)
print(f"æ±‚å’Œ: {x.sum()}")
print(f"å‡å€¼: {x.mean()}")
print(f"æœ€å¤§å€¼: {x.max()}")
print(f"æ²¿è½´æ±‚å’Œ: {x.sum(dim=0).shape}")  # æ²¿ç¬¬ 0 ç»´
print(f"æ²¿è½´æ±‚å’Œ: {x.sum(dim=1).shape}")  # æ²¿ç¬¬ 1 ç»´
```

### 3. GPU åŠ é€Ÿ

```python
# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å°†å¼ é‡ç§»åŠ¨åˆ° GPU
x_cpu = torch.randn(1000, 1000)
x_gpu = x_cpu.to(device)  # æˆ–è€… x_cpu.cuda()

# åœ¨ GPU ä¸Šè®¡ç®—
y_gpu = x_gpu @ x_gpu.t()

# ç§»å› CPU
y_cpu = y_gpu.to('cpu')  # æˆ–è€… y_gpu.cpu()

# æ€§èƒ½å¯¹æ¯”
import time

# CPU è®¡ç®—
x_cpu = torch.randn(5000, 5000)
start = time.time()
y_cpu = x_cpu @ x_cpu.t()
cpu_time = time.time() - start

# GPU è®¡ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if torch.cuda.is_available():
    x_gpu = x_cpu.to('cuda')
    torch.cuda.synchronize()  # ç­‰å¾… GPU å®Œæˆ
    start = time.time()
    y_gpu = x_gpu @ x_gpu.t()
    torch.cuda.synchronize()
    gpu_time = time.time() - start

    print(f"CPU æ—¶é—´: {cpu_time:.4f}s")
    print(f"GPU æ—¶é—´: {gpu_time:.4f}s")
    print(f"åŠ é€Ÿæ¯”: {cpu_time / gpu_time:.2f}x")
else:
    print("GPU ä¸å¯ç”¨")
```

---

## è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰ï¼šæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒ

æ·±åº¦å­¦ä¹ çš„æœ¬è´¨æ˜¯é€šè¿‡**åå‘ä¼ æ’­**ç®—æ³•è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ã€‚PyTorch çš„ `autograd` æ¨¡å—å¯ä»¥è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ã€‚

### 1. åŸºç¡€æ¦‚å¿µ

```python
# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
x = torch.tensor([2.0, 3.0], requires_grad=True)

# å®šä¹‰è®¡ç®—
y = x ** 2 + 3 * x + 1

# è®¡ç®—æ¢¯åº¦
# dy/dx = 2x + 3
# åœ¨ x=[2,3] å¤„ï¼Œdy/dx = [7, 9]
loss = y.sum()  # æ ‡é‡æŸå¤±ï¼ˆåå‘ä¼ æ’­éœ€è¦æ ‡é‡ï¼‰
loss.backward()  # è®¡ç®—æ¢¯åº¦

print(f"x: {x}")
print(f"y: {y}")
print(f"x.grad: {x.grad}")  # dy/dx
```

### 2. æ¢¯åº¦ç´¯ç§¯

```python
x = torch.tensor([1.0, 2.0], requires_grad=True)

# ç¬¬ä¸€æ¬¡è®¡ç®—
y = x ** 2
y.sum().backward()
print(f"ç¬¬ä¸€æ¬¡æ¢¯åº¦: {x.grad}")

# ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆæ¢¯åº¦ä¼šç´¯ç§¯ï¼ï¼‰
y = x ** 3
y.sum().backward()
print(f"ç´¯ç§¯åæ¢¯åº¦: {x.grad}")

# æ¸…é›¶æ¢¯åº¦
x.grad.zero_()
y = x ** 3
y.sum().backward()
print(f"æ¸…é›¶åæ¢¯åº¦: {x.grad}")
```

### 3. åœæ­¢æ¢¯åº¦è¿½è¸ª

```python
x = torch.tensor([1.0, 2.0], requires_grad=True)

# æ–¹æ³• 1ï¼štorch.no_grad()
with torch.no_grad():
    y = x * 2
    print(f"y.requires_grad: {y.requires_grad}")  # False

# æ–¹æ³• 2ï¼šdetach()
y = x * 2
y_detached = y.detach()
print(f"y_detached.requires_grad: {y_detached.requires_grad}")  # False
```

### 4. æ‰‹åŠ¨å®ç°æ¢¯åº¦ä¸‹é™

```python
# ç›®æ ‡ï¼šæ‰¾åˆ° y = (x - 3)^2 çš„æœ€å°å€¼

x = torch.tensor([0.0], requires_grad=True)
learning_rate = 0.1

for epoch in range(100):
    # å‰å‘ä¼ æ’­
    y = (x - 3) ** 2

    # åå‘ä¼ æ’­
    y.backward()

    # æ›´æ–°å‚æ•°ï¼ˆæ³¨æ„ï¼šä¸èƒ½åœ¨è®¡ç®—å›¾ä¸­æ›´æ–°ï¼‰
    with torch.no_grad():
        x -= learning_rate * x.grad

    # æ¸…é›¶æ¢¯åº¦
    x.grad.zero_()

    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: x = {x.item():.4f}, y = {y.item():.4f}")

print(f"\næœ€ä¼˜è§£: x = {x.item():.4f} (ç†è®ºå€¼: 3.0)")
```

---

## æ„å»ºç¥ç»ç½‘ç»œï¼šnn.Module

### 1. ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleNet(nn.Module):
    """
    ç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ

    æ¶æ„ï¼š
    è¾“å…¥å±‚ (784) â†’ éšè—å±‚ (128) â†’ ReLU â†’ è¾“å‡ºå±‚ (10) â†’ Softmax
    """

    def __init__(self, input_dim: int = 784, hidden_dim: int = 128, output_dim: int = 10):
        super(SimpleNet, self).__init__()

        # å®šä¹‰å±‚
        self.fc1 = nn.Linear(input_dim, hidden_dim)  # å…¨è¿æ¥å±‚ 1
        self.fc2 = nn.Linear(hidden_dim, output_dim)  # å…¨è¿æ¥å±‚ 2

    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # x: (batch_size, input_dim)

        # éšè—å±‚
        x = self.fc1(x)  # (batch_size, hidden_dim)
        x = F.relu(x)    # æ¿€æ´»å‡½æ•°

        # è¾“å‡ºå±‚
        x = self.fc2(x)  # (batch_size, output_dim)

        return x

# åˆ›å»ºæ¨¡å‹
model = SimpleNet(input_dim=784, hidden_dim=128, output_dim=10)
print(model)

# æŸ¥çœ‹å‚æ•°
print("\næ¨¡å‹å‚æ•°:")
for name, param in model.named_parameters():
    print(f"{name:15s}: {param.shape}")

# ç»Ÿè®¡å‚æ•°æ•°é‡
total_params = sum(p.numel() for p in model.parameters())
print(f"\næ€»å‚æ•°æ•°: {total_params:,}")

# å‰å‘ä¼ æ’­
x = torch.randn(32, 784)  # æ‰¹é‡å¤§å° 32
output = model(x)
print(f"\nè¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

### 2. å¸¸ç”¨å±‚

```python
# 1. å…¨è¿æ¥å±‚ï¼ˆLinearï¼‰
fc = nn.Linear(in_features=100, out_features=50)

# 2. å·ç§¯å±‚ï¼ˆConv2dï¼‰
conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)

# 3. æ± åŒ–å±‚ï¼ˆMaxPool2dï¼‰
pool = nn.MaxPool2d(kernel_size=2, stride=2)

# 4. Dropoutï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
dropout = nn.Dropout(p=0.5)

# 5. BatchNormï¼ˆæ‰¹é‡å½’ä¸€åŒ–ï¼‰
bn = nn.BatchNorm1d(num_features=100)

# 6. æ¿€æ´»å‡½æ•°
relu = nn.ReLU()
sigmoid = nn.Sigmoid()
tanh = nn.Tanh()
leaky_relu = nn.LeakyReLU(negative_slope=0.01)

# 7. å¾ªç¯å±‚
rnn = nn.RNN(input_size=100, hidden_size=50, num_layers=2)
lstm = nn.LSTM(input_size=100, hidden_size=50, num_layers=2)
gru = nn.GRU(input_size=100, hidden_size=50, num_layers=2)
```

### 3. æ›´å¤æ‚çš„ç½‘ç»œ

```python
class DeepNet(nn.Module):
    """
    æ·±å±‚ç¥ç»ç½‘ç»œï¼Œå¸¦æœ‰ Dropout å’Œ BatchNorm
    """

    def __init__(self):
        super(DeepNet, self).__init__()

        self.fc1 = nn.Linear(784, 512)
        self.bn1 = nn.BatchNorm1d(512)

        self.fc2 = nn.Linear(512, 256)
        self.bn2 = nn.BatchNorm1d(256)

        self.fc3 = nn.Linear(256, 128)
        self.bn3 = nn.BatchNorm1d(128)

        self.fc4 = nn.Linear(128, 10)

        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        # å±‚ 1
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)

        # å±‚ 2
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)

        # å±‚ 3
        x = self.fc3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.dropout(x)

        # è¾“å‡ºå±‚
        x = self.fc4(x)

        return x

model = DeepNet()
print(model)
```

### 4. ä½¿ç”¨ nn.Sequential

```python
# æ–¹æ³• 1ï¼šé¡ºåºå®šä¹‰
model = nn.Sequential(
    nn.Linear(784, 512),
    nn.ReLU(),
    nn.Dropout(0.5),
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Dropout(0.5),
    nn.Linear(256, 10)
)

# æ–¹æ³• 2ï¼šæœ‰åºå­—å…¸
from collections import OrderedDict

model = nn.Sequential(OrderedDict([
    ('fc1', nn.Linear(784, 512)),
    ('relu1', nn.ReLU()),
    ('dropout1', nn.Dropout(0.5)),
    ('fc2', nn.Linear(512, 256)),
    ('relu2', nn.ReLU()),
    ('dropout2', nn.Dropout(0.5)),
    ('fc3', nn.Linear(256, 10))
]))

print(model)
```

---

## è®­ç»ƒå¾ªç¯ï¼šå®Œæ•´çš„è®­ç»ƒæµç¨‹

### 1. æ•°æ®åŠ è½½

```python
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets, transforms

# å®šä¹‰æ•°æ®è½¬æ¢
transform = transforms.Compose([
    transforms.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
    transforms.Normalize((0.5,), (0.5,))  # å½’ä¸€åŒ–
])

# åŠ è½½ MNIST æ•°æ®é›†
train_dataset = datasets.MNIST(
    root='./data',
    train=True,
    transform=transform,
    download=True
)

test_dataset = datasets.MNIST(
    root='./data',
    train=False,
    transform=transform,
    download=True
)

# åˆ›å»º DataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    num_workers=2
)

test_loader = DataLoader(
    test_dataset,
    batch_size=64,
    shuffle=False,
    num_workers=2
)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

# æŸ¥çœ‹ä¸€ä¸ªæ‰¹æ¬¡
images, labels = next(iter(train_loader))
print(f"æ‰¹æ¬¡å›¾åƒå½¢çŠ¶: {images.shape}")  # (64, 1, 28, 28)
print(f"æ‰¹æ¬¡æ ‡ç­¾å½¢çŠ¶: {labels.shape}")  # (64,)
```

### 2. å®Œæ•´çš„è®­ç»ƒè„šæœ¬

```python
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple

class MNISTNet(nn.Module):
    """MNIST åˆ†ç±»ç½‘ç»œ"""

    def __init__(self):
        super(MNISTNet, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 10)
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        x = x.view(-1, 28 * 28)  # å±•å¹³
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

def train_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device
) -> Tuple[float, float]:
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼

    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in dataloader:
        # ç§»åŠ¨åˆ°è®¾å¤‡
        images, labels = images.to(device), labels.to(device)

        # æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        outputs = model(images)
        loss = criterion(outputs, labels)

        # åå‘ä¼ æ’­
        loss.backward()

        # æ›´æ–°å‚æ•°
        optimizer.step()

        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100.0 * correct / total

    return epoch_loss, epoch_acc

def evaluate(
    model: nn.Module,
    dataloader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> Tuple[float, float]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100.0 * correct / total

    return epoch_loss, epoch_acc

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åˆ›å»ºæ¨¡å‹
model = MNISTNet().to(device)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒ
num_epochs = 10
best_acc = 0.0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    test_loss, test_acc = evaluate(model, test_loader, criterion, device)

    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
    print(f"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_acc > best_acc:
        best_acc = test_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print(f"  â†’ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå‡†ç¡®ç‡: {best_acc:.2f}%ï¼‰")

print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
```

### 3. å­¦ä¹ ç‡è°ƒåº¦

```python
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR

# æ–¹æ³• 1ï¼šæ¯ N ä¸ª epoch é™ä½å­¦ä¹ ç‡
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# æ–¹æ³• 2ï¼šå½“æŒ‡æ ‡ä¸å†æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡
scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

# æ–¹æ³• 3ï¼šä½™å¼¦é€€ç«
scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)

    # StepLR å’Œ CosineAnnealingLR
    scheduler.step()

    # ReduceLROnPlateauï¼ˆéœ€è¦ä¼ å…¥ç›‘æ§æŒ‡æ ‡ï¼‰
    # scheduler.step(train_loss)

    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch {epoch+1}, LR: {current_lr:.6f}")
```

---

## å®æˆ˜ï¼šå›¾åƒåˆ†ç±»ä¸è¿ç§»å­¦ä¹ 

### 1. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

```python
from torchvision import models

# åŠ è½½é¢„è®­ç»ƒçš„ ResNet-18
model = models.resnet18(pretrained=True)

# å†»ç»“æ‰€æœ‰å±‚
for param in model.parameters():
    param.requires_grad = False

# æ›¿æ¢æœ€åçš„å…¨è¿æ¥å±‚ï¼ˆ1000 ç±» â†’ 10 ç±»ï¼‰
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)

# åªè®­ç»ƒæœ€åä¸€å±‚
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)

print(model)
```

### 2. è‡ªå®šä¹‰æ•°æ®é›†

```python
from torch.utils.data import Dataset
from PIL import Image
import os

class CustomImageDataset(Dataset):
    """
    è‡ªå®šä¹‰å›¾åƒæ•°æ®é›†

    ç›®å½•ç»“æ„ï¼š
    data/
        class1/
            img1.jpg
            img2.jpg
        class2/
            img3.jpg
            img4.jpg
    """

    def __init__(self, root_dir: str, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.classes = sorted(os.listdir(root_dir))
        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}

        # æ”¶é›†æ‰€æœ‰å›¾åƒè·¯å¾„å’Œæ ‡ç­¾
        self.images = []
        self.labels = []

        for class_name in self.classes:
            class_dir = os.path.join(root_dir, class_name)
            for img_name in os.listdir(class_dir):
                if img_name.endswith(('.jpg', '.png', '.jpeg')):
                    self.images.append(os.path.join(class_dir, img_name))
                    self.labels.append(self.class_to_idx[class_name])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]

        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')

        # åº”ç”¨è½¬æ¢
        if self.transform:
            image = self.transform(image)

        return image, label

# ä½¿ç”¨ç¤ºä¾‹
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# dataset = CustomImageDataset('path/to/data', transform=transform)
# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

### 3. æ•°æ®å¢å¼º

```python
from torchvision import transforms

# è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º
train_transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomCrop(224),  # éšæœºè£å‰ª
    transforms.RandomHorizontalFlip(),  # éšæœºæ°´å¹³ç¿»è½¬
    transforms.RandomRotation(15),  # éšæœºæ—‹è½¬
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # é¢œè‰²æŠ–åŠ¨
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# æµ‹è¯•æ—¶ä¸å¢å¼º
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])
```

---

## TensorFlow/Keras å¿«é€Ÿå…¥é—¨

è™½ç„¶æˆ‘ä»¬ä¸»è¦ä½¿ç”¨ PyTorchï¼Œä½†äº†è§£ TensorFlow ä¹Ÿå¾ˆæœ‰ä»·å€¼ã€‚

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# 1. æ„å»ºæ¨¡å‹ï¼ˆSequential APIï¼‰
model = keras.Sequential([
    layers.Flatten(input_shape=(28, 28)),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(10, activation='softmax')
])

# 2. ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# 3. è®­ç»ƒæ¨¡å‹
# history = model.fit(
#     x_train, y_train,
#     batch_size=64,
#     epochs=10,
#     validation_split=0.2
# )

# 4. è¯„ä¼°æ¨¡å‹
# test_loss, test_acc = model.evaluate(x_test, y_test)

# 5. é¢„æµ‹
# predictions = model.predict(x_test)
```

**Functional API**ï¼ˆæ›´çµæ´»ï¼‰ï¼š

```python
# è¾“å…¥å±‚
inputs = keras.Input(shape=(28, 28))

# éšè—å±‚
x = layers.Flatten()(inputs)
x = layers.Dense(512, activation='relu')(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.Dropout(0.2)(x)

# è¾“å‡ºå±‚
outputs = layers.Dense(10, activation='softmax')(x)

# åˆ›å»ºæ¨¡å‹
model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)
```

---

## PyTorch vs TensorFlow ä»£ç å¯¹æ¯”

### æ¨¡å‹å®šä¹‰

```python
# PyTorch
class PyTorchModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# TensorFlow/Keras
def create_keras_model():
    return keras.Sequential([
        layers.Dense(128, activation='relu', input_shape=(784,)),
        layers.Dense(10)
    ])
```

### è®­ç»ƒå¾ªç¯

```python
# PyTorchï¼ˆæ‰‹åŠ¨å¾ªç¯ï¼‰
for epoch in range(num_epochs):
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

# TensorFlow/Kerasï¼ˆè‡ªåŠ¨ï¼‰
model.fit(x_train, y_train, epochs=num_epochs, batch_size=64)
```

---

## é«˜çº§æŠ€å·§

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

# åˆ›å»ºæ¢¯åº¦ç¼©æ”¾å™¨
scaler = GradScaler()

for images, labels in train_loader:
    optimizer.zero_grad()

    # è‡ªåŠ¨æ··åˆç²¾åº¦
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)

    # ç¼©æ”¾æ¢¯åº¦
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

### 2. æ¢¯åº¦è£å‰ª

```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

### 3. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model, 'model.pth')
loaded_model = torch.load('model.pth')

# åªä¿å­˜å‚æ•°ï¼ˆæ¨èï¼‰
torch.save(model.state_dict(), 'model_weights.pth')

# åŠ è½½å‚æ•°
model = MNISTNet()
model.load_state_dict(torch.load('model_weights.pth'))
model.eval()

# ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
```

---

## å°ç»“

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

âœ… **PyTorch åŸºç¡€**
- å¼ é‡æ“ä½œå’Œ GPU åŠ é€Ÿ
- è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰
- æ¢¯åº¦è®¡ç®—å’Œä¼˜åŒ–

âœ… **æ„å»ºç¥ç»ç½‘ç»œ**
- nn.Module è®¾è®¡æ¨¡å¼
- å¸¸ç”¨å±‚å’Œæ¿€æ´»å‡½æ•°
- è‡ªå®šä¹‰ç½‘ç»œæ¶æ„

âœ… **è®­ç»ƒæµç¨‹**
- æ•°æ®åŠ è½½ï¼ˆDataset å’Œ DataLoaderï¼‰
- å®Œæ•´çš„è®­ç»ƒå¾ªç¯
- æ¨¡å‹è¯„ä¼°å’Œä¿å­˜

âœ… **å®æˆ˜åº”ç”¨**
- MNIST æ‰‹å†™æ•°å­—è¯†åˆ«
- è¿ç§»å­¦ä¹ å’Œé¢„è®­ç»ƒæ¨¡å‹
- è‡ªå®šä¹‰æ•°æ®é›†

âœ… **TensorFlow/Keras**
- Sequential å’Œ Functional API
- ä¸ PyTorch çš„å¯¹æ¯”

âœ… **é«˜çº§æŠ€å·§**
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦è£å‰ª
- å­¦ä¹ ç‡è°ƒåº¦

---

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

1. åˆ›å»ºä¸€ä¸ª 3x4 çš„éšæœºå¼ é‡ï¼Œè®¡ç®—å…¶è½¬ç½®å¹¶éªŒè¯å½¢çŠ¶
2. å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œæ‹Ÿåˆ y = 2x + 1
3. ä½¿ç”¨ nn.Sequential æ„å»ºä¸€ä¸ªä¸‰å±‚å…¨è¿æ¥ç½‘ç»œ

### è¿›é˜¶é¢˜

4. åœ¨ MNIST æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
5. å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ Dataset ç±»ï¼ŒåŠ è½½æœ¬åœ°å›¾åƒæ–‡ä»¶
6. ä½¿ç”¨é¢„è®­ç»ƒçš„ ResNet è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œåœ¨ CIFAR-10 ä¸Šå¾®è°ƒ

### æŒ‘æˆ˜é¢˜

7. å®ç°å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰+ ä½™å¼¦é€€ç«è°ƒåº¦å™¨
8. å¯¹æ¯”ä¸åŒä¼˜åŒ–å™¨ï¼ˆSGDã€Adamã€AdamWï¼‰åœ¨åŒä¸€ä»»åŠ¡ä¸Šçš„æ€§èƒ½
9. å®ç°æ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹é‡å¤§å°

---

**ä¸‹ä¸€èŠ‚ï¼š[10.5 ç¥ç»ç½‘ç»œåŸç†ï¼šä»æ„ŸçŸ¥æœºåˆ° Transformer](10.5-ç¥ç»ç½‘ç»œåŸç†ï¼šä»æ„ŸçŸ¥æœºåˆ°Transformer.md)**

åœ¨ä¸‹ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†æ·±å…¥ç¥ç»ç½‘ç»œçš„æ•°å­¦åŸç†ï¼Œç†è§£åå‘ä¼ æ’­ç®—æ³•ï¼Œå¹¶æœ€ç»ˆå®ç° Transformer æ¶æ„â€”â€”ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºçŸ³ï¼
