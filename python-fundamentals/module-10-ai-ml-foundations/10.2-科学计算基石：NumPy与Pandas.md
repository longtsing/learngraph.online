# 10.2 ç§‘å­¦è®¡ç®—åŸºçŸ³ï¼šNumPy ä¸ Pandas

## ä¸ºä»€ä¹ˆä» NumPy å¼€å§‹ï¼Ÿ

åœ¨æ·±å…¥ AI/ML ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»ç†è§£ä¸€ä¸ªæ ¹æœ¬æ€§çš„é—®é¢˜ï¼š**ä¸ºä»€ä¹ˆæ‰€æœ‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½å»ºç«‹åœ¨ NumPy ä¹‹ä¸Šï¼Ÿ**

> **ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ**ï¼šç°ä»£ AI çš„æœ¬è´¨æ˜¯**å¼ é‡è¿ç®—**ã€‚å›¾åƒæ˜¯å¼ é‡ï¼Œæ–‡æœ¬æ˜¯å¼ é‡ï¼Œç¥ç»ç½‘ç»œçš„æƒé‡æ˜¯å¼ é‡ï¼Œæ¢¯åº¦ä¹Ÿæ˜¯å¼ é‡ã€‚è€Œ NumPy æä¾›äº† Python ä¸­æœ€é«˜æ•ˆçš„å¤šç»´æ•°ç»„ï¼ˆå¼ é‡ï¼‰å®ç°ã€‚

ä» PyTorch çš„ `torch.Tensor` åˆ° TensorFlow çš„ `tf.Tensor`ï¼Œå®ƒä»¬çš„ API è®¾è®¡éƒ½æ·±å— NumPy å½±å“ã€‚æŒæ¡ NumPyï¼Œä½ å°±æŒæ¡äº†æ•´ä¸ª AI æŠ€æœ¯æ ˆçš„é€šç”¨è¯­è¨€ã€‚

## NumPyï¼šå¤šç»´æ•°ç»„çš„è‰ºæœ¯

### ä»€ä¹ˆæ˜¯å¼ é‡ï¼ˆTensorï¼‰ï¼Ÿ

åœ¨æ•°å­¦ä¸­ï¼š
- **æ ‡é‡ï¼ˆScalarï¼‰**ï¼š0 ç»´å¼ é‡ï¼Œä¾‹å¦‚ `5`, `3.14`
- **å‘é‡ï¼ˆVectorï¼‰**ï¼š1 ç»´å¼ é‡ï¼Œä¾‹å¦‚ `[1, 2, 3]`
- **çŸ©é˜µï¼ˆMatrixï¼‰**ï¼š2 ç»´å¼ é‡ï¼Œä¾‹å¦‚ `[[1, 2], [3, 4]]`
- **å¼ é‡ï¼ˆTensorï¼‰**ï¼šn ç»´æ•°ç»„ï¼Œä¾‹å¦‚ 3Dã€4D...

åœ¨ AI ä¸­ï¼š
- **æ ‡é‡**ï¼šæŸå¤±å€¼ã€å­¦ä¹ ç‡
- **å‘é‡**ï¼šå•è¯åµŒå…¥ï¼ˆword embeddingï¼‰
- **çŸ©é˜µ**ï¼šç¥ç»ç½‘ç»œæƒé‡
- **3D å¼ é‡**ï¼šä¸€æ‰¹å¥å­çš„åµŒå…¥ï¼ˆbatch_size, seq_len, embedding_dimï¼‰
- **4D å¼ é‡**ï¼šä¸€æ‰¹å›¾åƒï¼ˆbatch_size, height, width, channelsï¼‰

### NumPy æ•°ç»„åˆ›å»º

```python
import numpy as np
from typing import Tuple

# 1. ä» Python åˆ—è¡¨åˆ›å»º
arr_1d = np.array([1, 2, 3, 4, 5])
arr_2d = np.array([[1, 2, 3], [4, 5, 6]])

print(f"1D æ•°ç»„å½¢çŠ¶: {arr_1d.shape}")  # (5,)
print(f"2D æ•°ç»„å½¢çŠ¶: {arr_2d.shape}")  # (2, 3)

# 2. åˆ›å»ºç‰¹æ®Šæ•°ç»„
zeros = np.zeros((3, 4))  # 3x4 çš„é›¶çŸ©é˜µ
ones = np.ones((2, 3, 4))  # 2x3x4 çš„å…¨ 1 å¼ é‡
identity = np.eye(5)  # 5x5 å•ä½çŸ©é˜µ

# 3. åˆ›å»ºæ•°å€¼èŒƒå›´
arange = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
linspace = np.linspace(0, 1, 5)  # [0.0, 0.25, 0.5, 0.75, 1.0]

# 4. éšæœºæ•°ç»„ï¼ˆåœ¨ ML ä¸­ç”¨äºåˆå§‹åŒ–æƒé‡ï¼‰
np.random.seed(42)  # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å¯é‡ç°æ€§
random_normal = np.random.randn(3, 4)  # æ ‡å‡†æ­£æ€åˆ†å¸ƒ
random_uniform = np.random.rand(3, 4)  # [0, 1) å‡åŒ€åˆ†å¸ƒ

# ğŸ’¡ Xavier åˆå§‹åŒ–ï¼ˆæ·±åº¦å­¦ä¹ å¸¸ç”¨ï¼‰
def xavier_init(shape: Tuple[int, ...]) -> np.ndarray:
    """
    Xavier/Glorot åˆå§‹åŒ–ï¼šç”¨äºç¥ç»ç½‘ç»œæƒé‡åˆå§‹åŒ–

    åŸç†ï¼šä¿æŒå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ—¶æ–¹å·®ä¸€è‡´
    å…¬å¼ï¼šW ~ N(0, 2/(n_in + n_out))
    """
    fan_in, fan_out = shape[0], shape[1]
    std = np.sqrt(2.0 / (fan_in + fan_out))
    return np.random.randn(*shape) * std

weights = xavier_init((128, 64))  # 128 è¾“å…¥ï¼Œ64 è¾“å‡º
print(f"æƒé‡å½¢çŠ¶: {weights.shape}, å‡å€¼: {weights.mean():.4f}, æ ‡å‡†å·®: {weights.std():.4f}")
```

### ä¸ºä»€ä¹ˆ NumPy æ¯” Python åˆ—è¡¨å¿« 100 å€ï¼Ÿ

```python
import time

# Python åˆ—è¡¨çš„æ–¹å¼
def python_sum(n: int) -> float:
    """ä½¿ç”¨ Python åˆ—è¡¨æ±‚å’Œ"""
    data = list(range(n))
    start = time.time()
    result = sum([x**2 for x in data])
    return time.time() - start

# NumPy çš„æ–¹å¼
def numpy_sum(n: int) -> float:
    """ä½¿ç”¨ NumPy å‘é‡åŒ–æ±‚å’Œ"""
    data = np.arange(n)
    start = time.time()
    result = np.sum(data**2)
    return time.time() - start

n = 1_000_000
python_time = python_sum(n)
numpy_time = numpy_sum(n)

print(f"Python åˆ—è¡¨: {python_time:.4f} ç§’")
print(f"NumPy æ•°ç»„: {numpy_time:.4f} ç§’")
print(f"é€Ÿåº¦æå‡: {python_time / numpy_time:.1f}x")
```

**ä¸ºä»€ä¹ˆè¿™ä¹ˆå¿«ï¼Ÿ**

1. **å‘é‡åŒ–ï¼ˆVectorizationï¼‰**ï¼šNumPy ç”¨ C è¯­è¨€å®ç°ï¼Œé¿å…äº† Python è§£é‡Šå™¨å¼€é”€
2. **è¿ç»­å†…å­˜**ï¼šæ•°ç»„å…ƒç´ åœ¨å†…å­˜ä¸­è¿ç»­å­˜å‚¨ï¼ŒCPU ç¼“å­˜å‹å¥½
3. **SIMD æŒ‡ä»¤**ï¼šç°ä»£ CPU å¯ä»¥ä¸€æ¬¡å¤„ç†å¤šä¸ªæ•°æ®ï¼ˆSingle Instruction, Multiple Dataï¼‰

> **ğŸ”— ä¸ AI çš„è”ç³»**ï¼šè®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°ç™¾ä¸‡ä¸ªå‚æ•°è¿›è¡ŒçŸ©é˜µè¿ç®—ã€‚å¦‚æœä½¿ç”¨ Python å¾ªç¯ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹å¯èƒ½éœ€è¦å‡ ä¸ªæœˆï¼›ç”¨ NumPy/PyTorchï¼Œåªéœ€è¦å‡ å°æ—¶ã€‚

### å¹¿æ’­æœºåˆ¶ï¼ˆBroadcastingï¼‰â€”â€”NumPy çš„è®¾è®¡å“²å­¦

å¹¿æ’­æ˜¯ NumPy æœ€å¼ºå¤§ä¹Ÿæœ€å®¹æ˜“è¢«è¯¯è§£çš„ç‰¹æ€§ï¼š

```python
# 1. æ ‡é‡ä¸æ•°ç»„
arr = np.array([1, 2, 3, 4])
result = arr + 10  # æ¯ä¸ªå…ƒç´ åŠ  10
print(result)  # [11, 12, 13, 14]

# 2. ä¸åŒå½¢çŠ¶çš„æ•°ç»„ç›¸åŠ 
a = np.array([[1, 2, 3],
              [4, 5, 6]])  # (2, 3)
b = np.array([10, 20, 30])  # (3,)

# å¹¿æ’­è§„åˆ™ï¼šb è¢«"å¤åˆ¶"ä¸º [[10, 20, 30], [10, 20, 30]]
result = a + b
print(result)
# [[11, 22, 33]
#  [14, 25, 36]]

# 3. åœ¨ AI ä¸­çš„åº”ç”¨ï¼šæ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰
batch_data = np.random.randn(32, 128)  # 32 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª 128 ç»´
mean = batch_data.mean(axis=0, keepdims=True)  # (1, 128)
std = batch_data.std(axis=0, keepdims=True)    # (1, 128)

# å¹¿æ’­ï¼šæ¯ä¸ªæ ·æœ¬å‡å»å‡å€¼å¹¶é™¤ä»¥æ ‡å‡†å·®
normalized = (batch_data - mean) / std
print(f"å½’ä¸€åŒ–åå‡å€¼: {normalized.mean():.6f}, æ ‡å‡†å·®: {normalized.std():.6f}")
```

**å¹¿æ’­çš„ä¸‰æ¡è§„åˆ™**ï¼š

1. å¦‚æœä¸¤ä¸ªæ•°ç»„ç»´åº¦ä¸åŒï¼Œåœ¨è¾ƒå°æ•°ç»„çš„å½¢çŠ¶å‰é¢å¡«å…… 1
2. å¯¹äºä»»æ„ç»´åº¦ï¼Œå¦‚æœå…¶ä¸­ä¸€ä¸ªæ•°ç»„è¯¥ç»´åº¦å¤§å°ä¸º 1ï¼Œåˆ™"æ‹‰ä¼¸"åˆ°åŒ¹é…å¦ä¸€ä¸ªæ•°ç»„
3. å¦‚æœä»»æ„ç»´åº¦å¤§å°ä¸åŒ¹é…ä¸”éƒ½ä¸ä¸º 1ï¼Œåˆ™æŠ¥é”™

```python
# ğŸ“ å¹¿æ’­è§„åˆ™ç¤ºä¾‹
# A: (3, 1) + B: (1, 4) â†’ ç»“æœ: (3, 4)
A = np.array([[1], [2], [3]])  # (3, 1)
B = np.array([[10, 20, 30, 40]])  # (1, 4)
result = A + B  # (3, 4)
print(result)
# [[11, 21, 31, 41]
#  [12, 22, 32, 42]
#  [13, 23, 33, 43]]

# âš ï¸ å¸¸è§é™·é˜±ï¼šç»´åº¦ä¸åŒ¹é…
try:
    A = np.ones((3, 4))
    B = np.ones((3, 5))
    C = A + B  # æŠ¥é”™ï¼
except ValueError as e:
    print(f"é”™è¯¯: {e}")
```

### é«˜çº§ç´¢å¼•ä¸åˆ‡ç‰‡

```python
# 1. åŸºç¡€ç´¢å¼•
arr = np.array([[1, 2, 3, 4],
                [5, 6, 7, 8],
                [9, 10, 11, 12]])

# å–ç¬¬ 2 è¡Œç¬¬ 3 åˆ—
print(arr[1, 2])  # 7

# å–ç¬¬ 1 è¡Œçš„æ‰€æœ‰åˆ—
print(arr[0, :])  # [1, 2, 3, 4]

# å–æ‰€æœ‰è¡Œçš„ç¬¬ 2 åˆ—
print(arr[:, 1])  # [2, 6, 10]

# 2. å¸ƒå°”ç´¢å¼•ï¼ˆåœ¨æ•°æ®æ¸…æ´—ä¸­æå…¶é‡è¦ï¼‰
data = np.array([1, -2, 3, -4, 5, -6])
positive = data[data > 0]  # åªä¿ç•™æ­£æ•°
print(positive)  # [1, 3, 5]

# 3. èŠ±å¼ç´¢å¼•
indices = [0, 2, 3]
selected = arr[indices, :]  # é€‰æ‹©ç¬¬ 0, 2, 3 è¡Œ
print(selected)

# ğŸ”¬ å®é™…åº”ç”¨ï¼šä»æ•°æ®é›†ä¸­é€‰æ‹©ç‰¹å®šæ ·æœ¬
dataset = np.random.randn(1000, 784)  # 1000 å¼  28x28 å›¾åƒï¼ˆå±•å¹³ï¼‰
labels = np.random.randint(0, 10, 1000)  # æ ‡ç­¾

# åªé€‰æ‹©æ ‡ç­¾ä¸º 3 çš„æ ·æœ¬
class_3_mask = (labels == 3)
class_3_images = dataset[class_3_mask]
print(f"æ ‡ç­¾ä¸º 3 çš„æ ·æœ¬æ•°: {len(class_3_images)}")
```

### çŸ©é˜µè¿ç®—â€”â€”æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒ

```python
# 1. ç‚¹ç§¯ï¼ˆDot Productï¼‰
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
dot_product = np.dot(a, b)  # 1*4 + 2*5 + 3*6 = 32
print(f"ç‚¹ç§¯: {dot_product}")

# 2. çŸ©é˜µä¹˜æ³•ï¼ˆMatrix Multiplicationï¼‰
# è¿™æ˜¯ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­çš„åŸºç¡€æ“ä½œ
X = np.random.randn(64, 128)  # 64 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª 128 ç»´ç‰¹å¾
W = np.random.randn(128, 10)  # æƒé‡çŸ©é˜µï¼š128 è¾“å…¥ï¼Œ10 è¾“å‡º
b = np.random.randn(10)  # åç½®

# å‰å‘ä¼ æ’­ï¼šy = XW + b
y = np.dot(X, W) + b  # (64, 128) @ (128, 10) = (64, 10)
print(f"è¾“å‡ºå½¢çŠ¶: {y.shape}")

# 3. ä½¿ç”¨ @ è¿ç®—ç¬¦ï¼ˆPython 3.5+ï¼‰
y = X @ W + b  # ç­‰ä»·äº np.dot(X, W) + b

# ğŸ“ çŸ©é˜µç»´åº¦è§„åˆ™
# (m, n) @ (n, p) = (m, p)
# ç¬¬ä¸€ä¸ªçŸ©é˜µçš„åˆ—æ•°å¿…é¡»ç­‰äºç¬¬äºŒä¸ªçŸ©é˜µçš„è¡Œæ•°

# 4. è½¬ç½®ï¼ˆTransposeï¼‰
A = np.array([[1, 2, 3],
              [4, 5, 6]])  # (2, 3)
A_T = A.T  # (3, 2)
print(A_T)

# 5. æ›´é«˜ç»´çš„è½¬ç½®
tensor_3d = np.random.randn(32, 10, 128)  # (batch, seq_len, hidden)
# äº¤æ¢ seq_len å’Œ hidden ç»´åº¦
tensor_transposed = tensor_3d.transpose(0, 2, 1)  # (32, 128, 10)
print(f"è½¬ç½®åå½¢çŠ¶: {tensor_transposed.shape}")
```

### å®æˆ˜ï¼šå®ç°å…¨è¿æ¥å±‚çš„å‰å‘ä¼ æ’­

è®©æˆ‘ä»¬ä»é›¶å®ç°ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç»„ä»¶ï¼š

```python
from typing import Optional

class DenseLayer:
    """
    å…¨è¿æ¥å±‚ï¼ˆDense Layer / Fully Connected Layerï¼‰

    æ•°å­¦å…¬å¼ï¼šy = Ïƒ(Wx + b)
    å…¶ä¸­ï¼š
        - W: æƒé‡çŸ©é˜µ (input_dim, output_dim)
        - x: è¾“å…¥å‘é‡ (batch_size, input_dim)
        - b: åç½®å‘é‡ (output_dim,)
        - Ïƒ: æ¿€æ´»å‡½æ•°
    """

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        activation: str = "relu"
    ):
        """
        åˆå§‹åŒ–å…¨è¿æ¥å±‚

        Args:
            input_dim: è¾“å…¥ç»´åº¦
            output_dim: è¾“å‡ºç»´åº¦
            activation: æ¿€æ´»å‡½æ•° ("relu", "sigmoid", "tanh")
        """
        # Xavier åˆå§‹åŒ–
        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)
        self.b = np.zeros(output_dim)
        self.activation = activation

    def forward(self, X: np.ndarray) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­

        Args:
            X: è¾“å…¥æ•°æ® (batch_size, input_dim)

        Returns:
            è¾“å‡ºæ•°æ® (batch_size, output_dim)
        """
        # çº¿æ€§å˜æ¢
        Z = X @ self.W + self.b  # (batch_size, output_dim)

        # æ¿€æ´»å‡½æ•°
        if self.activation == "relu":
            A = np.maximum(0, Z)  # ReLU: max(0, x)
        elif self.activation == "sigmoid":
            A = 1 / (1 + np.exp(-Z))  # Sigmoid: 1/(1+e^-x)
        elif self.activation == "tanh":
            A = np.tanh(Z)  # Tanh: (e^x - e^-x)/(e^x + e^-x)
        else:
            A = Z  # çº¿æ€§æ¿€æ´»

        return A

# æµ‹è¯•å…¨è¿æ¥å±‚
layer = DenseLayer(input_dim=128, output_dim=64, activation="relu")
X_input = np.random.randn(32, 128)  # 32 ä¸ªæ ·æœ¬
output = layer.forward(X_input)
print(f"è¾“å…¥å½¢çŠ¶: {X_input.shape}, è¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"è¾“å‡ºå‡å€¼: {output.mean():.4f}, æ ‡å‡†å·®: {output.std():.4f}")
```

### è½´ï¼ˆAxisï¼‰æ“ä½œâ€”â€”ç†è§£å¤šç»´æ•°ç»„çš„å…³é”®

```python
# åˆ›å»º 3D æ•°ç»„ï¼š(æ‰¹æ¬¡, åºåˆ—é•¿åº¦, ç‰¹å¾ç»´åº¦)
data = np.random.randn(4, 5, 3)  # 4 ä¸ªå¥å­ï¼Œæ¯ä¸ª 5 ä¸ªè¯ï¼Œæ¯ä¸ªè¯ 3 ç»´åµŒå…¥

print(f"åŸå§‹å½¢çŠ¶: {data.shape}")  # (4, 5, 3)

# axis=0: æ²¿ç€æ‰¹æ¬¡ç»´åº¦æ“ä½œ
mean_over_batch = data.mean(axis=0)  # (5, 3) - æ¯ä¸ªä½ç½®çš„å¹³å‡åµŒå…¥
print(f"axis=0 (æ‰¹æ¬¡ç»´åº¦): {mean_over_batch.shape}")

# axis=1: æ²¿ç€åºåˆ—ç»´åº¦æ“ä½œ
mean_over_seq = data.mean(axis=1)  # (4, 3) - æ¯ä¸ªå¥å­çš„å¹³å‡åµŒå…¥
print(f"axis=1 (åºåˆ—ç»´åº¦): {mean_over_seq.shape}")

# axis=2: æ²¿ç€ç‰¹å¾ç»´åº¦æ“ä½œ
mean_over_features = data.mean(axis=2)  # (4, 5) - æ¯ä¸ªè¯çš„ç‰¹å¾å‡å€¼
print(f"axis=2 (ç‰¹å¾ç»´åº¦): {mean_over_features.shape}")

# axis=(0, 1): åŒæ—¶æ²¿ç€å¤šä¸ªç»´åº¦
mean_over_batch_seq = data.mean(axis=(0, 1))  # (3,) - æ•´ä½“ç‰¹å¾å‡å€¼
print(f"axis=(0,1): {mean_over_batch_seq.shape}")

# ğŸ”— å®é™…åº”ç”¨ï¼šæ± åŒ–æ“ä½œï¼ˆPoolingï¼‰
# åœ¨ CNN ä¸­ï¼Œå…¨å±€å¹³å‡æ± åŒ–ï¼ˆGlobal Average Poolingï¼‰ç”¨äºé™ç»´
image_batch = np.random.randn(32, 64, 64, 3)  # (batch, height, width, channels)
global_avg_pool = image_batch.mean(axis=(1, 2))  # (32, 3)
print(f"å…¨å±€å¹³å‡æ± åŒ–å: {global_avg_pool.shape}")
```

---

## Pandasï¼šæ•°æ®åˆ†æçš„æ ‡å‡†å·¥å…·

NumPy æä¾›äº†é«˜æ•ˆçš„æ•°å€¼è®¡ç®—ï¼Œä½†å¤„ç†çœŸå®ä¸–ç•Œçš„æ•°æ®ï¼ˆCSVã€Excelã€SQLï¼‰æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ›´é«˜çº§çš„å·¥å…·â€”â€”Pandasã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ Pandasï¼Ÿ

```python
import pandas as pd

# å‡è®¾æˆ‘ä»¬æœ‰å®¢æˆ·æµå¤±æ•°æ®
data = {
    'customer_id': [1, 2, 3, 4, 5],
    'age': [25, 35, None, 42, 28],  # æ³¨æ„ï¼šæœ‰ç¼ºå¤±å€¼
    'income': [50000, 75000, 60000, 95000, 55000],
    'churn': [0, 1, 0, 1, 0]  # 0=ç•™å­˜, 1=æµå¤±
}

# åˆ›å»º DataFrame
df = pd.DataFrame(data)
print(df)
```

è¾“å‡ºï¼š
```
   customer_id   age  income  churn
0            1  25.0   50000      0
1            2  35.0   75000      1
2            3   NaN   60000      0
3            4  42.0   95000      1
4            5  28.0   55000      0
```

### DataFrame æ ¸å¿ƒæ“ä½œ

```python
# 1. æ•°æ®åŠ è½½
# CSV
df_csv = pd.read_csv('data.csv')

# Excel
df_excel = pd.read_excel('data.xlsx')

# JSON
df_json = pd.read_json('data.json')

# SQL
# df_sql = pd.read_sql('SELECT * FROM customers', connection)

# 2. æ•°æ®æ¢ç´¢
print(df.head(3))  # å‰ 3 è¡Œ
print(df.tail(3))  # å 3 è¡Œ
print(df.info())  # æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼ä¿¡æ¯
print(df.describe())  # ç»Ÿè®¡æ‘˜è¦

# 3. é€‰æ‹©æ•°æ®
# é€‰æ‹©åˆ—
ages = df['age']
subset = df[['age', 'income']]

# é€‰æ‹©è¡Œ
first_row = df.iloc[0]  # æŒ‰ä½ç½®
filtered = df[df['age'] > 30]  # æŒ‰æ¡ä»¶

# 4. ç¼ºå¤±å€¼å¤„ç†
# æ£€æŸ¥ç¼ºå¤±å€¼
print(df.isnull().sum())

# å¡«å……ç¼ºå¤±å€¼
df_filled = df.fillna(df['age'].mean())  # ç”¨å‡å€¼å¡«å……

# åˆ é™¤ç¼ºå¤±å€¼
df_dropped = df.dropna()

# 5. æ•°æ®è½¬æ¢
df['age_group'] = pd.cut(
    df['age'],
    bins=[0, 30, 40, 100],
    labels=['young', 'middle', 'senior']
)

# 6. åˆ†ç»„èšåˆ
churn_by_age = df.groupby('age_group')['churn'].mean()
print(churn_by_age)
```

### ç‰¹å¾å·¥ç¨‹å®æˆ˜

```python
# åŠ è½½çœŸå®æ•°æ®é›†ç¤ºä¾‹ï¼ˆTitanic æ•°æ®é›†ï¼‰
# æˆ‘ä»¬ç”¨ä»£ç æ¨¡æ‹Ÿæ•°æ®
titanic_data = {
    'PassengerId': range(1, 6),
    'Pclass': [3, 1, 3, 1, 3],
    'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley',
             'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath',
             'Allen, Mr. William Henry'],
    'Sex': ['male', 'female', 'female', 'female', 'male'],
    'Age': [22, 38, 26, 35, 35],
    'SibSp': [1, 1, 0, 1, 0],
    'Parch': [0, 0, 0, 0, 0],
    'Fare': [7.25, 71.28, 7.92, 53.10, 8.05],
    'Survived': [0, 1, 1, 1, 0]
}

df_titanic = pd.DataFrame(titanic_data)

# ç‰¹å¾å·¥ç¨‹æ­¥éª¤
def engineer_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Titanic æ•°æ®é›†çš„ç‰¹å¾å·¥ç¨‹

    Returns:
        å¤„ç†åçš„ DataFrame
    """
    df = df.copy()

    # 1. ä»å§“åä¸­æå–ç§°è°“
    df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\.', expand=False)

    # 2. åˆ›å»ºå®¶åº­è§„æ¨¡ç‰¹å¾
    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1

    # 3. åˆ›å»ºæ˜¯å¦ç‹¬è‡ªä¸€äººçš„æ ‡è®°
    df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

    # 4. å¹´é¾„åˆ†æ®µ
    df['AgeGroup'] = pd.cut(
        df['Age'],
        bins=[0, 12, 18, 60, 100],
        labels=['Child', 'Teen', 'Adult', 'Senior']
    )

    # 5. ç¥¨ä»·åˆ†æ®µ
    df['FareGroup'] = pd.qcut(
        df['Fare'],
        q=4,
        labels=['Low', 'Medium', 'High', 'Very High']
    )

    # 6. æ€§åˆ«ç¼–ç 
    df['Sex_encoded'] = df['Sex'].map({'male': 0, 'female': 1})

    return df

df_engineered = engineer_features(df_titanic)
print(df_engineered[['Name', 'Title', 'FamilySize', 'IsAlone', 'AgeGroup']])
```

### å®æˆ˜ï¼šå®Œæ•´çš„æ•°æ®é¢„å¤„ç† Pipeline

```python
from typing import List
from sklearn.preprocessing import StandardScaler, LabelEncoder

class DataPreprocessor:
    """
    é€šç”¨æ•°æ®é¢„å¤„ç†ç±»

    åŒ…å«ï¼š
    - ç¼ºå¤±å€¼å¤„ç†
    - æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
    - ç±»åˆ«ç‰¹å¾ç¼–ç 
    - ç‰¹å¾å·¥ç¨‹
    """

    def __init__(self):
        self.scalers = {}
        self.encoders = {}

    def fit_transform(
        self,
        df: pd.DataFrame,
        numerical_cols: List[str],
        categorical_cols: List[str],
        target_col: str
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒæ•°æ®

        Args:
            df: åŸå§‹ DataFrame
            numerical_cols: æ•°å€¼åˆ—ååˆ—è¡¨
            categorical_cols: ç±»åˆ«åˆ—ååˆ—è¡¨
            target_col: ç›®æ ‡å˜é‡åˆ—å

        Returns:
            ç‰¹å¾çŸ©é˜µ X å’Œç›®æ ‡å‘é‡ y
        """
        df = df.copy()

        # 1. å¤„ç†æ•°å€¼ç‰¹å¾
        for col in numerical_cols:
            # å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¸­ä½æ•°ï¼‰
            median_value = df[col].median()
            df[col].fillna(median_value, inplace=True)

            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            df[col] = scaler.fit_transform(df[[col]])
            self.scalers[col] = scaler

        # 2. å¤„ç†ç±»åˆ«ç‰¹å¾
        for col in categorical_cols:
            # å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨ä¼—æ•°ï¼‰
            mode_value = df[col].mode()[0]
            df[col].fillna(mode_value, inplace=True)

            # æ ‡ç­¾ç¼–ç 
            encoder = LabelEncoder()
            df[col] = encoder.fit_transform(df[col])
            self.encoders[col] = encoder

        # 3. åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        feature_cols = numerical_cols + categorical_cols
        X = df[feature_cols].values
        y = df[target_col].values

        return X, y

    def transform(
        self,
        df: pd.DataFrame,
        numerical_cols: List[str],
        categorical_cols: List[str]
    ) -> np.ndarray:
        """è½¬æ¢æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨å·²æ‹Ÿåˆçš„è½¬æ¢å™¨ï¼‰"""
        df = df.copy()

        for col in numerical_cols:
            median_value = df[col].median()
            df[col].fillna(median_value, inplace=True)
            df[col] = self.scalers[col].transform(df[[col]])

        for col in categorical_cols:
            mode_value = df[col].mode()[0]
            df[col].fillna(mode_value, inplace=True)
            df[col] = self.encoders[col].transform(df[col])

        feature_cols = numerical_cols + categorical_cols
        return df[feature_cols].values

# ä½¿ç”¨ç¤ºä¾‹
preprocessor = DataPreprocessor()
X_train, y_train = preprocessor.fit_transform(
    df_titanic,
    numerical_cols=['Age', 'Fare'],
    categorical_cols=['Sex', 'Pclass'],
    target_col='Survived'
)

print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X_train.shape}")
print(f"ç›®æ ‡å‘é‡å½¢çŠ¶: {y_train.shape}")
```

---

## SciPyï¼šç§‘å­¦è®¡ç®—çš„è¡¥å……å·¥å…·

```python
from scipy import stats, optimize, signal
from scipy.spatial.distance import cosine, euclidean

# 1. ç»Ÿè®¡æµ‹è¯•
# t æ£€éªŒï¼šæ¯”è¾ƒä¸¤ç»„æ•°æ®çš„å‡å€¼æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
group_a = np.random.randn(100) + 5  # å‡å€¼çº¦ä¸º 5
group_b = np.random.randn(100) + 5.5  # å‡å€¼çº¦ä¸º 5.5

t_statistic, p_value = stats.ttest_ind(group_a, group_b)
print(f"t ç»Ÿè®¡é‡: {t_statistic:.4f}, p å€¼: {p_value:.4f}")

if p_value < 0.05:
    print("ä¸¤ç»„æœ‰æ˜¾è‘—å·®å¼‚ï¼ˆp < 0.05ï¼‰")
else:
    print("ä¸¤ç»„æ— æ˜¾è‘—å·®å¼‚")

# 2. è·ç¦»è®¡ç®—ï¼ˆåœ¨æ¨èç³»ç»Ÿå’Œç›¸ä¼¼åº¦è®¡ç®—ä¸­å¸¸ç”¨ï¼‰
vec1 = np.random.randn(128)  # ç”¨æˆ· 1 çš„åµŒå…¥
vec2 = np.random.randn(128)  # ç”¨æˆ· 2 çš„åµŒå…¥

# ä½™å¼¦ç›¸ä¼¼åº¦
cos_sim = 1 - cosine(vec1, vec2)  # scipy è¿”å›è·ç¦»ï¼Œ1-è·ç¦»=ç›¸ä¼¼åº¦
print(f"ä½™å¼¦ç›¸ä¼¼åº¦: {cos_sim:.4f}")

# æ¬§æ°è·ç¦»
eucl_dist = euclidean(vec1, vec2)
print(f"æ¬§æ°è·ç¦»: {eucl_dist:.4f}")

# 3. ä¼˜åŒ–é—®é¢˜
# ç¤ºä¾‹ï¼šæœ€å°åŒ–äºŒæ¬¡å‡½æ•° f(x) = x^2 + 10*sin(x)
def objective(x):
    return x**2 + 10 * np.sin(x)

result = optimize.minimize(objective, x0=0)
print(f"æœ€ä¼˜è§£: x = {result.x[0]:.4f}, f(x) = {result.fun:.4f}")
```

---

## ç»¼åˆå®æˆ˜ï¼šæ„å»ºç‰¹å¾å·¥ç¨‹ Pipeline

è®©æˆ‘ä»¬æ•´åˆæ‰€æœ‰çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š

```python
import numpy as np
import pandas as pd
from typing import Tuple, Dict, Any
from sklearn.model_selection import train_test_split

class MLDataPipeline:
    """
    æœºå™¨å­¦ä¹ æ•°æ®å¤„ç† Pipeline

    åŠŸèƒ½ï¼š
    - æ•°æ®åŠ è½½
    - æ¢ç´¢æ€§æ•°æ®åˆ†æ
    - ç‰¹å¾å·¥ç¨‹
    - æ•°æ®åˆ†å‰²
    """

    def __init__(self, data_path: str):
        self.data_path = data_path
        self.df = None
        self.preprocessor = None

    def load_data(self) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        # å®é™…åº”ç”¨ä¸­ä»æ–‡ä»¶åŠ è½½
        # self.df = pd.read_csv(self.data_path)

        # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿæ•°æ®
        np.random.seed(42)
        n_samples = 1000

        self.df = pd.DataFrame({
            'age': np.random.randint(18, 70, n_samples),
            'income': np.random.randint(30000, 150000, n_samples),
            'credit_score': np.random.randint(300, 850, n_samples),
            'num_products': np.random.randint(1, 5, n_samples),
            'tenure_months': np.random.randint(0, 120, n_samples),
            'is_active': np.random.choice([0, 1], n_samples),
            'country': np.random.choice(['US', 'UK', 'DE', 'FR'], n_samples),
            'churn': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
        })

        # éšæœºæ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
        mask = np.random.rand(*self.df.shape) < 0.05
        self.df = self.df.mask(mask)

        return self.df

    def eda(self) -> Dict[str, Any]:
        """æ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        report = {
            'shape': self.df.shape,
            'missing': self.df.isnull().sum().to_dict(),
            'statistics': self.df.describe().to_dict(),
            'churn_rate': self.df['churn'].mean()
        }
        return report

    def engineer_features(self) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        df = self.df.copy()

        # 1. åˆ›å»ºäº¤äº’ç‰¹å¾
        df['income_per_product'] = df['income'] / (df['num_products'] + 1)

        # 2. åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
        df['age_squared'] = df['age'] ** 2

        # 3. åˆ›å»ºåˆ†ç®±ç‰¹å¾
        df['age_group'] = pd.cut(
            df['age'],
            bins=[0, 30, 50, 100],
            labels=[0, 1, 2]
        ).astype(float)

        # 4. åˆ›å»ºæ¯”ç‡ç‰¹å¾
        df['products_per_month'] = df['num_products'] / (df['tenure_months'] + 1)

        return df

    def prepare_for_training(
        self,
        test_size: float = 0.2
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # ç‰¹å¾å·¥ç¨‹
        df_engineered = self.engineer_features()

        # å®šä¹‰ç‰¹å¾åˆ—
        numerical_cols = [
            'age', 'income', 'credit_score', 'num_products',
            'tenure_months', 'income_per_product', 'age_squared',
            'products_per_month'
        ]
        categorical_cols = ['is_active', 'country', 'age_group']

        # é¢„å¤„ç†
        self.preprocessor = DataPreprocessor()
        X, y = self.preprocessor.fit_transform(
            df_engineered,
            numerical_cols=numerical_cols,
            categorical_cols=categorical_cols,
            target_col='churn'
        )

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
        print(f"è®­ç»ƒé›†æµå¤±ç‡: {y_train.mean():.2%}")
        print(f"æµ‹è¯•é›†æµå¤±ç‡: {y_test.mean():.2%}")

        return X_train, X_test, y_train, y_test

# ä½¿ç”¨ Pipeline
pipeline = MLDataPipeline('customer_data.csv')
df = pipeline.load_data()

# æ¢ç´¢æ€§åˆ†æ
eda_report = pipeline.eda()
print("=" * 50)
print("æ•°æ®æ¦‚è§ˆ")
print("=" * 50)
print(f"æ•°æ®é›†å½¢çŠ¶: {eda_report['shape']}")
print(f"æµå¤±ç‡: {eda_report['churn_rate']:.2%}")

# å‡†å¤‡è®­ç»ƒæ•°æ®
X_train, X_test, y_train, y_test = pipeline.prepare_for_training()
```

---

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. å‘é‡åŒ– vs å¾ªç¯

```python
import time

# âŒ ä¸å¥½çš„åšæ³•ï¼šä½¿ç”¨ Python å¾ªç¯
def compute_distances_slow(points: np.ndarray) -> np.ndarray:
    n = len(points)
    distances = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            distances[i, j] = np.sqrt(np.sum((points[i] - points[j])**2))
    return distances

# âœ… å¥½çš„åšæ³•ï¼šä½¿ç”¨å‘é‡åŒ–
def compute_distances_fast(points: np.ndarray) -> np.ndarray:
    # ä½¿ç”¨å¹¿æ’­è®¡ç®—æ‰€æœ‰ç‚¹å¯¹ä¹‹é—´çš„è·ç¦»
    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]
    distances = np.sqrt((diff ** 2).sum(axis=2))
    return distances

# æ€§èƒ½å¯¹æ¯”
points = np.random.randn(100, 3)

start = time.time()
dist_slow = compute_distances_slow(points)
time_slow = time.time() - start

start = time.time()
dist_fast = compute_distances_fast(points)
time_fast = time.time() - start

print(f"å¾ªç¯æ–¹å¼: {time_slow:.4f} ç§’")
print(f"å‘é‡åŒ–æ–¹å¼: {time_fast:.4f} ç§’")
print(f"é€Ÿåº¦æå‡: {time_slow / time_fast:.1f}x")
```

### 2. å†…å­˜ä¼˜åŒ–

```python
# ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹èŠ‚çœå†…å­˜
df_large = pd.DataFrame({
    'id': range(1_000_000),
    'value': np.random.randn(1_000_000)
})

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
print("ä¼˜åŒ–å‰:")
print(df_large.memory_usage(deep=True))

# ä¼˜åŒ–æ•°æ®ç±»å‹
df_optimized = df_large.copy()
df_optimized['id'] = df_optimized['id'].astype('int32')  # int64 â†’ int32
df_optimized['value'] = df_optimized['value'].astype('float32')  # float64 â†’ float32

print("\nä¼˜åŒ–å:")
print(df_optimized.memory_usage(deep=True))
```

---

## å°ç»“

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†ï¼š

âœ… **NumPy æ ¸å¿ƒæ¦‚å¿µ**
- å¤šç»´æ•°ç»„ï¼ˆå¼ é‡ï¼‰çš„åˆ›å»ºå’Œæ“ä½œ
- å‘é‡åŒ–è®¡ç®—â€”â€”æ¯”å¾ªç¯å¿« 100 å€
- å¹¿æ’­æœºåˆ¶â€”â€”NumPy çš„è®¾è®¡å“²å­¦
- çŸ©é˜µè¿ç®—â€”â€”æ·±åº¦å­¦ä¹ çš„æ•°å­¦åŸºç¡€

âœ… **Pandas æ•°æ®å¤„ç†**
- DataFrame çš„åˆ›å»ºå’Œæ“ä½œ
- æ•°æ®æ¸…æ´—ï¼šç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼å¤„ç†
- ç‰¹å¾å·¥ç¨‹ï¼šåˆ›å»ºã€è½¬æ¢ã€ç¼–ç ç‰¹å¾
- å®Œæ•´çš„æ•°æ®é¢„å¤„ç† Pipeline

âœ… **SciPy ç§‘å­¦è®¡ç®—**
- ç»Ÿè®¡æµ‹è¯•å’Œå‡è®¾æ£€éªŒ
- è·ç¦»å’Œç›¸ä¼¼åº¦è®¡ç®—
- ä¼˜åŒ–é—®é¢˜æ±‚è§£

âœ… **æœ€ä½³å®è·µ**
- å‘é‡åŒ–ä¼˜å…ˆäºå¾ªç¯
- å†…å­˜ä¼˜åŒ–æŠ€å·§
- ä»£ç ç»„ç»‡å’Œå¯é‡ç”¨æ€§

---

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

1. åˆ›å»ºä¸€ä¸ª 5x5 çš„çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸º 1ï¼Œå…¶ä½™ä¸º 0
2. è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆä¸ä½¿ç”¨ SciPyï¼‰
3. ç”¨ Pandas è¯»å– CSV æ–‡ä»¶å¹¶å¤„ç†ç¼ºå¤±å€¼

### è¿›é˜¶é¢˜

4. å®ç°æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰çš„å‰å‘ä¼ æ’­
5. ç”¨ NumPy å®ç° Softmax å‡½æ•°ï¼ˆæ³¨æ„æ•°å€¼ç¨³å®šæ€§ï¼‰
6. æ„å»ºä¸€ä¸ªå®Œæ•´çš„ç‰¹å¾å·¥ç¨‹ Pipelineï¼ŒåŒ…æ‹¬ï¼š
   - æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
   - ç±»åˆ«ç‰¹å¾ç¼–ç 
   - åˆ›å»ºäº¤äº’ç‰¹å¾

### æŒ‘æˆ˜é¢˜

7. ä»é›¶å®ç°ä¸€ä¸ªç®€å•çš„ k-NN åˆ†ç±»å™¨ï¼ˆåªç”¨ NumPyï¼‰
8. å®ç° PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰é™ç»´ç®—æ³•
9. ç”¨å‘é‡åŒ–æ“ä½œå®ç°å›¾åƒå·ç§¯ï¼ˆä¸ä½¿ç”¨ for å¾ªç¯ï¼‰

---

**ä¸‹ä¸€èŠ‚ï¼š[10.3 æœºå™¨å­¦ä¹ å®æˆ˜ï¼šScikit-Learn å®Œå…¨æŒ‡å—](10.3-æœºå™¨å­¦ä¹ å®æˆ˜ï¼šScikit-Learnå®Œå…¨æŒ‡å—.md)**

åœ¨ä¸‹ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Scikit-Learn å®ç°å®Œæ•´çš„æœºå™¨å­¦ä¹ æµç¨‹ï¼Œä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹è¯„ä¼°ï¼Œæ„å»ºä½ çš„ç¬¬ä¸€ä¸ªç”Ÿäº§çº§ ML ç³»ç»Ÿã€‚
