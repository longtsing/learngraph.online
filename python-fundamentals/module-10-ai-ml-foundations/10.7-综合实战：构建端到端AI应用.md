# 10.7 ç»¼åˆå®žæˆ˜ï¼šæž„å»ºç«¯åˆ°ç«¯ AI åº”ç”¨

## ä»Žå®žéªŒåˆ°ç”Ÿäº§ï¼šå®Œæ•´çš„ ML ç”Ÿå‘½å‘¨æœŸ

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¹ äº† AI/ML çš„å„ä¸ªç»„ä»¶ã€‚çŽ°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ‰€æœ‰çŸ¥è¯†æ•´åˆï¼Œæž„å»ºä¸€ä¸ª**ç”Ÿäº§çº§**çš„ç«¯åˆ°ç«¯ AI åº”ç”¨ã€‚

> **ðŸ’¡ æ ¸å¿ƒç†å¿µ**ï¼šåœ¨ Jupyter Notebook é‡Œè®­ç»ƒæ¨¡åž‹åªæ˜¯å¼€å§‹ï¼ŒçœŸæ­£çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•å°†æ¨¡åž‹éƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒï¼Œå¹¶æŒç»­ç»´æŠ¤å’Œæ”¹è¿›ã€‚

**å®Œæ•´çš„ ML ç”Ÿå‘½å‘¨æœŸ**ï¼š
```
1. é—®é¢˜å®šä¹‰ â†’ ç¡®å®šä¸šåŠ¡ç›®æ ‡å’ŒæˆåŠŸæŒ‡æ ‡
2. æ•°æ®æ”¶é›† â†’ æž„å»ºæ•°æ® Pipeline
3. æŽ¢ç´¢åˆ†æž â†’ EDA å’Œç‰¹å¾å·¥ç¨‹
4. æ¨¡åž‹è®­ç»ƒ â†’ å®žéªŒè·Ÿè¸ªå’Œç‰ˆæœ¬ç®¡ç†
5. æ¨¡åž‹è¯„ä¼° â†’ ç¦»çº¿è¯„ä¼°å’Œ A/B æµ‹è¯•
6. æ¨¡åž‹éƒ¨ç½² â†’ API æœåŠ¡å’ŒæŽ¨ç†ä¼˜åŒ–
7. ç›‘æŽ§è¿ç»´ â†’ æ€§èƒ½ç›‘æŽ§å’Œæ¨¡åž‹æ›´æ–°
8. åé¦ˆè¿­ä»£ â†’ æ”¶é›†ç”¨æˆ·åé¦ˆï¼ŒæŒç»­æ”¹è¿›
```

æœ¬ç« å°†æž„å»ºä¸€ä¸ª**æ™ºèƒ½å®¢æœé—®ç­”ç³»ç»Ÿ**ï¼Œæ¶µç›–æ‰€æœ‰çŽ¯èŠ‚ã€‚

---

## é¡¹ç›®æ¦‚è¿°ï¼šæ™ºèƒ½å®¢æœé—®ç­”ç³»ç»Ÿ

### 1. ä¸šåŠ¡éœ€æ±‚

**ç›®æ ‡**ï¼šæž„å»ºä¸€ä¸ªæ™ºèƒ½å®¢æœç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š
- ç†è§£ç”¨æˆ·é—®é¢˜
- ä»ŽçŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
- ç”Ÿæˆå‡†ç¡®ã€è‡ªç„¶çš„å›žç­”
- å¤„ç†å¤šè½®å¯¹è¯
- æ”¯æŒé«˜å¹¶å‘ï¼ˆ1000+ QPSï¼‰

**æŠ€æœ¯æ ˆ**ï¼š
- **æ•°æ®å¤„ç†**ï¼šPandas, NumPy
- **å‘é‡æ•°æ®åº“**ï¼šChroma
- **æ¨¡åž‹**ï¼šSentence Transformers + GPT
- **åŽç«¯æ¡†æž¶**ï¼šFastAPI
- **å®žéªŒè·Ÿè¸ª**ï¼šMLflow
- **å®¹å™¨åŒ–**ï¼šDocker
- **ç›‘æŽ§**ï¼šPrometheus + Grafana

### 2. ç³»ç»Ÿæž¶æž„

```
ç”¨æˆ·è¯·æ±‚
    â†“
FastAPI æœåŠ¡
    â†“
    â”œâ†’ é—®é¢˜ç†è§£ï¼ˆNLUï¼‰
    â”œâ†’ æ£€ç´¢å¢žå¼ºï¼ˆRAGï¼‰
    â”‚   â”œâ†’ å‘é‡æ£€ç´¢ï¼ˆChromaï¼‰
    â”‚   â””â†’ é‡æŽ’åºï¼ˆCross-Encoderï¼‰
    â”œâ†’ ç­”æ¡ˆç”Ÿæˆï¼ˆLLMï¼‰
    â””â†’ æ—¥å¿—è®°å½•
    â†“
è¿”å›žç­”æ¡ˆ
    â†“
ç›‘æŽ§å’Œåˆ†æž
```

---

## ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å‡†å¤‡ä¸Žç‰¹å¾å·¥ç¨‹

### 1. æ•°æ®é‡‡é›†

```python
import pandas as pd
import numpy as np
from typing import List, Dict
import json

class DataCollector:
    """
    æ•°æ®é‡‡é›†å™¨

    åŠŸèƒ½ï¼š
    - ä»Žå¤šä¸ªæ¥æºé‡‡é›†æ•°æ®
    - æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
    - æž„å»ºçŸ¥è¯†åº“
    """

    def __init__(self):
        self.knowledge_base = []

    def collect_from_faq(self, faq_file: str) -> List[Dict]:
        """ä»Ž FAQ æ–‡ä»¶é‡‡é›†"""
        # ç¤ºä¾‹ FAQ æ•°æ®
        faq_data = [
            {
                "question": "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
                "answer": "æ‚¨å¯ä»¥ç‚¹å‡»ç™»å½•é¡µé¢çš„'å¿˜è®°å¯†ç 'é“¾æŽ¥ï¼Œè¾“å…¥æ³¨å†Œé‚®ç®±ï¼Œæˆ‘ä»¬ä¼šå‘é€é‡ç½®é“¾æŽ¥åˆ°æ‚¨çš„é‚®ç®±ã€‚",
                "category": "è´¦æˆ·ç®¡ç†",
                "keywords": ["å¯†ç ", "é‡ç½®", "æ‰¾å›ž"]
            },
            {
                "question": "å¦‚ä½•è”ç³»å®¢æœï¼Ÿ",
                "answer": "æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ä»¬ï¼š\n1. åœ¨çº¿å®¢æœï¼ˆå·¥ä½œæ—¥ 9:00-18:00ï¼‰\n2. å®¢æœé‚®ç®±ï¼šsupport@example.com\n3. å®¢æœçƒ­çº¿ï¼š400-123-4567",
                "category": "å®¢æˆ·æœåŠ¡",
                "keywords": ["å®¢æœ", "è”ç³»", "å¸®åŠ©"]
            },
            {
                "question": "è®¢å•å¤šä¹…å‘è´§ï¼Ÿ",
                "answer": "æ­£å¸¸æƒ…å†µä¸‹ï¼Œè®¢å•ä¼šåœ¨48å°æ—¶å†…å‘è´§ã€‚èŠ‚å‡æ—¥å¯èƒ½ä¼šå»¶è¿Ÿï¼Œå…·ä½“è¯·ä»¥ç‰©æµä¿¡æ¯ä¸ºå‡†ã€‚",
                "category": "ç‰©æµé…é€",
                "keywords": ["å‘è´§", "è®¢å•", "ç‰©æµ"]
            }
        ]

        return faq_data

    def collect_from_docs(self, doc_dir: str) -> List[Dict]:
        """ä»Žæ–‡æ¡£ç›®å½•é‡‡é›†"""
        # å®žé™…é¡¹ç›®ä¸­ä¼šè¯»å– PDFã€Markdown ç­‰æ–‡æ¡£
        doc_data = [
            {
                "title": "äº§å“ä½¿ç”¨æŒ‡å—",
                "content": "æœ¬äº§å“æ˜¯ä¸€æ¬¾æ™ºèƒ½å®¢æœç³»ç»Ÿï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€é—®ç­”...",
                "source": "user_manual.pdf",
                "page": 1
            }
        ]
        return doc_data

    def clean_and_structure(self, raw_data: List[Dict]) -> pd.DataFrame:
        """æ¸…æ´—å’Œç»“æž„åŒ–æ•°æ®"""
        df = pd.DataFrame(raw_data)

        # æ•°æ®æ¸…æ´—
        df = df.dropna(subset=['question', 'answer'])
        df['question'] = df['question'].str.strip()
        df['answer'] = df['answer'].str.strip()

        # æ·»åŠ å…ƒæ•°æ®
        df['doc_id'] = range(len(df))
        df['created_at'] = pd.Timestamp.now()

        return df

# ä½¿ç”¨ç¤ºä¾‹
collector = DataCollector()
faq_data = collector.collect_from_faq("faq.json")
df = collector.clean_and_structure(faq_data)

print(df.head())
```

### 2. æž„å»ºå‘é‡ç´¢å¼•

```python
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings

class VectorIndexBuilder:
    """
    å‘é‡ç´¢å¼•æž„å»ºå™¨

    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ Sentence Transformers ç”ŸæˆåµŒå…¥
    - å­˜å‚¨åˆ° Chroma å‘é‡æ•°æ®åº“
    - æ”¯æŒå¢žé‡æ›´æ–°
    """

    def __init__(
        self,
        model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        persist_directory: str = "./chroma_db"
    ):
        # åŠ è½½åµŒå…¥æ¨¡åž‹
        self.embedding_model = SentenceTransformer(model_name)

        # åˆå§‹åŒ– Chroma å®¢æˆ·ç«¯
        self.chroma_client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_directory
        ))

        # åˆ›å»ºæˆ–èŽ·å– collection
        self.collection = self.chroma_client.get_or_create_collection(
            name="knowledge_base",
            metadata={"description": "FAQ and documentation"}
        )

    def build_index(self, df: pd.DataFrame) -> None:
        """æž„å»ºå‘é‡ç´¢å¼•"""
        # ç”ŸæˆåµŒå…¥
        questions = df['question'].tolist()
        embeddings = self.embedding_model.encode(questions, show_progress_bar=True)

        # å‡†å¤‡æ–‡æ¡£
        documents = []
        metadatas = []
        ids = []

        for idx, row in df.iterrows():
            doc_id = f"doc_{row['doc_id']}"
            ids.append(doc_id)

            # ç»„åˆé—®é¢˜å’Œç­”æ¡ˆä½œä¸ºæ–‡æ¡£
            doc = f"é—®é¢˜ï¼š{row['question']}\nç­”æ¡ˆï¼š{row['answer']}"
            documents.append(doc)

            # å…ƒæ•°æ®
            metadata = {
                "question": row['question'],
                "answer": row['answer'],
                "category": row.get('category', 'general')
            }
            metadatas.append(metadata)

        # æ·»åŠ åˆ° Chroma
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )

        print(f"æˆåŠŸç´¢å¼• {len(documents)} ä¸ªæ–‡æ¡£")

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        è¯­ä¹‰æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›žå‰ k ä¸ªç»“æžœ

        Returns:
            æœç´¢ç»“æžœåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = self.embedding_model.encode([query])[0]

        # æ£€ç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )

        # è§£æžç»“æžœ
        retrieved_docs = []
        for i in range(len(results['ids'][0])):
            doc = {
                'id': results['ids'][0][i],
                'document': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            }
            retrieved_docs.append(doc)

        return retrieved_docs

# ä½¿ç”¨ç¤ºä¾‹
indexer = VectorIndexBuilder()
indexer.build_index(df)

# æµ‹è¯•æ£€ç´¢
query = "å¿˜è®°å¯†ç æ€Žä¹ˆåŠžï¼Ÿ"
results = indexer.search(query, top_k=3)

print(f"\næŸ¥è¯¢ï¼š{query}\n")
for i, result in enumerate(results):
    print(f"{i+1}. {result['metadata']['question']}")
    print(f"   ç›¸ä¼¼åº¦: {1 - result['distance']:.4f}")
    print()
```

---

## ç¬¬äºŒé˜¶æ®µï¼šæ¨¡åž‹è®­ç»ƒä¸Žå®žéªŒç®¡ç†

### 1. å®žéªŒè·Ÿè¸ªï¼ˆMLflowï¼‰

```python
import mlflow
import mlflow.pytorch
from typing import Dict, Any

class ExperimentTracker:
    """
    å®žéªŒè·Ÿè¸ªå™¨

    åŠŸèƒ½ï¼š
    - è®°å½•è¶…å‚æ•°å’ŒæŒ‡æ ‡
    - ç‰ˆæœ¬ç®¡ç†æ¨¡åž‹
    - å¯è§†åŒ–å®žéªŒç»“æžœ
    """

    def __init__(self, experiment_name: str = "qa_system"):
        mlflow.set_experiment(experiment_name)
        self.experiment_name = experiment_name

    def log_params(self, params: Dict[str, Any]) -> None:
        """è®°å½•è¶…å‚æ•°"""
        mlflow.log_params(params)

    def log_metrics(self, metrics: Dict[str, float], step: int = None) -> None:
        """è®°å½•æŒ‡æ ‡"""
        mlflow.log_metrics(metrics, step=step)

    def log_model(self, model, artifact_path: str = "model") -> None:
        """è®°å½•æ¨¡åž‹"""
        mlflow.pytorch.log_model(model, artifact_path)

    def start_run(self, run_name: str = None):
        """å¼€å§‹ä¸€ä¸ªå®žéªŒè¿è¡Œ"""
        return mlflow.start_run(run_name=run_name)

# ä½¿ç”¨ç¤ºä¾‹
tracker = ExperimentTracker()

with tracker.start_run(run_name="baseline_model"):
    # è®°å½•è¶…å‚æ•°
    params = {
        "embedding_model": "all-MiniLM-L6-v2",
        "top_k": 5,
        "temperature": 0.7
    }
    tracker.log_params(params)

    # è®­ç»ƒæ¨¡åž‹...
    # è®°å½•æŒ‡æ ‡
    metrics = {
        "accuracy": 0.85,
        "f1_score": 0.82,
        "latency_ms": 150
    }
    tracker.log_metrics(metrics)
```

### 2. æ¨¡åž‹å¾®è°ƒ

```python
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import Dataset
import torch

class IntentClassifier:
    """
    æ„å›¾åˆ†ç±»å™¨

    åŠŸèƒ½ï¼š
    - è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼ˆé—®å€™ã€æŸ¥è¯¢ã€æŠ•è¯‰ç­‰ï¼‰
    - åŸºäºŽ BERT å¾®è°ƒ
    - æ”¯æŒå¤šåˆ†ç±»
    """

    def __init__(self, model_name: str = "bert-base-chinese", num_labels: int = 5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=num_labels
        )

        self.label_map = {
            0: "greeting",  # é—®å€™
            1: "question",  # é—®é¢˜æŸ¥è¯¢
            2: "complaint",  # æŠ•è¯‰
            3: "feedback",  # åé¦ˆ
            4: "other"  # å…¶ä»–
        }

    def train(
        self,
        train_texts: List[str],
        train_labels: List[int],
        output_dir: str = "./intent_model"
    ):
        """è®­ç»ƒæ¨¡åž‹"""
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = Dataset.from_dict({
            "text": train_texts,
            "label": train_labels
        })

        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=128
            )

        train_dataset = train_dataset.map(tokenize_function, batched=True)

        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=16,
            save_steps=500,
            logging_steps=100,
            evaluation_strategy="steps",
            eval_steps=500
        )

        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
        )

        # è®­ç»ƒ
        trainer.train()

        # ä¿å­˜æ¨¡åž‹
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)

    def predict(self, text: str) -> Dict[str, Any]:
        """é¢„æµ‹æ„å›¾"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        )

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)

        pred_label = probabilities.argmax().item()
        pred_score = probabilities.max().item()

        return {
            "intent": self.label_map[pred_label],
            "confidence": pred_score
        }

# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå‡è®¾æœ‰è®­ç»ƒæ•°æ®ï¼‰
# classifier = IntentClassifier()
# classifier.train(train_texts, train_labels)
# result = classifier.predict("å¦‚ä½•é‡ç½®æˆ‘çš„å¯†ç ï¼Ÿ")
# print(result)
```

---

## ç¬¬ä¸‰é˜¶æ®µï¼šæž„å»º RAG ç³»ç»Ÿ

### 1. æ£€ç´¢å¢žå¼ºç”Ÿæˆï¼ˆRAGï¼‰

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List, Tuple
import torch

class RAGSystem:
    """
    æ£€ç´¢å¢žå¼ºç”Ÿæˆç³»ç»Ÿ

    æµç¨‹ï¼š
    1. è¯­ä¹‰æ£€ç´¢ç›¸å…³æ–‡æ¡£
    2. æž„å»ºä¸Šä¸‹æ–‡ prompt
    3. LLM ç”Ÿæˆç­”æ¡ˆ
    """

    def __init__(
        self,
        retriever: VectorIndexBuilder,
        model_name: str = "gpt2"
    ):
        self.retriever = retriever

        # åŠ è½½ç”Ÿæˆæ¨¡åž‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)

        # è®¾ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)

    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        return self.retriever.search(query, top_k=top_k)

    def build_prompt(self, query: str, context_docs: List[Dict]) -> str:
        """æž„å»º prompt"""
        # ç»„ç»‡ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc['document']}"
            for i, doc in enumerate(context_docs)
        ])

        # æž„å»º prompt
        prompt = f"""åŸºäºŽä»¥ä¸‹æ–‡æ¡£å›žç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æžœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®žå‘ŠçŸ¥ã€‚

{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›žç­”ï¼š"""

        return prompt

    def generate(
        self,
        prompt: str,
        max_length: int = 200,
        temperature: float = 0.7
    ) -> str:
        """ç”Ÿæˆç­”æ¡ˆ"""
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs['input_ids'],
                max_length=len(inputs['input_ids'][0]) + max_length,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–ç­”æ¡ˆéƒ¨åˆ†ï¼ˆåŽ»æŽ‰ promptï¼‰
        answer = generated_text[len(prompt):].strip()

        return answer

    def answer_question(self, query: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹

        Returns:
            {
                'query': åŽŸå§‹é—®é¢˜,
                'answer': ç”Ÿæˆçš„ç­”æ¡ˆ,
                'retrieved_docs': æ£€ç´¢åˆ°çš„æ–‡æ¡£,
                'confidence': ç½®ä¿¡åº¦
            }
        """
        # 1. æ£€ç´¢
        retrieved_docs = self.retrieve(query, top_k=3)

        # 2. æž„å»º prompt
        prompt = self.build_prompt(query, retrieved_docs)

        # 3. ç”Ÿæˆç­”æ¡ˆ
        answer = self.generate(prompt)

        # 4. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºŽæ£€ç´¢åˆ†æ•°ï¼‰
        if retrieved_docs:
            confidence = 1 - retrieved_docs[0]['distance']
        else:
            confidence = 0.0

        return {
            'query': query,
            'answer': answer,
            'retrieved_docs': retrieved_docs,
            'confidence': confidence
        }

# ä½¿ç”¨ç¤ºä¾‹
# rag_system = RAGSystem(indexer)
# result = rag_system.answer_question("å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ")
# print(result['answer'])
```

---

## ç¬¬å››é˜¶æ®µï¼šAPI æœåŠ¡éƒ¨ç½²

### 1. FastAPI æœåŠ¡

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
import uvicorn
import time

app = FastAPI(
    title="æ™ºèƒ½å®¢æœ API",
    description="åŸºäºŽ RAG çš„é—®ç­”ç³»ç»Ÿ",
    version="1.0.0"
)

# è¯·æ±‚æ¨¡åž‹
class QuestionRequest(BaseModel):
    question: str
    top_k: Optional[int] = 3
    temperature: Optional[float] = 0.7

class QuestionResponse(BaseModel):
    answer: str
    confidence: float
    retrieved_docs: List[Dict]
    latency_ms: float

# åˆå§‹åŒ–ç³»ç»Ÿï¼ˆå®žé™…åº”ç”¨ä¸­åº”è¯¥åœ¨å¯åŠ¨æ—¶åŠ è½½ï¼‰
# indexer = VectorIndexBuilder()
# rag_system = RAGSystem(indexer)

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "message": "æ™ºèƒ½å®¢æœ API æ­£åœ¨è¿è¡Œ"}

@app.post("/ask", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest):
    """
    é—®ç­”æŽ¥å£

    Args:
        request: åŒ…å«é—®é¢˜çš„è¯·æ±‚

    Returns:
        ç­”æ¡ˆå’Œç›¸å…³ä¿¡æ¯
    """
    try:
        start_time = time.time()

        # è°ƒç”¨ RAG ç³»ç»Ÿ
        # result = rag_system.answer_question(request.question)

        # æ¨¡æ‹Ÿå“åº”ï¼ˆå®žé™…åº”è¯¥ç”¨ä¸Šé¢çš„ resultï¼‰
        result = {
            'answer': "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿç­”æ¡ˆã€‚",
            'confidence': 0.95,
            'retrieved_docs': []
        }

        latency_ms = (time.time() - start_time) * 1000

        return QuestionResponse(
            answer=result['answer'],
            confidence=result['confidence'],
            retrieved_docs=result['retrieved_docs'],
            latency_ms=latency_ms
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/feedback")
async def submit_feedback(
    question: str,
    answer: str,
    rating: int,
    comment: Optional[str] = None
):
    """
    ç”¨æˆ·åé¦ˆæŽ¥å£

    ç”¨äºŽæ”¶é›†ç”¨æˆ·å¯¹ç­”æ¡ˆçš„è¯„ä»·ï¼Œæ”¹è¿›æ¨¡åž‹
    """
    # å­˜å‚¨åé¦ˆåˆ°æ•°æ®åº“
    feedback_data = {
        "question": question,
        "answer": answer,
        "rating": rating,
        "comment": comment,
        "timestamp": time.time()
    }

    # å®žé™…åº”è¯¥å­˜å‚¨åˆ°æ•°æ®åº“
    # db.save_feedback(feedback_data)

    return {"status": "success", "message": "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼"}

# è¿è¡ŒæœåŠ¡
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. Dockerfile

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# ä¸‹è½½æ¨¡åž‹ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥åœ¨è¿è¡Œæ—¶ä¸‹è½½ï¼‰
# RUN python -c "from transformers import AutoModel; AutoModel.from_pretrained('bert-base-chinese')"

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 3. Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_NAME=bert-base-chinese
      - CHROMA_PERSIST_DIR=/data/chroma_db
    volumes:
      - ./data:/data
    restart: unless-stopped

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

---

## ç¬¬äº”é˜¶æ®µï¼šç›‘æŽ§å’Œä¼˜åŒ–

### 1. æ€§èƒ½ç›‘æŽ§

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Request
import time

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint', 'status'])
REQUEST_LATENCY = Histogram('api_request_latency_seconds', 'Request latency', ['endpoint'])
ACTIVE_USERS = Gauge('active_users', 'Number of active users')
MODEL_CONFIDENCE = Histogram('model_confidence', 'Model confidence scores')

@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    """è¯·æ±‚ç›‘æŽ§ä¸­é—´ä»¶"""
    start_time = time.time()

    # å¤„ç†è¯·æ±‚
    response = await call_next(request)

    # è®°å½•æŒ‡æ ‡
    latency = time.time() - start_time
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    REQUEST_LATENCY.labels(endpoint=request.url.path).observe(latency)

    return response

@app.get("/metrics")
async def metrics():
    """Prometheus æŒ‡æ ‡ç«¯ç‚¹"""
    return generate_latest()
```

### 2. æ¨¡åž‹æ€§èƒ½åˆ†æž

```python
import psutil
import GPUtil

class ModelProfiler:
    """
    æ¨¡åž‹æ€§èƒ½åˆ†æžå™¨

    åŠŸèƒ½ï¼š
    - CPU/GPU ä½¿ç”¨çŽ‡
    - å†…å­˜å ç”¨
    - æŽ¨ç†å»¶è¿Ÿ
    """

    @staticmethod
    def get_system_stats() -> Dict[str, Any]:
        """èŽ·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        # CPU
        cpu_percent = psutil.cpu_percent(interval=1)

        # å†…å­˜
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_gb = memory.used / (1024 ** 3)

        # GPUï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        gpu_stats = []
        try:
            gpus = GPUtil.getGPUs()
            for gpu in gpus:
                gpu_stats.append({
                    'name': gpu.name,
                    'load': gpu.load * 100,
                    'memory_used_mb': gpu.memoryUsed,
                    'memory_total_mb': gpu.memoryTotal,
                    'temperature': gpu.temperature
                })
        except:
            pass

        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory_percent,
            'memory_used_gb': round(memory_used_gb, 2),
            'gpu_stats': gpu_stats
        }

    @staticmethod
    def profile_inference(model, input_data, num_runs: int = 100):
        """
        æ€§èƒ½åˆ†æž

        Returns:
            å¹³å‡å»¶è¿Ÿã€åžåé‡ç­‰æŒ‡æ ‡
        """
        import time

        latencies = []

        for _ in range(num_runs):
            start = time.time()
            _ = model(input_data)
            latency = time.time() - start
            latencies.append(latency)

        return {
            'mean_latency_ms': np.mean(latencies) * 1000,
            'p50_latency_ms': np.percentile(latencies, 50) * 1000,
            'p95_latency_ms': np.percentile(latencies, 95) * 1000,
            'p99_latency_ms': np.percentile(latencies, 99) * 1000,
            'throughput_qps': 1 / np.mean(latencies)
        }

# ä½¿ç”¨ç¤ºä¾‹
profiler = ModelProfiler()
stats = profiler.get_system_stats()
print(stats)
```

---

## ç¬¬å…­é˜¶æ®µï¼šæŒç»­æ”¹è¿›

### 1. A/B æµ‹è¯•æ¡†æž¶

```python
import random
from typing import Dict

class ABTestingFramework:
    """
    A/B æµ‹è¯•æ¡†æž¶

    åŠŸèƒ½ï¼š
    - æµé‡åˆ†é…
    - æŒ‡æ ‡æ”¶é›†
    - ç»Ÿè®¡åˆ†æž
    """

    def __init__(self):
        self.experiments = {}

    def create_experiment(
        self,
        experiment_id: str,
        variants: Dict[str, float]
    ):
        """
        åˆ›å»ºå®žéªŒ

        Args:
            experiment_id: å®žéªŒ ID
            variants: å˜ä½“åŠå…¶æµé‡æ¯”ä¾‹ {'A': 0.5, 'B': 0.5}
        """
        self.experiments[experiment_id] = {
            'variants': variants,
            'results': {variant: [] for variant in variants}
        }

    def assign_variant(self, experiment_id: str, user_id: str) -> str:
        """ä¸ºç”¨æˆ·åˆ†é…å˜ä½“"""
        experiment = self.experiments[experiment_id]
        variants = list(experiment['variants'].keys())
        weights = list(experiment['variants'].values())

        # åŸºäºŽç”¨æˆ· ID çš„ä¸€è‡´æ€§å“ˆå¸Œ
        random.seed(hash(user_id))
        variant = random.choices(variants, weights=weights)[0]

        return variant

    def record_result(
        self,
        experiment_id: str,
        variant: str,
        metric_value: float
    ):
        """è®°å½•å®žéªŒç»“æžœ"""
        self.experiments[experiment_id]['results'][variant].append(metric_value)

    def analyze_results(self, experiment_id: str) -> Dict:
        """åˆ†æžå®žéªŒç»“æžœ"""
        from scipy import stats

        experiment = self.experiments[experiment_id]
        results = experiment['results']

        # è®¡ç®—ç»Ÿè®¡é‡
        analysis = {}
        for variant, values in results.items():
            if len(values) > 0:
                analysis[variant] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'count': len(values)
                }

        # t æ£€éªŒï¼ˆå¦‚æžœæœ‰ä¸¤ä¸ªå˜ä½“ï¼‰
        if len(results) == 2:
            variant_names = list(results.keys())
            values_a = results[variant_names[0]]
            values_b = results[variant_names[1]]

            if len(values_a) > 0 and len(values_b) > 0:
                t_stat, p_value = stats.ttest_ind(values_a, values_b)
                analysis['t_test'] = {
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'significant': p_value < 0.05
                }

        return analysis

# ä½¿ç”¨ç¤ºä¾‹
ab_test = ABTestingFramework()
ab_test.create_experiment('model_v2', {'v1': 0.5, 'v2': 0.5})

# åœ¨å®žé™…è¯·æ±‚ä¸­ä½¿ç”¨
user_id = "user_123"
variant = ab_test.assign_variant('model_v2', user_id)
print(f"ç”¨æˆ· {user_id} è¢«åˆ†é…åˆ°å˜ä½“ï¼š{variant}")

# è®°å½•ç»“æžœ
ab_test.record_result('model_v2', variant, 0.95)  # æ»¡æ„åº¦åˆ†æ•°

# åˆ†æžç»“æžœ
# results = ab_test.analyze_results('model_v2')
# print(results)
```

### 2. æ¨¡åž‹æ›´æ–°ç­–ç•¥

```python
class ModelVersionManager:
    """
    æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    - ç‰ˆæœ¬åˆ‡æ¢
    - ç°åº¦å‘å¸ƒ
    - å›žæ»šæœºåˆ¶
    """

    def __init__(self):
        self.models = {}
        self.active_version = None

    def register_model(self, version: str, model):
        """æ³¨å†Œæ¨¡åž‹ç‰ˆæœ¬"""
        self.models[version] = {
            'model': model,
            'deployed_at': time.time(),
            'request_count': 0,
            'error_count': 0
        }

    def set_active_version(self, version: str):
        """è®¾ç½®æ´»è·ƒç‰ˆæœ¬"""
        if version not in self.models:
            raise ValueError(f"æ¨¡åž‹ç‰ˆæœ¬ {version} ä¸å­˜åœ¨")
        self.active_version = version

    def get_model(self, version: str = None):
        """èŽ·å–æ¨¡åž‹ï¼ˆå¯æŒ‡å®šç‰ˆæœ¬ï¼‰"""
        if version is None:
            version = self.active_version

        if version not in self.models:
            raise ValueError(f"æ¨¡åž‹ç‰ˆæœ¬ {version} ä¸å­˜åœ¨")

        self.models[version]['request_count'] += 1
        return self.models[version]['model']

    def canary_deployment(
        self,
        new_version: str,
        canary_percent: float = 0.1
    ):
        """
        é‡‘ä¸é›€éƒ¨ç½²

        Args:
            new_version: æ–°ç‰ˆæœ¬
            canary_percent: æµé‡æ¯”ä¾‹
        """
        if random.random() < canary_percent:
            return self.get_model(new_version)
        else:
            return self.get_model(self.active_version)

# ä½¿ç”¨ç¤ºä¾‹
version_manager = ModelVersionManager()
# version_manager.register_model('v1.0', model_v1)
# version_manager.register_model('v1.1', model_v2)
# version_manager.set_active_version('v1.0')

# é‡‘ä¸é›€éƒ¨ç½²
# model = version_manager.canary_deployment('v1.1', canary_percent=0.1)
```

---

## å°ç»“

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æž„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯ AI åº”ç”¨ï¼š

âœ… **æ•°æ® Pipeline**
- æ•°æ®é‡‡é›†å’Œæ¸…æ´—
- å‘é‡ç´¢å¼•æž„å»º
- å¢žé‡æ›´æ–°æœºåˆ¶

âœ… **æ¨¡åž‹è®­ç»ƒ**
- å®žéªŒè·Ÿè¸ªï¼ˆMLflowï¼‰
- æ¨¡åž‹å¾®è°ƒ
- è¶…å‚æ•°ä¼˜åŒ–

âœ… **RAG ç³»ç»Ÿ**
- è¯­ä¹‰æ£€ç´¢
- ä¸Šä¸‹æ–‡æž„å»º
- ç­”æ¡ˆç”Ÿæˆ

âœ… **API æœåŠ¡**
- FastAPI åŽç«¯
- Docker å®¹å™¨åŒ–
- è´Ÿè½½å‡è¡¡

âœ… **ç›‘æŽ§è¿ç»´**
- Prometheus æŒ‡æ ‡
- æ€§èƒ½åˆ†æž
- æ—¥å¿—è®°å½•

âœ… **æŒç»­æ”¹è¿›**
- A/B æµ‹è¯•
- æ¨¡åž‹æ›´æ–°
- åé¦ˆå¾ªçŽ¯

---

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜
1. éƒ¨ç½²ä¸€ä¸ªç®€å•çš„ FastAPI æœåŠ¡ï¼Œå®žçŽ°æ–‡æœ¬åˆ†ç±»æŽ¥å£
2. ä½¿ç”¨ Docker å®¹å™¨åŒ–ä½ çš„åº”ç”¨
3. æ·»åŠ åŸºæœ¬çš„æ—¥å¿—è®°å½•

### è¿›é˜¶é¢˜
4. å®žçŽ°ä¸€ä¸ªå®Œæ•´çš„ RAG ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ£€ç´¢å’Œç”Ÿæˆ
5. æ·»åŠ  Prometheus ç›‘æŽ§å’Œ Grafana å¯è§†åŒ–
6. å®žçŽ°æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†å’Œç°åº¦å‘å¸ƒ

### æŒ‘æˆ˜é¢˜
7. æž„å»ºä¸€ä¸ªæ”¯æŒå¤šè½®å¯¹è¯çš„èŠå¤©æœºå™¨äºº
8. å®žçŽ°åˆ†å¸ƒå¼æŽ¨ç†ï¼ˆå¤š GPU/å¤šæœºå™¨ï¼‰
9. è®¾è®¡å¹¶å®žçŽ°ä¸€ä¸ªå®Œæ•´çš„ A/B æµ‹è¯•ç³»ç»Ÿ

---

**ä¸‹ä¸€èŠ‚ï¼š[10.8 æœ¬ç« å°ç»“å’Œå¤ä¹ ](10.8-æœ¬ç« å°ç»“å’Œå¤ä¹ .md)**

æœ€åŽä¸€èŠ‚å°†æ€»ç»“æœ¬ç« æ‰€æœ‰å†…å®¹ï¼Œæä¾›ç»¼åˆç»ƒä¹ å’Œå­¦ä¹ è·¯å¾„å»ºè®®ï¼
