# 10.3 æœºå™¨å­¦ä¹ å®æˆ˜ï¼šScikit-Learn å®Œå…¨æŒ‡å—

## ä»æ•°æ®åˆ°æ™ºèƒ½ï¼šæœºå™¨å­¦ä¹ çš„æœ¬è´¨

åœ¨æŒæ¡äº† NumPy å’Œ Pandas è¿™äº›æ•°æ®å¤„ç†å·¥å…·åï¼Œæˆ‘ä»¬ç»ˆäºå¯ä»¥å›ç­”è¿™ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š**å¦‚ä½•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ ï¼Ÿ**

> **ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ**ï¼šæœºå™¨å­¦ä¹ çš„æœ¬è´¨æ˜¯**ä»æ•°æ®ä¸­è‡ªåŠ¨å‘ç°æ¨¡å¼**ï¼Œå¹¶ç”¨è¿™äº›æ¨¡å¼å¯¹æœªè§è¿‡çš„æ•°æ®åšå‡ºé¢„æµ‹ã€‚

ä¼ ç»Ÿç¼–ç¨‹ï¼š
```
è§„åˆ™ + æ•°æ® â†’ ç»“æœ
```

æœºå™¨å­¦ä¹ ï¼š
```
æ•°æ® + ç»“æœ â†’ è§„åˆ™
```

Scikit-Learn æ˜¯ Python ä¸­æœ€æˆç†Ÿã€æœ€æ˜“ç”¨çš„æœºå™¨å­¦ä¹ åº“ï¼Œè¢«å·¥ä¸šç•Œå¹¿æ³›é‡‡ç”¨ã€‚å®ƒæä¾›äº†ï¼š
- ç»Ÿä¸€çš„ API è®¾è®¡ï¼ˆæ‰€æœ‰æ¨¡å‹éƒ½æœ‰ `fit()` å’Œ `predict()` æ–¹æ³•ï¼‰
- ä¸°å¯Œçš„ç®—æ³•å®ç°ï¼ˆåˆ†ç±»ã€å›å½’ã€èšç±»ã€é™ç»´ï¼‰
- å®Œå–„çš„æ•°æ®é¢„å¤„ç†å·¥å…·
- å¼ºå¤§çš„æ¨¡å‹è¯„ä¼°å’Œé€‰æ‹©åŠŸèƒ½

---

## æœºå™¨å­¦ä¹ çš„ä¸‰å¤§èŒƒå¼

### 1. ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰

**å®šä¹‰**ï¼šä»**æœ‰æ ‡ç­¾**çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œé¢„æµ‹æ–°æ•°æ®çš„æ ‡ç­¾ã€‚

```python
è®­ç»ƒæ•°æ®ï¼š(ç‰¹å¾, æ ‡ç­¾) å¯¹
ä¾‹å¦‚ï¼š(æˆ¿å±‹é¢ç§¯ã€ä½ç½®ã€æˆ¿é¾„, æˆ¿ä»·)
     (é‚®ä»¶å†…å®¹, æ˜¯å¦åƒåœ¾é‚®ä»¶)
```

**ä¸¤å¤§å­ç±»**ï¼š
- **åˆ†ç±»ï¼ˆClassificationï¼‰**ï¼šé¢„æµ‹ç¦»æ•£æ ‡ç­¾ï¼ˆå¦‚ï¼šåƒåœ¾é‚®ä»¶/æ­£å¸¸é‚®ä»¶ï¼‰
- **å›å½’ï¼ˆRegressionï¼‰**ï¼šé¢„æµ‹è¿ç»­å€¼ï¼ˆå¦‚ï¼šæˆ¿ä»·ã€æ¸©åº¦ï¼‰

### 2. æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰

**å®šä¹‰**ï¼šä»**æ— æ ‡ç­¾**çš„æ•°æ®ä¸­å‘ç°éšè—çš„ç»“æ„æˆ–æ¨¡å¼ã€‚

```python
è®­ç»ƒæ•°æ®ï¼šåªæœ‰ç‰¹å¾ï¼Œæ²¡æœ‰æ ‡ç­¾
ä¾‹å¦‚ï¼š(ç”¨æˆ·æµè§ˆè®°å½•) â†’ ç”¨æˆ·åˆ†ç¾¤
     (æ–‡ç« å†…å®¹) â†’ ä¸»é¢˜èšç±»
```

**å¸¸è§ä»»åŠ¡**ï¼š
- **èšç±»ï¼ˆClusteringï¼‰**ï¼šå°†ç›¸ä¼¼çš„æ•°æ®ç‚¹åˆ†ç»„
- **é™ç»´ï¼ˆDimensionality Reductionï¼‰**ï¼šå‹ç¼©é«˜ç»´æ•°æ®
- **å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detectionï¼‰**ï¼šå‘ç°å¼‚å¸¸æ•°æ®ç‚¹

### 3. å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰

**å®šä¹‰**ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œå­¦ä¹ æœ€ä¼˜ç­–ç•¥ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚

```python
Agent â†’ æ‰§è¡ŒåŠ¨ä½œ â†’ ç¯å¢ƒåé¦ˆå¥–åŠ± â†’ Agent å­¦ä¹ 
```

**ç»å…¸åº”ç”¨**ï¼šAlphaGoã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººæ§åˆ¶

> **ğŸ“Œ æœ¬èŠ‚é‡ç‚¹**ï¼šæˆ‘ä»¬å°†æ·±å…¥ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ ï¼Œå¼ºåŒ–å­¦ä¹ åœ¨åç»­ç« èŠ‚ä»‹ç»ã€‚

---

## Scikit-Learn çš„è®¾è®¡å“²å­¦

### ç»Ÿä¸€çš„ Estimator API

æ‰€æœ‰ Scikit-Learn æ¨¡å‹éƒ½éµå¾ªç›¸åŒçš„æ¥å£ï¼š

```python
from sklearn.base import BaseEstimator, ClassifierMixin

# æ‰€æœ‰æ¨¡å‹éƒ½ç»§æ‰¿è‡ª BaseEstimator
class MyModel(BaseEstimator, ClassifierMixin):
    def fit(self, X, y):
        """ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ """
        # å­¦ä¹ å‚æ•°
        return self

    def predict(self, X):
        """å¯¹æ–°æ•°æ®åšé¢„æµ‹"""
        # è¿”å›é¢„æµ‹ç»“æœ
        return predictions

    def score(self, X, y):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # è¿”å›å‡†ç¡®ç‡ã€RÂ²ç­‰æŒ‡æ ‡
        return accuracy
```

è¿™ç§ç»Ÿä¸€çš„è®¾è®¡è®©æˆ‘ä»¬å¯ä»¥è½»æ¾åˆ‡æ¢ä¸åŒç®—æ³•ï¼š

```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# ä¸‰ä¸ªä¸åŒçš„æ¨¡å‹ï¼Œä½† API å®Œå…¨ç›¸åŒ
models = [
    LogisticRegression(),
    RandomForestClassifier(),
    SVC()
]

for model in models:
    model.fit(X_train, y_train)  # è®­ç»ƒ
    score = model.score(X_test, y_test)  # è¯„ä¼°
    print(f"{model.__class__.__name__}: {score:.4f}")
```

---

## ç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®ï¼šé¸¢å°¾èŠ±åˆ†ç±»

è®©æˆ‘ä»¬ç”¨ç»å…¸çš„ Iris æ•°æ®é›†å¼€å§‹ï¼š

```python
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from typing import Tuple

# 1. åŠ è½½æ•°æ®
iris = load_iris()
X = iris.data  # ç‰¹å¾ï¼šèŠ±è¼é•¿åº¦ã€èŠ±è¼å®½åº¦ã€èŠ±ç“£é•¿åº¦ã€èŠ±ç“£å®½åº¦
y = iris.target  # æ ‡ç­¾ï¼š0=setosa, 1=versicolor, 2=virginica

print(f"æ•°æ®é›†å½¢çŠ¶: {X.shape}")
print(f"ç‰¹å¾åç§°: {iris.feature_names}")
print(f"ç±»åˆ«åç§°: {iris.target_names}")

# è½¬æ¢ä¸º DataFrame ä¾¿äºåˆ†æ
df = pd.DataFrame(X, columns=iris.feature_names)
df['species'] = iris.target_names[y]
print(df.head())

# 2. æ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒé›† 80%ï¼Œæµ‹è¯•é›† 20%ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nè®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
print(f"è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_train)}")
print(f"æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_test)}")

# 3. ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\næ ‡å‡†åŒ–å‰å‡å€¼: {X_train[:, 0].mean():.4f}, æ ‡å‡†å·®: {X_train[:, 0].std():.4f}")
print(f"æ ‡å‡†åŒ–åå‡å€¼: {X_train_scaled[:, 0].mean():.4f}, æ ‡å‡†å·®: {X_train_scaled[:, 0].std():.4f}")

# 4. è®­ç»ƒæ¨¡å‹
model = LogisticRegression(max_iter=200, random_state=42)
model.fit(X_train_scaled, y_train)

# 5. é¢„æµ‹
y_pred = model.predict(X_test_scaled)

# 6. è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"\nå‡†ç¡®ç‡: {accuracy:.4f}")

# è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ•°æ®é›†å½¢çŠ¶: (150, 4)
è®­ç»ƒé›†: (120, 4), æµ‹è¯•é›†: (30, 4)
å‡†ç¡®ç‡: 1.0000

åˆ†ç±»æŠ¥å‘Š:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        10
  versicolor       1.00      1.00      1.00         9
   virginica       1.00      1.00      1.00        11

    accuracy                           1.00        30
```

> **ğŸ’¡ å…³é”®æ­¥éª¤**ï¼š
> 1. **åŠ è½½æ•°æ®**ï¼šä½¿ç”¨å†…ç½®æ•°æ®é›†æˆ–è¯»å–æ–‡ä»¶
> 2. **æ•°æ®åˆ†å‰²**ï¼šè®­ç»ƒé›†ç”¨äºå­¦ä¹ ï¼Œæµ‹è¯•é›†ç”¨äºè¯„ä¼°
> 3. **ç‰¹å¾ç¼©æ”¾**ï¼šè®©ä¸åŒå°ºåº¦çš„ç‰¹å¾åœ¨åŒä¸€é‡çº§
> 4. **è®­ç»ƒæ¨¡å‹**ï¼šè°ƒç”¨ `fit()` æ–¹æ³•
> 5. **é¢„æµ‹è¯„ä¼°**ï¼šç”¨ `predict()` å’Œè¯„ä¼°æŒ‡æ ‡

---

## æ•°æ®é¢„å¤„ç†ï¼šæœºå™¨å­¦ä¹ çš„ 80% å·¥ä½œ

### 1. ç‰¹å¾ç¼©æ”¾ï¼ˆFeature Scalingï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾ï¼Ÿ**

è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ KNNã€SVMã€ç¥ç»ç½‘ç»œï¼‰å¯¹ç‰¹å¾çš„å°ºåº¦æ•æ„Ÿã€‚è€ƒè™‘è¿™ä¸ªä¾‹å­ï¼š

```python
# ä¸¤ä¸ªç‰¹å¾ï¼šæˆ¿å±‹é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰å’Œæˆ¿é—´æ•°
X = np.array([
    [50, 2],    # 50 å¹³ç±³ï¼Œ2 ä¸ªæˆ¿é—´
    [150, 4],   # 150 å¹³ç±³ï¼Œ4 ä¸ªæˆ¿é—´
])

# å¦‚æœç›´æ¥è®¡ç®—æ¬§æ°è·ç¦»ï¼Œé¢ç§¯çš„å½±å“ä¼šè¿œå¤§äºæˆ¿é—´æ•°
# å› ä¸ºé¢ç§¯çš„æ•°å€¼èŒƒå›´ [50, 150]ï¼Œè€Œæˆ¿é—´æ•°åªæœ‰ [2, 4]
```

#### æ ‡å‡†åŒ–ï¼ˆStandardizationï¼‰

```python
from sklearn.preprocessing import StandardScaler

# å…¬å¼ï¼šz = (x - Î¼) / Ïƒ
# ç»“æœï¼šå‡å€¼=0ï¼Œæ ‡å‡†å·®=1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("åŸå§‹æ•°æ®:")
print(X)
print("\næ ‡å‡†åŒ–å:")
print(X_scaled)
print(f"å‡å€¼: {X_scaled.mean(axis=0)}")
print(f"æ ‡å‡†å·®: {X_scaled.std(axis=0)}")
```

#### å½’ä¸€åŒ–ï¼ˆNormalizationï¼‰

```python
from sklearn.preprocessing import MinMaxScaler

# å…¬å¼ï¼šx' = (x - x_min) / (x_max - x_min)
# ç»“æœï¼šæ•°æ®èŒƒå›´ [0, 1]
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

print("å½’ä¸€åŒ–å:")
print(X_normalized)
print(f"æœ€å°å€¼: {X_normalized.min(axis=0)}")
print(f"æœ€å¤§å€¼: {X_normalized.max(axis=0)}")
```

> **âš ï¸ é‡è¦**ï¼šå¿…é¡»å…ˆåœ¨è®­ç»ƒé›†ä¸Š `fit()`ï¼Œå†å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½è¿›è¡Œ `transform()`ã€‚
>
> ```python
> # âŒ é”™è¯¯åšæ³•
> X_train = scaler.fit_transform(X_train)
> X_test = scaler.fit_transform(X_test)  # ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ï¼
>
> # âœ… æ­£ç¡®åšæ³•
> X_train = scaler.fit_transform(X_train)
> X_test = scaler.transform(X_test)  # ä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡
> ```

### 2. ç±»åˆ«ç‰¹å¾ç¼–ç 

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pandas as pd

# ç¤ºä¾‹æ•°æ®
data = pd.DataFrame({
    'city': ['Beijing', 'Shanghai', 'Beijing', 'Guangzhou', 'Shanghai'],
    'size': ['S', 'M', 'L', 'M', 'S'],
    'price': [100, 200, 150, 180, 120]
})

# æ–¹æ³• 1ï¼šæ ‡ç­¾ç¼–ç ï¼ˆLabelEncoderï¼‰
# é€‚ç”¨äºæœ‰åºç±»åˆ«ï¼ˆå¦‚ S < M < Lï¼‰
le = LabelEncoder()
data['size_encoded'] = le.fit_transform(data['size'])
print("æ ‡ç­¾ç¼–ç :")
print(data[['size', 'size_encoded']])

# æ–¹æ³• 2ï¼šç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰
# é€‚ç”¨äºæ— åºç±»åˆ«ï¼ˆå¦‚åŸå¸‚ï¼‰
data_onehot = pd.get_dummies(data, columns=['city'], prefix='city')
print("\nç‹¬çƒ­ç¼–ç :")
print(data_onehot)

# ä½¿ç”¨ Scikit-Learn çš„ OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
city_encoded = ohe.fit_transform(data[['city']])
print(f"\nOneHotEncoder ç»“æœå½¢çŠ¶: {city_encoded.shape}")
print(ohe.get_feature_names_out())
```

### 3. ç¼ºå¤±å€¼å¤„ç†

```python
from sklearn.impute import SimpleImputer
import numpy as np

# åˆ›å»ºå¸¦ç¼ºå¤±å€¼çš„æ•°æ®
X = np.array([
    [1, 2, np.nan],
    [3, np.nan, 6],
    [np.nan, 8, 9],
    [4, 5, 6]
])

# ç­–ç•¥ 1ï¼šç”¨å‡å€¼å¡«å……
imputer_mean = SimpleImputer(strategy='mean')
X_mean = imputer_mean.fit_transform(X)
print("å‡å€¼å¡«å……:")
print(X_mean)

# ç­–ç•¥ 2ï¼šç”¨ä¸­ä½æ•°å¡«å……
imputer_median = SimpleImputer(strategy='median')
X_median = imputer_median.fit_transform(X)
print("\nä¸­ä½æ•°å¡«å……:")
print(X_median)

# ç­–ç•¥ 3ï¼šç”¨æœ€é¢‘ç¹å€¼å¡«å……
imputer_frequent = SimpleImputer(strategy='most_frequent')
X_frequent = imputer_frequent.fit_transform(X)
print("\nä¼—æ•°å¡«å……:")
print(X_frequent)
```

### 4. å®Œæ•´çš„é¢„å¤„ç† Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# å®šä¹‰æ•°å€¼ç‰¹å¾å’Œç±»åˆ«ç‰¹å¾çš„å¤„ç†æµç¨‹
numeric_features = ['age', 'income', 'credit_score']
categorical_features = ['city', 'occupation']

# æ•°å€¼ç‰¹å¾ Pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# ç±»åˆ«ç‰¹å¾ Pipeline
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# ç»„åˆæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# å®Œæ•´çš„ ML Pipeline
full_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression())
])

# ä¸€æ­¥åˆ°ä½ï¼šé¢„å¤„ç† + è®­ç»ƒ
# full_pipeline.fit(X_train, y_train)
# y_pred = full_pipeline.predict(X_test)
```

---

## åˆ†ç±»ç®—æ³•è¯¦è§£

### 1. é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰

è™½ç„¶åå­—é‡Œæœ‰"å›å½’"ï¼Œä½†é€»è¾‘å›å½’æ˜¯**åˆ†ç±»ç®—æ³•**ã€‚

**æ•°å­¦åŸç†**ï¼š
```
1. çº¿æ€§ç»„åˆï¼šz = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + b
2. Sigmoid å‡½æ•°ï¼šÏƒ(z) = 1 / (1 + eâ»á¶»)
3. é¢„æµ‹æ¦‚ç‡ï¼šP(y=1|x) = Ïƒ(z)
4. å†³ç­–è§„åˆ™ï¼šå¦‚æœ P(y=1|x) > 0.5ï¼Œé¢„æµ‹ç±»åˆ« 1ï¼Œå¦åˆ™ç±»åˆ« 0
```

```python
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
from sklearn.datasets import make_classification

X, y = make_classification(
    n_samples=200,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    n_clusters_per_class=1,
    random_state=42
)

# è®­ç»ƒé€»è¾‘å›å½’
lr = LogisticRegression()
lr.fit(X, y)

# å¯è§†åŒ–å†³ç­–è¾¹ç•Œ
def plot_decision_boundary(model, X, y):
    h = 0.02  # ç½‘æ ¼æ­¥é•¿
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Decision Boundary')
    plt.show()

plot_decision_boundary(lr, X, y)

# é¢„æµ‹æ¦‚ç‡
proba = lr.predict_proba(X[:5])
print("é¢„æµ‹æ¦‚ç‡:")
print(proba)
```

### 2. å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰

å†³ç­–æ ‘é€šè¿‡ä¸€ç³»åˆ— if-else è§„åˆ™è¿›è¡Œé¢„æµ‹ã€‚

```python
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# è®­ç»ƒå†³ç­–æ ‘
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(X, y)

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(20, 10))
plot_tree(dt, filled=True, feature_names=['Feature 1', 'Feature 2'])
plt.show()

# ç‰¹å¾é‡è¦æ€§
importances = dt.feature_importances_
print(f"ç‰¹å¾é‡è¦æ€§: {importances}")
```

**å…³é”®è¶…å‚æ•°**ï¼š
- `max_depth`ï¼šæ ‘çš„æœ€å¤§æ·±åº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
- `min_samples_split`ï¼šåˆ†è£‚å†…éƒ¨èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
- `min_samples_leaf`ï¼šå¶å­èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°

### 3. éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰

éšæœºæ£®æ—æ˜¯**é›†æˆå­¦ä¹ **çš„ä»£è¡¨ï¼Œé€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘å¹¶æŠ•ç¥¨ã€‚

```python
from sklearn.ensemble import RandomForestClassifier

# è®­ç»ƒéšæœºæ£®æ—
rf = RandomForestClassifier(
    n_estimators=100,  # æ ‘çš„æ•°é‡
    max_depth=10,
    random_state=42
)
rf.fit(X_train, y_train)

# è¯„ä¼°
accuracy = rf.score(X_test, y_test)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

# ç‰¹å¾é‡è¦æ€§
feature_importances = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("\nç‰¹å¾é‡è¦æ€§:")
print(feature_importances)
```

**ä¸ºä»€ä¹ˆéšæœºæ£®æ—æ•ˆæœå¥½ï¼Ÿ**
- **Bagging**ï¼šæ¯æ£µæ ‘è®­ç»ƒåœ¨ä¸åŒçš„æ•°æ®å­é›†ä¸Šï¼ˆæœ‰æ”¾å›æŠ½æ ·ï¼‰
- **ç‰¹å¾éšæœºæ€§**ï¼šæ¯æ¬¡åˆ†è£‚åªè€ƒè™‘éƒ¨åˆ†ç‰¹å¾
- **é™ä½æ–¹å·®**ï¼šå¤šä¸ªæ¨¡å‹å¹³å‡å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆ

### 4. æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰

SVM å¯»æ‰¾æœ€ä¼˜çš„å†³ç­–è¾¹ç•Œï¼ˆæœ€å¤§é—´éš”è¶…å¹³é¢ï¼‰ã€‚

```python
from sklearn.svm import SVC

# çº¿æ€§ SVM
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train, y_train)

# RBF æ ¸ SVMï¼ˆå¯ä»¥å¤„ç†éçº¿æ€§é—®é¢˜ï¼‰
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
svm_rbf.fit(X_train, y_train)

# å¯¹æ¯”æ€§èƒ½
print(f"çº¿æ€§ SVM å‡†ç¡®ç‡: {svm_linear.score(X_test, y_test):.4f}")
print(f"RBF SVM å‡†ç¡®ç‡: {svm_rbf.score(X_test, y_test):.4f}")
```

**å…³é”®å‚æ•°**ï¼š
- `C`ï¼šæ­£åˆ™åŒ–å‚æ•°ï¼ˆè¶Šå¤§è¶Šå€¾å‘äºæ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼‰
- `kernel`ï¼šæ ¸å‡½æ•°ç±»å‹ï¼ˆ'linear', 'rbf', 'poly'ï¼‰
- `gamma`ï¼šRBF æ ¸çš„å‚æ•°ï¼ˆå½±å“å†³ç­–è¾¹ç•Œçš„"å…‰æ»‘ç¨‹åº¦"ï¼‰

### 5. Kè¿‘é‚»ï¼ˆK-Nearest Neighborsï¼‰

KNN æ˜¯æœ€ç®€å•çš„ç®—æ³•ä¹‹ä¸€ï¼šé¢„æµ‹æ–°æ ·æœ¬æ—¶ï¼Œæ‰¾åˆ°æœ€è¿‘çš„ K ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œç”¨å®ƒä»¬çš„æ ‡ç­¾æŠ•ç¥¨ã€‚

```python
from sklearn.neighbors import KNeighborsClassifier

# è®­ç»ƒ KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# é¢„æµ‹
y_pred = knn.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

# æ‰¾åˆ°ä¸åŒ K å€¼çš„æœ€ä½³è®¾ç½®
k_values = range(1, 31)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    accuracies.append(knn.score(X_test, y_test))

plt.plot(k_values, accuracies)
plt.xlabel('K')
plt.ylabel('Accuracy')
plt.title('KNN: Accuracy vs K')
plt.show()
```

---

## å›å½’ç®—æ³•

### 1. çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰

```python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression

# ç”Ÿæˆå›å½’æ•°æ®
X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)

# è®­ç»ƒçº¿æ€§å›å½’
lr = LinearRegression()
lr.fit(X, y)

# å‚æ•°
print(f"æ–œç‡: {lr.coef_[0]:.4f}")
print(f"æˆªè·: {lr.intercept_:.4f}")

# é¢„æµ‹
y_pred = lr.predict(X)

# RÂ² è¯„åˆ†ï¼ˆå†³å®šç³»æ•°ï¼‰
from sklearn.metrics import r2_score, mean_squared_error

r2 = r2_score(y, y_pred)
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)

print(f"RÂ²: {r2:.4f}")
print(f"RMSE: {rmse:.4f}")

# å¯è§†åŒ–
plt.scatter(X, y, alpha=0.5)
plt.plot(X, y_pred, color='red', linewidth=2)
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression')
plt.show()
```

### 2. æ­£åˆ™åŒ–å›å½’

**Lassoï¼ˆL1 æ­£åˆ™åŒ–ï¼‰**ï¼š
```python
from sklearn.linear_model import Lasso

# Lasso ä¼šå°†ä¸€äº›ç³»æ•°å‹ç¼©åˆ° 0ï¼Œå®ç°ç‰¹å¾é€‰æ‹©
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

**Ridgeï¼ˆL2 æ­£åˆ™åŒ–ï¼‰**ï¼š
```python
from sklearn.linear_model import Ridge

# Ridge ä¼šç¼©å°æ‰€æœ‰ç³»æ•°ï¼Œä½†ä¸ä¼šå˜ä¸º 0
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
```

**ElasticNetï¼ˆL1 + L2ï¼‰**ï¼š
```python
from sklearn.linear_model import ElasticNet

# ç»“åˆäº† Lasso å’Œ Ridge çš„ä¼˜ç‚¹
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic.fit(X_train, y_train)
```

---

## æ¨¡å‹è¯„ä¼°ï¼šä¸ä»…ä»…æ˜¯å‡†ç¡®ç‡

### åˆ†ç±»è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    roc_auc_score,
    roc_curve
)

# å‡è®¾æˆ‘ä»¬æœ‰é¢„æµ‹ç»“æœ
y_true = np.array([0, 1, 1, 0, 1, 1, 0, 0, 1, 0])
y_pred = np.array([0, 1, 0, 0, 1, 1, 0, 1, 1, 0])

# 1. å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰
accuracy = accuracy_score(y_true, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

# 2. ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ï¼šé¢„æµ‹ä¸ºæ­£çš„æ ·æœ¬ä¸­ï¼ŒçœŸæ­£ä¸ºæ­£çš„æ¯”ä¾‹
precision = precision_score(y_true, y_pred)
print(f"ç²¾ç¡®ç‡: {precision:.4f}")

# 3. å¬å›ç‡ï¼ˆRecallï¼‰ï¼šçœŸæ­£ä¸ºæ­£çš„æ ·æœ¬ä¸­ï¼Œè¢«é¢„æµ‹ä¸ºæ­£çš„æ¯”ä¾‹
recall = recall_score(y_true, y_pred)
print(f"å¬å›ç‡: {recall:.4f}")

# 4. F1 åˆ†æ•°ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°
f1 = f1_score(y_true, y_pred)
print(f"F1 åˆ†æ•°: {f1:.4f}")

# 5. æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)
print("\næ··æ·†çŸ©é˜µ:")
print(cm)
```

**æ··æ·†çŸ©é˜µè§£é‡Š**ï¼š
```
              é¢„æµ‹ä¸ºè´Ÿ   é¢„æµ‹ä¸ºæ­£
å®é™…ä¸ºè´Ÿ (TN)   4      1
å®é™…ä¸ºæ­£ (FN)   1      4
```

- **True Positive (TP)**ï¼šé¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºæ­£
- **True Negative (TN)**ï¼šé¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºè´Ÿ
- **False Positive (FP)**ï¼šé¢„æµ‹ä¸ºæ­£ï¼Œå®é™…ä¸ºè´Ÿï¼ˆç¬¬ä¸€ç±»é”™è¯¯ï¼‰
- **False Negative (FN)**ï¼šé¢„æµ‹ä¸ºè´Ÿï¼Œå®é™…ä¸ºæ­£ï¼ˆç¬¬äºŒç±»é”™è¯¯ï¼‰

```python
# è®¡ç®—å„ä¸ªæŒ‡æ ‡
TN, FP, FN, TP = cm.ravel()

print(f"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}")
print(f"ç²¾ç¡®ç‡: {TP / (TP + FP):.4f}")
print(f"å¬å›ç‡: {TP / (TP + FN):.4f}")
```

### ROC æ›²çº¿å’Œ AUC

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# ç”Ÿæˆæ•°æ®
X, y = make_classification(n_samples=1000, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# è®­ç»ƒæ¨¡å‹
lr = LogisticRegression()
lr.fit(X_train, y_train)

# é¢„æµ‹æ¦‚ç‡
y_proba = lr.predict_proba(X_test)[:, 1]

# è®¡ç®— ROC æ›²çº¿
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)

# ç»˜åˆ¶ ROC æ›²çº¿
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

> **ğŸ’¡ AUC è§£é‡Š**ï¼š
> - AUC = 1.0ï¼šå®Œç¾åˆ†ç±»å™¨
> - AUC = 0.5ï¼šéšæœºçŒœæµ‹
> - AUC < 0.5ï¼šæ¯”éšæœºçŒœæµ‹è¿˜å·®ï¼ˆè¯´æ˜æ¨¡å‹åäº†ï¼‰

### å›å½’è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_true = np.array([3.0, -0.5, 2.0, 7.0])
y_pred = np.array([2.5, 0.0, 2.0, 8.0])

# 1. å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰
mae = mean_absolute_error(y_true, y_pred)
print(f"MAE: {mae:.4f}")

# 2. å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
mse = mean_squared_error(y_true, y_pred)
print(f"MSE: {mse:.4f}")

# 3. å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.4f}")

# 4. RÂ² åˆ†æ•°ï¼ˆå†³å®šç³»æ•°ï¼‰
r2 = r2_score(y_true, y_pred)
print(f"RÂ²: {r2:.4f}")
```

---

## äº¤å‰éªŒè¯ï¼šæ›´å¯é çš„æ€§èƒ½è¯„ä¼°

**é—®é¢˜**ï¼šå¦‚æœåªåˆ†å‰²ä¸€æ¬¡è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç»“æœå¯èƒ½ä¸ç¨³å®šï¼ˆå–å†³äºéšæœºåˆ†å‰²ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šK æŠ˜äº¤å‰éªŒè¯ï¼ˆK-Fold Cross-Validationï¼‰

```python
from sklearn.model_selection import cross_val_score, KFold

# K æŠ˜äº¤å‰éªŒè¯
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(lr, X, y, cv=kfold, scoring='accuracy')

print(f"æ¯æŠ˜å‡†ç¡®ç‡: {scores}")
print(f"å¹³å‡å‡†ç¡®ç‡: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

**å·¥ä½œåŸç†**ï¼š
```
æ•°æ®é›†åˆ†ä¸º 5 ä»½ï¼š

ç¬¬ 1 æŠ˜ï¼š[Test] [Train] [Train] [Train] [Train]
ç¬¬ 2 æŠ˜ï¼š[Train] [Test] [Train] [Train] [Train]
ç¬¬ 3 æŠ˜ï¼š[Train] [Train] [Test] [Train] [Train]
ç¬¬ 4 æŠ˜ï¼š[Train] [Train] [Train] [Test] [Train]
ç¬¬ 5 æŠ˜ï¼š[Train] [Train] [Train] [Train] [Test]

æœ€ç»ˆç»“æœ = 5 æ¬¡æµ‹è¯•çš„å¹³å‡å€¼
```

**åˆ†å±‚ K æŠ˜äº¤å‰éªŒè¯**ï¼ˆé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡ï¼‰ï¼š

```python
from sklearn.model_selection import StratifiedKFold

# ç¡®ä¿æ¯æŠ˜ä¸­å„ç±»åˆ«çš„æ¯”ä¾‹ä¸æ•´ä½“ä¸€è‡´
skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(lr, X, y, cv=skfold, scoring='accuracy')
```

---

## è¶…å‚æ•°è°ƒä¼˜

### 1. ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

# å®šä¹‰å‚æ•°ç½‘æ ¼
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],
    'kernel': ['rbf', 'linear']
}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    SVC(),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,  # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
    verbose=2
)

grid_search.fit(X_train, y_train)

# æœ€ä½³å‚æ•°
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å‡†ç¡®ç‡: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹é¢„æµ‹
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
```

### 2. éšæœºæœç´¢ï¼ˆRandom Searchï¼‰

å½“å‚æ•°ç©ºé—´å¾ˆå¤§æ—¶ï¼Œéšæœºæœç´¢æ¯”ç½‘æ ¼æœç´¢æ›´é«˜æ•ˆã€‚

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# å®šä¹‰å‚æ•°åˆ†å¸ƒ
param_distributions = {
    'C': uniform(0.1, 100),
    'gamma': uniform(0.001, 0.1),
    'kernel': ['rbf', 'linear']
}

# éšæœºæœç´¢
random_search = RandomizedSearchCV(
    SVC(),
    param_distributions,
    n_iter=50,  # å°è¯• 50 ç§éšæœºç»„åˆ
    cv=5,
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
```

---

## æ— ç›‘ç£å­¦ä¹ 

### 1. K-Means èšç±»

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# ç”Ÿæˆèšç±»æ•°æ®
X, y_true = make_blobs(n_samples=300, centers=4, random_state=42)

# K-Means èšç±»
kmeans = KMeans(n_clusters=4, random_state=42)
y_pred = kmeans.fit_predict(X)

# å¯è§†åŒ–
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0],
            kmeans.cluster_centers_[:, 1],
            s=300, c='red', marker='X', label='Centroids')
plt.legend()
plt.title('K-Means Clustering')
plt.show()

# æƒ¯æ€§ï¼ˆInertiaï¼‰ï¼šæ ·æœ¬åˆ°æœ€è¿‘èšç±»ä¸­å¿ƒçš„è·ç¦»å¹³æ–¹å’Œ
print(f"Inertia: {kmeans.inertia_:.2f}")
```

**è‚˜éƒ¨æ³•åˆ™ï¼ˆElbow Methodï¼‰**ï¼šé€‰æ‹©æœ€ä½³ K å€¼

```python
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

plt.plot(K_range, inertias, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method')
plt.show()
```

### 2. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰

PCA ç”¨äºé™ç»´ï¼Œä¿ç•™æœ€é‡è¦çš„ç‰¹å¾ã€‚

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_digits

# åŠ è½½æ‰‹å†™æ•°å­—æ•°æ®é›†ï¼ˆ64 ç»´ï¼‰
digits = load_digits()
X = digits.data
y = digits.target

print(f"åŸå§‹ç»´åº¦: {X.shape}")

# é™ç»´åˆ° 2 ç»´
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print(f"é™ç»´å: {X_pca.shape}")
print(f"è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.5)
plt.colorbar(scatter)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA of Digits Dataset')
plt.show()
```

**é€‰æ‹©åˆé€‚çš„ä¸»æˆåˆ†æ•°é‡**ï¼š

```python
# ä¿ç•™ 95% çš„æ–¹å·®
pca_95 = PCA(n_components=0.95)
X_pca_95 = pca_95.fit_transform(X)
print(f"ä¿ç•™ 95% æ–¹å·®éœ€è¦ {X_pca_95.shape[1]} ä¸ªä¸»æˆåˆ†")
```

---

## ç»¼åˆå®æˆ˜ï¼šå®¢æˆ·æµå¤±é¢„æµ‹

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼š

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from typing import Tuple, Dict, Any
import matplotlib.pyplot as plt
import seaborn as sns

class ChurnPredictionPipeline:
    """
    å®¢æˆ·æµå¤±é¢„æµ‹å®Œæ•´æµç¨‹

    åŒ…å«ï¼š
    1. æ•°æ®åŠ è½½å’Œæ¢ç´¢
    2. ç‰¹å¾å·¥ç¨‹
    3. æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
    4. è¶…å‚æ•°è°ƒä¼˜
    5. æ¨¡å‹è§£é‡Š
    """

    def __init__(self):
        self.models = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}

    def generate_data(self, n_samples: int = 5000) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿå®¢æˆ·æ•°æ®"""
        np.random.seed(42)

        data = {
            'customer_id': range(1, n_samples + 1),
            'age': np.random.randint(18, 70, n_samples),
            'tenure_months': np.random.randint(1, 72, n_samples),
            'monthly_charges': np.random.uniform(20, 120, n_samples),
            'total_charges': np.random.uniform(100, 8000, n_samples),
            'num_products': np.random.randint(1, 5, n_samples),
            'has_internet': np.random.choice([0, 1], n_samples),
            'has_phone': np.random.choice([0, 1], n_samples),
            'contract_type': np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples),
            'payment_method': np.random.choice(['Electronic', 'Mail', 'Bank', 'Credit'], n_samples),
            'support_calls': np.random.randint(0, 10, n_samples),
        }

        df = pd.DataFrame(data)

        # åŸºäºç‰¹å¾ç”Ÿæˆæµå¤±æ ‡ç­¾ï¼ˆæœ‰ä¸€å®šçš„é€»è¾‘å…³ç³»ï¼‰
        churn_prob = (
            0.1 +
            0.3 * (df['contract_type'] == 'Month-to-month').astype(int) +
            0.2 * (df['tenure_months'] < 12).astype(int) +
            0.1 * (df['support_calls'] > 5).astype(int) +
            0.15 * (df['monthly_charges'] > 80).astype(int)
        )

        churn_prob = np.clip(churn_prob, 0, 1)
        df['churn'] = np.random.binomial(1, churn_prob)

        return df

    def eda(self, df: pd.DataFrame) -> None:
        """æ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        print("=" * 60)
        print("æ•°æ®æ¦‚è§ˆ")
        print("=" * 60)
        print(f"æ•°æ®é›†å½¢çŠ¶: {df.shape}")
        print(f"\næµå¤±ç‡: {df['churn'].mean():.2%}")

        print("\næ•°å€¼ç‰¹å¾ç»Ÿè®¡:")
        print(df.describe())

        print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
        print(df.isnull().sum())

        # å¯è§†åŒ–æµå¤±ç‡
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. åˆåŒç±»å‹ vs æµå¤±ç‡
        churn_by_contract = df.groupby('contract_type')['churn'].mean()
        axes[0, 0].bar(churn_by_contract.index, churn_by_contract.values)
        axes[0, 0].set_title('Churn Rate by Contract Type')
        axes[0, 0].set_ylabel('Churn Rate')

        # 2. åœ¨ç½‘æ—¶é•¿ vs æµå¤±ç‡
        tenure_bins = [0, 12, 24, 36, 48, 72]
        df['tenure_group'] = pd.cut(df['tenure_months'], bins=tenure_bins)
        churn_by_tenure = df.groupby('tenure_group')['churn'].mean()
        axes[0, 1].bar(range(len(churn_by_tenure)), churn_by_tenure.values)
        axes[0, 1].set_title('Churn Rate by Tenure')
        axes[0, 1].set_ylabel('Churn Rate')
        axes[0, 1].set_xticklabels(churn_by_tenure.index, rotation=45)

        # 3. æœˆè´¹ç”¨åˆ†å¸ƒ
        axes[1, 0].hist(df[df['churn'] == 0]['monthly_charges'], alpha=0.5, label='No Churn', bins=30)
        axes[1, 0].hist(df[df['churn'] == 1]['monthly_charges'], alpha=0.5, label='Churn', bins=30)
        axes[1, 0].set_title('Monthly Charges Distribution')
        axes[1, 0].legend()

        # 4. ç›¸å…³æ€§çƒ­å›¾
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        corr = df[numeric_cols].corr()
        sns.heatmap(corr, annot=True, fmt='.2f', ax=axes[1, 1], cmap='coolwarm')
        axes[1, 1].set_title('Feature Correlation')

        plt.tight_layout()
        plt.show()

    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        df = df.copy()

        # 1. åˆ›å»ºæ–°ç‰¹å¾
        df['avg_monthly_charges'] = df['total_charges'] / (df['tenure_months'] + 1)
        df['charges_to_products'] = df['monthly_charges'] / (df['num_products'] + 1)
        df['tenure_x_products'] = df['tenure_months'] * df['num_products']

        # 2. åˆ†ç®±ç‰¹å¾
        df['age_group'] = pd.cut(df['age'], bins=[0, 30, 50, 100], labels=['young', 'middle', 'senior'])
        df['tenure_group'] = pd.cut(df['tenure_months'], bins=[0, 12, 24, 72], labels=['new', 'medium', 'long'])

        # 3. æ˜¯å¦é«˜ä»·å€¼å®¢æˆ·
        df['is_high_value'] = (df['monthly_charges'] > df['monthly_charges'].median()).astype(int)

        return df

    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—
        df = df.drop(['customer_id'], axis=1)

        # ç¼–ç ç±»åˆ«ç‰¹å¾
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns

        for col in categorical_cols:
            if col != 'churn':
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                self.label_encoders[col] = le

        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
        X = df.drop('churn', axis=1)
        y = df['churn']

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # æ ‡å‡†åŒ–
        X_train = self.scaler.fit_transform(X_train)
        X_test = self.scaler.transform(X_test)

        return X_train, X_test, y_train, y_test

    def train_models(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray
    ) -> Dict[str, Any]:
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        models = {
            'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
        }

        print("\n" + "=" * 60)
        print("è®­ç»ƒæ¨¡å‹ï¼ˆ5 æŠ˜äº¤å‰éªŒè¯ï¼‰")
        print("=" * 60)

        results = {}
        for name, model in models.items():
            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')
            results[name] = {
                'model': model,
                'cv_mean': scores.mean(),
                'cv_std': scores.std()
            }
            print(f"{name:25s}: AUC = {scores.mean():.4f} (+/- {scores.std():.4f})")

            # è®­ç»ƒå®Œæ•´æ¨¡å‹
            model.fit(X_train, y_train)

        self.models = results
        return results

    def evaluate_model(
        self,
        model: Any,
        X_test: np.ndarray,
        y_test: np.ndarray,
        model_name: str
    ) -> None:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1]

        print(f"\n{'=' * 60}")
        print(f"{model_name} - æµ‹è¯•é›†è¯„ä¼°")
        print(f"{'=' * 60}")

        # åˆ†ç±»æŠ¥å‘Š
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))

        # AUC
        auc = roc_auc_score(y_test, y_proba)
        print(f"\nROC AUC: {auc:.4f}")

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f'{model_name} - Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show()

    def tune_best_model(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray
    ) -> Any:
        """å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜"""
        print("\n" + "=" * 60)
        print("è¶…å‚æ•°è°ƒä¼˜ï¼ˆRandom Forestï¼‰")
        print("=" * 60)

        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }

        rf = RandomForestClassifier(random_state=42)

        grid_search = GridSearchCV(
            rf,
            param_grid,
            cv=5,
            scoring='roc_auc',
            n_jobs=-1,
            verbose=1
        )

        grid_search.fit(X_train, y_train)

        print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"æœ€ä½³ AUC: {grid_search.best_score_:.4f}")

        self.best_model = grid_search.best_estimator_
        return self.best_model

    def feature_importance(self, feature_names: list) -> None:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        if self.best_model is None:
            print("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return

        importances = self.best_model.feature_importances_
        indices = np.argsort(importances)[::-1]

        plt.figure(figsize=(12, 8))
        plt.title("Feature Importances")
        plt.bar(range(len(importances)), importances[indices])
        plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)
        plt.tight_layout()
        plt.show()

        print("\nTop 10 é‡è¦ç‰¹å¾:")
        for i in range(min(10, len(importances))):
            idx = indices[i]
            print(f"{i+1:2d}. {feature_names[idx]:30s}: {importances[idx]:.4f}")

# è¿è¡Œå®Œæ•´ Pipeline
pipeline = ChurnPredictionPipeline()

# 1. ç”Ÿæˆæ•°æ®
df = pipeline.generate_data(n_samples=5000)

# 2. æ¢ç´¢æ€§åˆ†æ
pipeline.eda(df)

# 3. ç‰¹å¾å·¥ç¨‹
df_engineered = pipeline.feature_engineering(df)

# 4. å‡†å¤‡æ•°æ®
X_train, X_test, y_train, y_test = pipeline.prepare_data(df_engineered)
feature_names = [col for col in df_engineered.columns if col not in ['churn', 'customer_id']]

print(f"\nç‰¹å¾æ•°é‡: {X_train.shape[1]}")
print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")

# 5. è®­ç»ƒå¤šä¸ªæ¨¡å‹
results = pipeline.train_models(X_train, y_train)

# 6. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
for name, result in results.items():
    pipeline.evaluate_model(result['model'], X_test, y_test, name)

# 7. è¶…å‚æ•°è°ƒä¼˜
best_model = pipeline.tune_best_model(X_train, y_train)

# 8. æœ€ç»ˆè¯„ä¼°
pipeline.evaluate_model(best_model, X_test, y_test, "Tuned Random Forest")

# 9. ç‰¹å¾é‡è¦æ€§åˆ†æ
pipeline.feature_importance(feature_names)
```

---

## åå·®-æ–¹å·®æƒè¡¡ï¼ˆBias-Variance Tradeoffï¼‰

è¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€é‡è¦çš„æ¦‚å¿µä¹‹ä¸€ã€‚

```python
# æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + ä¸å¯çº¦è¯¯å·®
```

- **é«˜åå·®ï¼ˆHigh Biasï¼‰**ï¼šæ¨¡å‹å¤ªç®€å•ï¼Œæ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰
  - ä¾‹å¦‚ï¼šç”¨çº¿æ€§æ¨¡å‹æ‹Ÿåˆéçº¿æ€§æ•°æ®

- **é«˜æ–¹å·®ï¼ˆHigh Varianceï¼‰**ï¼šæ¨¡å‹å¤ªå¤æ‚ï¼Œè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰
  - ä¾‹å¦‚ï¼šç”¨é«˜é˜¶å¤šé¡¹å¼æ‹Ÿåˆå°‘é‡æ•°æ®

**ç¤ºä¾‹**ï¼š

```python
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# ç”Ÿæˆéçº¿æ€§æ•°æ®
np.random.seed(0)
X = np.sort(np.random.rand(40, 1), axis=0)
y = np.sin(2 * np.pi * X).ravel() + np.random.normal(0, 0.1, 40)

# æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
degrees = [1, 4, 15]
plt.figure(figsize=(15, 5))

for i, degree in enumerate(degrees):
    ax = plt.subplot(1, 3, i + 1)

    # åˆ›å»ºå¤šé¡¹å¼å›å½’æ¨¡å‹
    model = make_pipeline(PolynomialFeatures(degree), Ridge())
    model.fit(X, y)

    # ç»˜åˆ¶é¢„æµ‹æ›²çº¿
    X_test = np.linspace(0, 1, 100).reshape(-1, 1)
    y_pred = model.predict(X_test)

    plt.scatter(X, y, color='blue', alpha=0.5)
    plt.plot(X_test, y_pred, color='red', linewidth=2)
    plt.title(f'Degree {degree}')
    plt.ylim(-2, 2)

plt.show()
```

**å¦‚ä½•è§£å†³**ï¼š
- **æ­£åˆ™åŒ–**ï¼šL1/L2 æƒ©ç½š
- **æ›´å¤šæ•°æ®**ï¼šæ›´å¤šè®­ç»ƒæ•°æ®å¯ä»¥é™ä½æ–¹å·®
- **ç‰¹å¾é€‰æ‹©**ï¼šå»é™¤æ— å…³ç‰¹å¾
- **é›†æˆæ–¹æ³•**ï¼šBagging é™ä½æ–¹å·®ï¼ŒBoosting é™ä½åå·®

---

## å®ç”¨æŠ€å·§å’Œæœ€ä½³å®è·µ

### 1. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡

```python
from sklearn.utils.class_weight import compute_class_weight

# æ–¹æ³• 1ï¼šç±»æƒé‡
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
model = LogisticRegression(class_weight='balanced')

# æ–¹æ³• 2ï¼šè¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# æ–¹æ³• 3ï¼šæ¬ é‡‡æ ·
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)
```

### 2. ç‰¹å¾é€‰æ‹©

```python
from sklearn.feature_selection import SelectKBest, f_classif, RFE

# æ–¹æ³• 1ï¼šå•å˜é‡ç‰¹å¾é€‰æ‹©
selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X_train, y_train)

# æ–¹æ³• 2ï¼šé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰
rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)
X_rfe = rfe.fit_transform(X_train, y_train)

# æ–¹æ³• 3ï¼šåŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
from sklearn.feature_selection import SelectFromModel

selector = SelectFromModel(RandomForestClassifier(n_estimators=100))
selector.fit(X_train, y_train)
X_selected = selector.transform(X_train)
```

### 3. æ¨¡å‹æŒä¹…åŒ–

```python
import joblib

# ä¿å­˜æ¨¡å‹
joblib.dump(model, 'model.pkl')

# åŠ è½½æ¨¡å‹
loaded_model = joblib.load('model.pkl')
predictions = loaded_model.predict(X_test)
```

---

## å°ç»“

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†ï¼š

âœ… **æœºå™¨å­¦ä¹ ä¸‰å¤§èŒƒå¼**
- ç›‘ç£å­¦ä¹ ï¼šåˆ†ç±»å’Œå›å½’
- æ— ç›‘ç£å­¦ä¹ ï¼šèšç±»å’Œé™ç»´
- å¼ºåŒ–å­¦ä¹ ï¼šç­–ç•¥å­¦ä¹ 

âœ… **Scikit-Learn æ ¸å¿ƒ API**
- ç»Ÿä¸€çš„ Estimator æ¥å£
- Pipeline å’Œ ColumnTransformer
- æ¨¡å‹æŒä¹…åŒ–

âœ… **æ•°æ®é¢„å¤„ç†**
- ç‰¹å¾ç¼©æ”¾ï¼ˆæ ‡å‡†åŒ–ã€å½’ä¸€åŒ–ï¼‰
- ç±»åˆ«ç¼–ç ï¼ˆLabel Encodingã€One-Hot Encodingï¼‰
- ç¼ºå¤±å€¼å¤„ç†

âœ… **åˆ†ç±»ç®—æ³•**
- é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—
- SVMã€KNN
- é›†æˆå­¦ä¹ 

âœ… **æ¨¡å‹è¯„ä¼°**
- å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
- ROC æ›²çº¿å’Œ AUC
- äº¤å‰éªŒè¯

âœ… **è¶…å‚æ•°è°ƒä¼˜**
- ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰
- éšæœºæœç´¢ï¼ˆRandom Searchï¼‰

âœ… **å®æˆ˜é¡¹ç›®**
- å®¢æˆ·æµå¤±é¢„æµ‹å®Œæ•´æµç¨‹
- ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è§£é‡Š

---

## ç»ƒä¹ é¢˜

### åŸºç¡€é¢˜

1. ä½¿ç”¨ Scikit-Learn çš„ `load_breast_cancer` æ•°æ®é›†ï¼Œè®­ç»ƒä¸€ä¸ªé€»è¾‘å›å½’æ¨¡å‹å¹¶è®¡ç®—å‡†ç¡®ç‡
2. ç”¨ç½‘æ ¼æœç´¢ä¸ºå†³ç­–æ ‘æ‰¾åˆ°æœ€ä½³çš„ `max_depth` å‚æ•°
3. ç»˜åˆ¶ä¸€ä¸ªæ¨¡å‹çš„ ROC æ›²çº¿

### è¿›é˜¶é¢˜

4. å®ç°ä¸€ä¸ªå®Œæ•´çš„ Pipelineï¼ŒåŒ…å«ï¼š
   - æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
   - ç±»åˆ«ç‰¹å¾ One-Hot ç¼–ç 
   - PCA é™ç»´åˆ° 10 ç»´
   - éšæœºæ£®æ—åˆ†ç±»
5. å¯¹ä¸€ä¸ªä¸å¹³è¡¡æ•°æ®é›†ï¼Œæ¯”è¾ƒä½¿ç”¨ç±»æƒé‡ã€SMOTEã€æ¬ é‡‡æ ·ä¸‰ç§æ–¹æ³•çš„æ•ˆæœ
6. ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰é€‰æ‹©æœ€é‡è¦çš„ 5 ä¸ªç‰¹å¾

### æŒ‘æˆ˜é¢˜

7. æ„å»ºä¸€ä¸ª Voting Classifierï¼Œç»“åˆé€»è¾‘å›å½’ã€éšæœºæ£®æ—å’Œ SVMï¼Œæ¯”è¾ƒä¸å•ä¸ªæ¨¡å‹çš„æ€§èƒ½
8. å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„ Transformerï¼Œå¯ä»¥åœ¨ Pipeline ä¸­ä½¿ç”¨
9. ç”¨ K-Means å¯¹ Iris æ•°æ®é›†èšç±»ï¼Œç„¶åç”¨ PCA å¯è§†åŒ–ç»“æœï¼Œå¹¶ä¸çœŸå®æ ‡ç­¾å¯¹æ¯”

---

## ğŸ”— ä¸ LangChain çš„è”ç³»

Scikit-Learn åœ¨ AI Agent å¼€å‘ä¸­çš„åº”ç”¨ï¼š

1. **æ–‡æœ¬åˆ†ç±»å™¨**ï¼šç”¨äºæ„å›¾è¯†åˆ«
   ```python
   from sklearn.feature_extraction.text import TfidfVectorizer
   from sklearn.naive_bayes import MultinomialNB

   # è®­ç»ƒæ„å›¾åˆ†ç±»å™¨
   vectorizer = TfidfVectorizer()
   classifier = MultinomialNB()
   ```

2. **èšç±»**ï¼šç”¨äºå¯¹è¯å†å²åˆ†ç»„
3. **å¼‚å¸¸æ£€æµ‹**ï¼šè¯†åˆ«å¼‚å¸¸ç”¨æˆ·è¡Œä¸º
4. **æ¨èç³»ç»Ÿ**ï¼šåŸºäºç›¸ä¼¼åº¦çš„æ–‡æ¡£æ¨è

åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ æ·±åº¦å­¦ä¹ æ¡†æ¶ PyTorch å’Œ TensorFlowï¼Œå®ƒä»¬å°†ä¸ºæˆ‘ä»¬æ‰“å¼€ç¥ç»ç½‘ç»œçš„å¤§é—¨ï¼

---

**ä¸‹ä¸€èŠ‚ï¼š[10.4 æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šPyTorch ä¸ TensorFlow](10.4-æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šPyTorchä¸TensorFlow.md)**

åœ¨ä¸‹ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ è¿ˆå‘æ·±åº¦å­¦ä¹ ï¼Œå­¦ä¹  PyTorch çš„å¼ é‡æ“ä½œã€è‡ªåŠ¨å¾®åˆ†å’Œç¥ç»ç½‘ç»œæ„å»ºã€‚
